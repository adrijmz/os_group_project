{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from rdflib.namespace import RDF, RDFS, XSD\n",
    "from rdflib import Graph, Literal, URIRef\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Publication:\n",
    "    def __init__(self, title, doi, cites, num_pages, publication_date, language, pages, published_in, main_subject, instance_of, author, topic, similar_papers, entities):\n",
    "        self._title = title\n",
    "        self._doi = doi\n",
    "        self._cites = cites\n",
    "        self._num_pages = num_pages\n",
    "        self._publication_date = publication_date\n",
    "        self._language = language\n",
    "        self._pages = pages\n",
    "        self._published_in = published_in\n",
    "        self._main_subject = main_subject\n",
    "        self._instance_of = instance_of\n",
    "        self._author = author\n",
    "        self._topic = topic\n",
    "        self._similar_papers = similar_papers\n",
    "        self._entities = entities\n",
    "\n",
    "    # Getters\n",
    "    def get_title(self):\n",
    "        return self._title\n",
    "\n",
    "    def get_doi(self):\n",
    "        return self._doi\n",
    "\n",
    "    def get_cites(self):\n",
    "        return self._cites\n",
    "\n",
    "    def get_num_pages(self):\n",
    "        return self._num_pages\n",
    "\n",
    "    def get_publication_date(self):\n",
    "        return self._publication_date\n",
    "\n",
    "    def get_language(self):\n",
    "        return self._language\n",
    "\n",
    "    def get_pages(self):\n",
    "        return self._pages\n",
    "\n",
    "    def get_published_in(self):\n",
    "        return self._published_in\n",
    "\n",
    "    def get_main_subject(self):\n",
    "        return self._main_subject\n",
    "\n",
    "    def get_instance_of(self):\n",
    "        return self._instance_of\n",
    "\n",
    "    def get_author(self):\n",
    "        return self._author\n",
    "    \n",
    "    def get_topic(self):\n",
    "        return self._topic\n",
    "\n",
    "    def get_similar_papers(self):\n",
    "        return self._similar_papers\n",
    "    \n",
    "    def get_entities(self):\n",
    "        return self._entities\n",
    "\n",
    "    #Other fucntions\n",
    "    def display_info(self):\n",
    "        print(\"Title:\", self._title)\n",
    "        print(\"DOI:\", self._doi)\n",
    "        print(\"Cites:\", self._cites)\n",
    "        print(\"Number of Pages:\", self._num_pages)\n",
    "        print(\"Publication Date:\", self._publication_date)\n",
    "        print(\"Language:\", self._language)\n",
    "        print(\"Pages:\", self._pages)\n",
    "        print(\"Published In:\", self._published_in)\n",
    "        print(\"Main Subject:\", self._main_subject)\n",
    "        print(\"Instance Of:\", self._instance_of)\n",
    "        print(\"Author:\", self._author)\n",
    "        print(\"Topic:\", self._topic)\n",
    "        print(\"Similar Papers:\", self._similar_papers)\n",
    "        print(\"Entities:\", self._entities)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_papers = []\n",
    "author_by_doi = {}\n",
    "topic_by_doi = {}\n",
    "topic_and_prob_by_title = {}\n",
    "topic_and_prob_by_doi = {}\n",
    "possible_topics = []\n",
    "similarities_by_doi = {}\n",
    "similarities_by_title = []\n",
    "entities_by_doi = {}\n",
    "\n",
    "wikidata_res = '../../papers/wikidata/results.csv'\n",
    "openalex_res = '../../papers/openalex/results.csv'\n",
    "prob_res = '../../papers/probabilities/'\n",
    "doi_res = '../../papers/doi/'\n",
    "topic_res = '../../papers/topics/'\n",
    "similarities_res = '../../papers/similarities/'\n",
    "ner_res = '../../papers/ner/'\n",
    "aux_list = []\n",
    "\n",
    "# read authors\n",
    "with open(openalex_res, 'r') as f:\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        line_split = line.split(',')\n",
    "        doi = line_split[0]\n",
    "        author = line_split[1]\n",
    "        author_institution = line_split[2]\n",
    "\n",
    "        if doi in author_by_doi and author not in author_by_doi[doi]:\n",
    "            author_by_doi[doi].append(author)\n",
    "        else:\n",
    "            author_by_doi[doi] = [author]\n",
    "\n",
    "# read topics\n",
    "with open(topic_res + 'topics.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        if line == '\\n':\n",
    "            continue\n",
    "        line = line.replace('\\n', '')\n",
    "        topic = line.split(\":\")[1].strip().replace(\",\", \"_\").replace(\" \", \"\")\n",
    "        topic = str(topic)\n",
    "        possible_topics.append(topic)\n",
    "\n",
    "# read topics and probabilities\n",
    "for filename in os.listdir(prob_res):\n",
    "    with open(prob_res + filename, 'r') as f:\n",
    "        line = f.read()\n",
    "\n",
    "        topic_start = line.find(\"Topic: [\") + len(\"Topic: [\")\n",
    "        topic_end = line.find(\"]\", topic_start)\n",
    "        prob_start = line.find(\"Probability: \") + len(\"Probability: \")\n",
    "        prob_end = line.find(\"\\n\", prob_start)\n",
    "\n",
    "        topic = line[topic_start:topic_end]\n",
    "        probabilidad = float(line[prob_start:prob_end])\n",
    "        topic_and_prob_by_title[filename] = (topic, probabilidad)\n",
    "\n",
    "with open(similarities_res + 'similarities.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.replace('\\n', '')\n",
    "        line_split = line.split(\";\")\n",
    "        title1 = line_split[0]\n",
    "        title2 = line_split[1]\n",
    "        similarity = float(line_split[2])\n",
    "\n",
    "        similarities_by_title.append((title1, title2, similarity))\n",
    "        \n",
    "        if title1 not in aux_list:\n",
    "            aux_list.append(title1)\n",
    "        if title2 not in aux_list:\n",
    "            aux_list.append(title2)    \n",
    "    \n",
    "for filename in os.listdir(doi_res):\n",
    "    with open(doi_res + filename, 'r') as f:\n",
    "        if filename in topic_and_prob_by_title:\n",
    "            doi = f.read()\n",
    "            topic = topic_and_prob_by_title[filename][0]\n",
    "            probabilidad = topic_and_prob_by_title[filename][1]\n",
    "            topic_and_prob_by_doi[doi] = (topic, probabilidad)\n",
    "        \n",
    "        if filename in aux_list:\n",
    "            for title1, title2, similarity in similarities_by_title:\n",
    "                doi1 = open(doi_res + title1, 'r').read()\n",
    "                doi2 = open(doi_res + title2, 'r').read()\n",
    "                if title1 == filename:\n",
    "                    if similarities_by_doi.get(doi1) is None:\n",
    "                        similarities_by_doi[doi1] = [doi2]\n",
    "                    else:\n",
    "                        similarities_by_doi[doi1].append(doi2)\n",
    "                elif title2 == filename:\n",
    "                    if similarities_by_doi.get(doi2) is None:\n",
    "                        similarities_by_doi[doi2] = [doi1]\n",
    "                    else:\n",
    "                        similarities_by_doi[doi2].append(doi1)\n",
    "\n",
    "# read ner\n",
    "for filename in os.listdir(ner_res):\n",
    "    with open(ner_res + filename, 'r') as f:\n",
    "        with open(doi_res + filename, 'r') as f2:\n",
    "            entities = f.read()\n",
    "            doi = f2.read()\n",
    "            entities = str(entities.replace('[', '').replace(']', '').replace(\",\", \" -\"))\n",
    "            entities_by_doi[doi] = entities\n",
    "            \n",
    "# read csv\n",
    "with open(wikidata_res, 'r') as f:\n",
    "    # skip header\n",
    "    f.readline()\n",
    "    for line in f:\n",
    "        if line.startswith('\"'):\n",
    "            title = line.split('\"')[1]\n",
    "            doi = line.split('\"')[2]\n",
    "            doi = doi.split(',')[1]\n",
    "            # change content between double quotes\n",
    "            line = line.replace(title, \"\")\n",
    "        else:    \n",
    "            line_split = line.split(',')\n",
    "            # save all atributes in paper object\n",
    "            title = line_split[0]\n",
    "            doi = line_split[1]\n",
    "            \n",
    "        line_split = line.split(',')\n",
    "\n",
    "        if doi in author_by_doi:\n",
    "            authors = author_by_doi[doi]\n",
    "            authors = str(authors).replace(',', ' -').replace('[', '').replace(']', '')\n",
    "        else:\n",
    "            authors = \"\"\n",
    "\n",
    "        if doi in topic_and_prob_by_doi:\n",
    "            topic, probabilidad = topic_and_prob_by_doi[doi]\n",
    "        else:\n",
    "            topic = \"\"\n",
    "\n",
    "        if doi in entities_by_doi:\n",
    "            entities = entities_by_doi[doi]\n",
    "        else:\n",
    "            entities = \"\"\n",
    "\n",
    "        similar_papers = []\n",
    "        \n",
    "        if doi in similarities_by_doi:\n",
    "            similar_papers = similarities_by_doi[doi]\n",
    "        \n",
    "        similar_papers = str(similar_papers).replace(',', ' -').replace('[', '').replace(']', '')\n",
    "        \n",
    "        line_split[9] = line_split[9].replace('\\n', '')\n",
    "        title = title.replace(\" \", \"_\")\n",
    "        title = title.replace(\",\", \"\")\n",
    "        cleaned_topics = topic.replace(\"'\", \"\").replace(\",\", \"\").replace(\" \", \"_\")\n",
    "\n",
    "        paper = Publication(title, doi, line_split[2], line_split[3], line_split[4], line_split[5], line_split[6], line_split[7], line_split[8], line_split[9], authors, cleaned_topics, similar_papers, entities)\n",
    "        list_papers.append(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "\n",
    "for paper in list_papers:\n",
    "    paper_uri = URIRef(\"http://example.org/\" + paper.get_doi())\n",
    "    g.add((paper_uri, RDF.type, URIRef(\"http://schema.org/paper\")))\n",
    "    g.add((paper_uri, URIRef(\"http://schema.org/title\"), Literal(paper.get_title())))\n",
    "    g.add((paper_uri, URIRef(\"http://schema.org/doi\"), Literal(paper.get_doi())))\n",
    "    g.add((paper_uri, URIRef(\"http://schema.org/cites\"), Literal(paper.get_cites())))\n",
    "    g.add((paper_uri, URIRef(\"http://schema.org/numPages\"), Literal(paper.get_num_pages())))\n",
    "    g.add((paper_uri, URIRef(\"http://schema.org/publicationDate\"), Literal(paper.get_publication_date())))\n",
    "    g.add((paper_uri, URIRef(\"http://schema.org/inLanguage\"), Literal(paper.get_language())))\n",
    "    g.add((paper_uri, URIRef(\"http://schema.org/pages\"), Literal(paper.get_pages())))\n",
    "    g.add((paper_uri, URIRef(\"http://schema.org/publishedIn\"), Literal(paper.get_published_in())))\n",
    "    g.add((paper_uri, URIRef(\"http://schema.org/mainSubject\"), Literal(paper.get_main_subject())))\n",
    "    g.add((paper_uri, URIRef(\"http://schema.org/author\"), Literal(paper.get_author())))\n",
    "    g.add((paper_uri, URIRef(\"http://schema.org/topic\"), Literal(paper.get_topic())))\n",
    "    g.add((paper_uri, URIRef(\"http://schema.org/similarPapers\"), Literal(paper.get_similar_papers())))\n",
    "    g.add((paper_uri, URIRef(\"http://schema.org/acknowledges\"), Literal(paper.get_entities())))\n",
    "\n",
    "for topic in possible_topics:\n",
    "    topic_uri = URIRef(\"http://example.org/\"+topic)\n",
    "    g.add((topic_uri, RDF.type, URIRef(\"http://schema.org/topic\")))\n",
    "    g.add((topic_uri, URIRef(\"http://schema.org/name\"), Literal(topic)))\n",
    "\n",
    "# g.serialize(destination='papers.xml', format='xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Namespace\n",
    "onto = Namespace(\"http://example.org/\")\n",
    "g.bind(\"onto\", onto)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        query = input(\"Enter a SPARQL query or 'exit' to finish: \")\n",
    "        if query == 'exit':\n",
    "            break\n",
    "        else:\n",
    "            for row in g.query(query):\n",
    "                print(row)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying the graph for a specific paper\n",
      "\n",
      "\n",
      "(rdflib.term.Literal('An_Efficient_Deep_Learning_Classification_Model_for_Predicting_Credit_Card_Fraud_on_Skewed_Data'), rdflib.term.Literal('wikipedia_claim_citation_system_card_model_source_human_user_information'), rdflib.term.Literal(''))\n"
     ]
    }
   ],
   "source": [
    "print(\"Querying the graph for a specific paper\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for result in g.query(\n",
    "    '''PREFIX schema: <http://schema.org/>\n",
    "\n",
    "    SELECT ?title ?topic ?author\n",
    "    WHERE {\n",
    "    ?paper a schema:paper ;\n",
    "        schema:doi \"10.26735/TLYG7256\" ;\n",
    "        schema:title ?title ;\n",
    "        schema:topic ?topic ;\n",
    "        schema:author ?author .\n",
    "    }'''\n",
    "):\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All possible topics:\n",
      "\n",
      "\n",
      "(rdflib.term.Literal('learning_technology_ml_ai_deep_higher_education_artificial_recent_paper'),)\n",
      "(rdflib.term.Literal('ai_transaction_fraud_technology_card_algorithm_credit_machine_data_use'),)\n",
      "(rdflib.term.Literal('ai_transaction_development_sustainable_target_card_data_study_authorized_credit'),)\n",
      "(rdflib.term.Literal('task_text_row_knowledge_base_additional_table_generation_using_gap'),)\n",
      "(rdflib.term.Literal('wikipedia_claim_citation_system_card_model_source_human_user_information'),)\n",
      "(rdflib.term.Literal('dl_gene_ha_credit_data_fraud_card_wiki_challenge_community'),)\n",
      "(rdflib.term.Literal('data_fraud_model_card_ha_disease_ml_credit_using_study'),)\n",
      "(rdflib.term.Literal('fraud_card_credit_imaging_increase_learning_algorithm_transaction_diagnostic_problem'),)\n",
      "(rdflib.term.Literal('application_deep_area_learning_processing_information_three_natural_language_use'),)\n",
      "(rdflib.term.Literal('model_wit_learning_wikipedia_information_performance_article_multimodal_example_image'),)\n"
     ]
    }
   ],
   "source": [
    "print(\"All possible topics:\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for result in g.query(\n",
    "    '''PREFIX schema: <http://schema.org/>\n",
    "    SELECT ?topic\n",
    "    WHERE {\n",
    "        ?paper a schema:topic ;\n",
    "               schema:name ?topic .\n",
    "    }\n",
    "    '''\n",
    "):\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All papers:\n",
      "\n",
      "\n",
      "(rdflib.term.Literal('A_multilevel_review_of_artificial_intelligence_in_organizations:_Implications_for_organizational_behavior_research_and_practice'),)\n",
      "(rdflib.term.Literal('A_systematic_review_of_the_applications_of_artificial_intelligence_and_machine_learning_in_autoimmune_diseases'),)\n",
      "(rdflib.term.Literal('Exploring_the_impact_of_artificial_intelligence_on_teaching_and_learning_in_higher_education'),)\n",
      "(rdflib.term.Literal('Deep_Learning:_Methods_and_Applications'),)\n",
      "(rdflib.term.Literal('Deep_learning'),)\n",
      "(rdflib.term.Literal('Surrogate_techniques_for_testing_fraud_detection_algorithms_in_credit_card_operations'),)\n",
      "(rdflib.term.Literal('use_of_data_mining_of_to_create_of_a_fraud_prevention_and_detection_system_in_credit_card'),)\n",
      "(rdflib.term.Literal('WIT:_Wikipedia-based_Image_Text_Dataset_for_Multimodal_Multilingual_Machine_Learning'),)\n",
      "(rdflib.term.Literal('Analysis_and_prediction_for_credit_card_fraud_detection_dataset_using_data_mining_approaches'),)\n",
      "(rdflib.term.Literal('Credit_Card_Fraud_Detection_Using_Random_Forest_Classification'),)\n",
      "(rdflib.term.Literal('A_deep_learning-based_quality_assessment_model_of_collaboratively_edited_documents:_A_case_study_of_Wikipedia'),)\n",
      "(rdflib.term.Literal('Deep_learning_in_neural_networks:_an_overview'),)\n",
      "(rdflib.term.Literal('Linking_ImageNet_WordNet_Synsets_with_Wikidata'),)\n",
      "(rdflib.term.Literal('Credit_Card_Fraud_Detection_using_Logistic_Regression_Compared_with_t-SNE_to_Improve_Accuracy'),)\n",
      "(rdflib.term.Literal('An_Efficient_Deep_Learning_Classification_Model_for_Predicting_Credit_Card_Fraud_on_Skewed_Data'),)\n",
      "(rdflib.term.Literal('Artificial_intelligence_machine_learning_computer-aided_diagnosis_and_radiomics:_advances_in_imaging_towards_to_precision_medicine'),)\n",
      "(rdflib.term.Literal('A_widespread_Survey_on_Machine_Learning_Techniques_and_User_Substantiation_Methods_for_Credit_Card_Fraud_Detection'),)\n",
      "(rdflib.term.Literal('The_role_of_artificial_intelligence_in_achieving_the_Sustainable_Development_Goals'),)\n",
      "(rdflib.term.Literal('Credit_Card_Fraud_Detection_using_Python_&amp;_Machine_Learning_Algorithms'),)\n",
      "(rdflib.term.Literal('Artificial_intelligence_and_deep_learning_in_ophthalmology'),)\n",
      "(rdflib.term.Literal('Artificial_intelligence_in_healthcare:_past_present_and_future.'),)\n",
      "(rdflib.term.Literal('Credit_Card_Fraud_Detection_Using_Machine_Learning_and_Blockchain'),)\n",
      "(rdflib.term.Literal('Credit_Card_Fraud_Detection_Using_Machine_Learning_&amp;_Python'),)\n",
      "(rdflib.term.Literal('Applications_of_machine_learning_and_artificial_intelligence_for_Covid-19_(SARS-CoV-2)_pandemic:_A_review'),)\n",
      "(rdflib.term.Literal('Improving_Wikipedia_verifiability_with_AI'),)\n",
      "(rdflib.term.Literal('The_Gene_Wiki:_community_intelligence_applied_to_human_gene_annotation'),)\n",
      "(rdflib.term.Literal('Rows_from_Many_Sources:_Enriching_row_completions_from_Wikidata_with_a_pre-trained_Language_Model'),)\n"
     ]
    }
   ],
   "source": [
    "print(\"All papers:\")\n",
    "print(\"\\n\")\n",
    "\n",
    "for result in g.query(\n",
    "    '''\n",
    "    PREFIX schema: <http://schema.org/>\n",
    "    SELECT ?title\n",
    "    WHERE {\n",
    "        ?paper a schema:paper ;\n",
    "               schema:title ?title .\n",
    "    }\n",
    "    '''\n",
    "):\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "os_group",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/os_group/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial intelligence 10.txt ['Christian Resick', 'Macquarie University', 'Wiley - Macquarie University', 'Council of Australian University Librarians']\n",
      "artificial intelligence 9.txt ['Institute for Life Sciences', 'University of Southampton', 'National Institute for Health Research', 'NIHR', 'Southampton Biomedical Centre', 'NIHR', 'Department of Health and Social Care']\n",
      "artificial intelligence 8.txt ['SP', 'SK']\n",
      "NatureDeepReview.txt ['Natural Sciences and Engineering Research Council of Canada', 'Canadian Institute For Advanced Research', 'CIFAR', 'National Science Foundation', 'Office of Naval Research', 'CIFAR']\n",
      "2014_ICCST_FraudSurrogates_v12rg.txt ['Universitat Politècnica de València', 'Generalitat Valenciana', 'Spanish Administration']\n",
      "Adeeplearning-basedqualityassessmentmodelofcollabora.txt ['Daniel Hasan Dalip', 'Pa ´vel Calado']\n",
      "DeepLearningInNeuralNetworksOverview.JSchmidhuber201.txt ['DL', 'SNF', 'DFG', 'European Commission', 'DL']\n",
      "IJRPR6507.txt ['Saveetha School of Engineering', 'Saveetha Institute of Medical and Technical Sciences', 'Saveetha University']\n",
      "artificial intelligence 4.txt ['KTH Sustainability Office', 'Swedish Research Council', 'Jacobs Foundation', 'Swedish Foundation for Strategic Research', 'Autonomous Systems', 'Leibniz Competition', 'European Union', 'Horizon 2020', 'Research and Innovation', 'Ethics and Governance of AI Fund', 'F. N']\n",
      "artificial intelligence 2.txt ['M. Redi', 'D. Saez - Trumper', 'Wikimedia Foundation', 'R. Nkama', 'Wikipedia', 'FAIR']\n",
      "artificial intelligence 1.txt ['Konrad F. Koehler', 'Jeff Janes', 'Julia Turner', 'Molecular and Cellular Biology W', 'MC']\n"
     ]
    }
   ],
   "source": [
    "acknowledges_dir = '../../papers/acknowledgements'\n",
    "ner_dir = '../../papers/ner'\n",
    "\n",
    "os.makedirs(ner_dir, exist_ok=True)\n",
    "\n",
    "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "results = []\n",
    "\n",
    "nlp_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "\n",
    "for filename in os.listdir(acknowledges_dir):\n",
    "    with open(os.path.join(acknowledges_dir, filename)) as f:\n",
    "        with open(os.path.join(ner_dir, filename), 'w') as f2:\n",
    "            entities = nlp_ner(f.read())\n",
    "            filter_entities = []\n",
    "            for entity in entities:\n",
    "                if entity[\"entity_group\"] == 'MISC' or entity['entity_group'] == 'ORG' or entity['entity_group'] == 'PER':\n",
    "                    if entity['score'] > 0.85 and len(entity['word'])>1:\n",
    "                        filter_entities.append(entity[\"word\"])\n",
    "            f2.write(str(filter_entities))\n",
    "            print(filename, filter_entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "os_group",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

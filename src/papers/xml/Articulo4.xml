<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yifei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrea</forename><surname>Zanette</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiayi</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aviral</forename><surname>Kumar</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google DeepMind</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D1C32795F7AE9CBAF9D0190D4540B926</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2024-04-07T18:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>A broad use case of large language models (LLMs) is in goal-directed decision-making tasks (or &quot;agent&quot; tasks), where an LLM needs to not just generate probable completions for a given prompt, but rather make intelligent decisions over an extended period of multi-turn interaction to accomplish a task (e.g., when interacting with the web, using software tools, or engaging in customer support). Reinforcement learning (RL) provides a general paradigm to address such agent tasks, but current RL methods for LLMs largely focus on single-turn reward maximization. By construction, single-turn RL methods of today cannot actually train LLMs to intelligently seek and incorporate information over multiple turns, perform credit assignment, or reason about their past actions-all of which are critical in agent tasks. This raises the question: how can we design effective and efficient multi-turn RL algorithms for LLMs? In this paper, we propose an algorithmic framework for developing multi-turn RL algorithms for fine-tuning LLMs, that preserves the flexibility of existing single-turn RL methods for LLMs (e.g., proximal policy optimization), while accommodating multiple turns, long horizons, and delayed rewards effectively. To do this, our framework adopts a hierarchical RL approach and runs two RL algorithms in parallel: a high-level off-policy RL algorithm that trains a value function to aggregate reward over utterances, and a low-level RL algorithm that utilizes this high-level value function (in place of a reward model used in single-turn RL) to train a token-by-token policy within each utterance or turn. This hierarchical approach prescribed by our framework, Actor-Critic Framework with a Hierarchical Structure (ArCHer), can also give rise to a number of other RL approaches. Empirically, we find that ArCHer significantly improves efficiency and performance on multi-turn tasks, attaining sample efficiency of about 100x over existing on-policy methods, while also benefitting favorably from scaling up model capacity (upto the 7 billion scale that we could test on in our experiments). Project page can be found in https://yifeizhou02.github.io/archer.io/ and code can be found in https://github.com/YifeiZhou02/ArCHer.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

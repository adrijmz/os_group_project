<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RL-GPT: Integrating Reinforcement Learning and Code-as-policy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shaoteng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoqi</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minda</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanwei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yukang</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shu</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zongqing</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<address>
									<country>Peking University</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Beijing Academy of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Kong</surname></persName>
						</author>
						<title level="a" type="main">RL-GPT: Integrating Reinforcement Learning and Code-as-policy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4B303A8AF0AB0D1037A344086E7F99B0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2024-04-07T18:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>Large Language Models (LLMs) have demonstrated proficiency in utilizing various tools by coding, yet they face limitations in handling intricate logic and precise control. In embodied tasks, high-level planning is amenable to direct coding, while low-level actions often necessitate taskspecific refinement, such as Reinforcement Learning (RL). To seamlessly integrate both modalities, we introduce a twolevel hierarchical framework, RL-GPT, comprising a slow agent and a fast agent. The slow agent analyzes actions suitable for coding, while the fast agent executes coding tasks. This decomposition effectively focuses each agent on specific tasks, proving highly efficient within our pipeline. Our approach outperforms traditional RL methods and existing GPT agents, demonstrating superior efficiency. In the Minecraft game, it rapidly obtains diamonds within a single day on an RTX3090. Additionally, it achieves SOTA performance across all designated MineDojo tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

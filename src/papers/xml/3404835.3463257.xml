<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Krishna</forename><surname>Srinivasan</surname></persName>
							<email>krishnaps@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Karthik</forename><surname>Raman</surname></persName>
							<email>karthikraman@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Jiecao</forename><surname>Chen</surname></persName>
							<email>chenjiecao@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
							<email>najork@google.com</email>
						</author>
						<title level="a" type="main">WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">673041AE4B5DEF98F5A1AB469E844E81</idno>
					<idno type="DOI">10.1145/3404835.3463257</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2024-04-22T21:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems → Specialized information retrieval</term>
					<term>Multimedia and multimodal retrieval</term>
					<term>Image search</term>
					<term>Structure and multilingual text search</term>
					<term>• Computing methodologies → Machine learning machine learning</term>
					<term>neural networks</term>
					<term>multimodal</term>
					<term>multilingual</term>
					<term>imagetext retrieval</term>
					<term>wikipedia</term>
					<term>dataset</term>
				</keywords>
			</textClass>
			<abstract>
				<p>The milestone improvements brought about by deep representation learning and pre-training techniques have led to large performance gains across downstream NLP, IR and Vision tasks. Multimodal modeling techniques aim to leverage large high-quality visio-linguistic datasets for learning complementary information across image and text modalities. In this paper, we introduce the Wikipedia-based Image Text (WIT) Dataset to better facilitate multimodal, multilingual learning. WIT is composed of a curated set of 37.5 million entity rich image-text examples with 11.5 million unique images across 108 Wikipedia languages. Its size enables WIT to be used as a pretraining dataset for multimodal models, as we show when applied to downstream tasks such as image-text retrieval. WIT has four main and unique advantages. First, WIT is the largest multimodal dataset by the number of image-text examples by 3x (at the time of writing). Second, WIT is massively multilingual (first of its kind) with coverage over 100+ languages (each of which has at least 12K examples) and provides cross-lingual texts for many images. Third, WIT represents a more diverse set of concepts and real world entities relative to what previous datasets cover. Lastly, WIT provides a very challenging real-world test set, as we empirically illustrate using an image-text retrieval task as an example. WIT Dataset is available for download and use via a Creative Commons license here: https://github.com/google-research-datasets/wit.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
	</text>
</TEI>

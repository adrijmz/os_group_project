<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning: Methods and Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
							<email>deng@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
							<email>dong.yu@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research One Microsoft Way Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research One Microsoft Way Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">University of Toronto</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<orgName type="institution" key="instit3">University of Montreal</orgName>
								<orgName type="institution" key="instit4">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">UC-Irvine</orgName>
								<orgName type="institution" key="instit2">IDIAP</orgName>
								<orgName type="institution" key="instit3">IDSIA</orgName>
								<orgName type="institution" key="instit4">University College London</orgName>
								<orgName type="institution" key="instit5">University of Michigan</orgName>
								<orgName type="institution" key="instit6">Massachusetts Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning: Methods and Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E5224038CAB815B9D12DA586DB163AA3</idno>
					<idno type="DOI">10.1561/2000000039</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2024-05-13T19:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This monogrph provides an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria in mind: (1) expertise or knowledge of the authors; (2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and (3) the application areas that have the potential to be impacted significantly by deep learning and that have been experiencing research growth, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction 1.Definitions and background</head><p>Since 2006, deep structured learning, or more commonly called deep learning or hierarchical learning, has emerged as a new area of machine learning research <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b150">163]</ref>. During the past several years, the techniques developed from deep learning research have already been impacting a wide range of signal and information processing work within the traditional and the new, widened scopes including key aspects of machine learning and artificial intelligence; see overview articles in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr">77,</ref><ref type="bibr" target="#b81">94,</ref><ref type="bibr" target="#b148">161,</ref><ref type="bibr" target="#b399">412]</ref>, and also the media coverage of this progress in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b224">237]</ref>. A series of workshops, tutorials, and special issues or conference special sessions in recent years have been devoted exclusively to deep learning and its applications to various signal and information processing areas. These include: • 2013 International Conference on Learning Representations;</p><formula xml:id="formula_0">•</formula><p>• 2013 ICML Workshop on Representation Learning Challenges;</p><p>• 2013 ICML Workshop on Deep Learning for Audio, Speech, and Language Processing;</p><p>• 2013 ICASSP Special Session on New Types of Deep Neural Network Learning for Speech Recognition and Related Applications.</p><p>The authors have been actively involved in deep learning research and in organizing or providing several of the above events, tutorials, and editorials. In particular, they gave tutorials and invited lectures on this topic at various places. Part of this monograph is based on their tutorials and lecture material. Before embarking on describing details of deep learning, let's provide necessary definitions. Deep learning has various closely related definitions or high-level descriptions:</p><p>• Definition 1 : A class of machine learning techniques that exploit many layers of non-linear information processing for Introduction supervised or unsupervised feature extraction and transformation, and for pattern analysis and classification. • Definition 2 : "A sub-field within machine learning that is based on algorithms for learning multiple levels of representation in order to model complex relationships among data. Higher-level features and concepts are thus defined in terms of lower-level ones, and such a hierarchy of features is called a deep architecture. Most of these models are based on unsupervised learning of representations." (Wikipedia on "Deep Learning" around March 2012.)</p><p>• Definition 3 : "A sub-field of machine learning that is based on learning several levels of representations, corresponding to a hierarchy of features or factors or concepts, where higher-level concepts are defined from lower-level ones, and the same lowerlevel concepts can help to define many higher-level concepts. Deep learning is part of a broader family of machine learning methods based on learning representations. An observation (e.g., an image) can be represented in many ways (e.g., a vector of pixels), but some representations make it easier to learn tasks of interest (e.g., is this the image of a human face?) from examples, and research in this area attempts to define what makes better representations and how to learn them." (Wikipedia on "Deep Learning" around February 2013.)</p><p>• Definition 4 : "Deep learning is a set of algorithms in machine learning that attempt to learn in multiple levels, corresponding to different levels of abstraction. It typically uses artificial neural networks. The levels in these learned statistical models correspond to distinct levels of concepts, where higher-level concepts are defined from lower-level ones, and the same lowerlevel concepts can help to define many higher-level concepts." See Wikipedia http://en.wikipedia.org/wiki/Deep_learning on "Deep Learning" as of this most recent update in October 2013.</p><p>• Definition 5 : "Deep Learning is a new area of Machine Learning research, which has been introduced with the objective of moving Machine Learning closer to one of its original goals: Artificial</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Technology, University of Washington, and numerous other places; see http://deeplearning.net/deep-learning-research-groups-and-labs/ for a more detailed list. These researchers have demonstrated empirical successes of deep learning in diverse applications of computer vision, phonetic recognition, voice search, conversational speech recognition, speech and image feature coding, semantic utterance classification, natural language understanding, hand-writing recognition, audio processing, information retrieval, robotics, and even in the analysis of molecules that may lead to discovery of new drugs as reported recently by <ref type="bibr" target="#b224">[237]</ref>.</p><p>In addition to the reference list provided at the end of this monograph, which may be outdated not long after the publication of this monograph, there are a number of excellent and frequently updated reading lists, tutorials, software, and video lectures online at:</p><p>• http://deeplearning.net/reading-list/</p><p>• http://ufldl.stanford.edu/wiki/index.php/ UFLDL_Recommended_Readings</p><p>• http://www.cs.toronto.edu/∼hinton/</p><p>• http://deeplearning.net/tutorial/</p><p>• http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Organization of this monograph</head><p>The rest of the monograph is organized as follows:</p><p>In Section 2, we provide a brief historical account of deep learning, mainly from the perspective of how speech recognition technology has been hugely impacted by deep learning, and how the revolution got started and has gained and sustained immense momentum.</p><p>In Section 3, a three-way categorization scheme for a majority of the work in deep learning is developed. They include unsupervised, supervised, and hybrid deep learning networks, where in the latter category unsupervised learning (or pre-training) is exploited to assist the subsequent stage of supervised learning when the final tasks pertain to classification. The supervised and hybrid deep networks often have the same type of architectures or the structures in the deep networks, but the unsupervised deep networks tend to have different architectures from the others.</p><p>Sections 4-6 are devoted, respectively, to three popular types of deep architectures, one from each of the classes in the three-way categorization scheme reviewed in Section 3. In Section 4, we discuss in detail deep autoencoders as a prominent example of the unsupervised deep learning networks. No class labels are used in the learning, although supervised learning methods such as back-propagation are cleverly exploited when the input signal itself, instead of any label information of interest to possible classification tasks, is treated as the "supervision" signal.</p><p>In Section 5, as a major example in the hybrid deep network category, we present in detail the deep neural networks with unsupervised and largely generative pre-training to boost the effectiveness of supervised training. This benefit is found critical when the training data are limited and no other appropriate regularization approaches (i.e., dropout) are exploited. The particular pre-training method based on restricted Boltzmann machines and the related deep belief networks described in this section has been historically significant as it ignited the intense interest in the early applications of deep learning to speech recognition and other information processing tasks. In addition to this retrospective review, subsequent development and different paths from the more recent perspective are discussed.</p><p>In Section 6, the basic deep stacking networks and their several extensions are discussed in detail, which exemplify the discriminative, supervised deep learning networks in the three-way classification scheme. This group of deep networks operate in many ways that are distinct from the deep neural networks. Most notably, they use target labels in constructing each of many layers or modules in the overall deep networks. Assumptions made about part of the networks, such as linear output units in each of the modules, simplify the learning algorithms and enable a much wider variety of network architectures to be constructed and learned than the networks discussed in Sections 4 and 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In Sections 7-11, we select a set of typical and successful applications of deep learning in diverse areas of signal and information processing. In Section 7, we review the applications of deep learning to speech recognition, speech synthesis, and audio processing. Subsections surrounding the main subject of speech recognition are created based on several prominent themes on the topic in the literature.</p><p>In Section 8, we present recent results of applying deep learning to language modeling and natural language processing, where we highlight the key recent development in embedding symbolic entities such as words into low-dimensional, continuous-valued vectors.</p><p>Section 9 is devoted to selected applications of deep learning to information retrieval including web search.</p><p>In Section 10, we cover selected applications of deep learning to image object recognition in computer vision. The section is divided to two main classes of deep learning approaches: (1) unsupervised feature learning, and (2) supervised learning for end-to-end and joint feature learning and classification.</p><p>Selected applications to multi-modal processing and multi-task learning are reviewed in Section 11, divided into three categories according to the nature of the multi-modal data as inputs to the deep learning systems. For single-modality data of speech, text, or image, a number of recent multi-task learning studies based on deep learning methods are reviewed in the literature.</p><p>Finally, conclusions are given in Section 12 to summarize the monograph and to discuss future challenges and directions.</p><p>This short monograph contains the material expanded from two tutorials that the authors gave, one at APSIPA in October 2011 and the other at ICASSP in March 2012. Substantial updates have been made based on the literature up to January 2014 (including the materials presented at NIPS-2013 and at IEEE-ASRU-2013 both held in December of 2013), focusing on practical aspects in the fast development of deep learning research and technology during the interim years.</p><p>Until recently, most machine learning and signal processing techniques had exploited shallow-structured architectures. These architectures typically contain at most one or two layers of nonlinear feature transformations. Examples of the shallow architectures are Gaussian mixture models (GMMs), linear or nonlinear dynamical systems, conditional random fields (CRFs), maximum entropy (MaxEnt) models, support vector machines (SVMs), logistic regression, kernel regression, multilayer perceptrons (MLPs) with a single hidden layer including extreme learning machines (ELMs). For instance, SVMs use a shallow linear pattern separation model with one or zero feature transformation layer when the kernel trick is used or otherwise. (Notable exceptions are the recent kernel methods that have been inspired by and integrated with deep learning; e.g. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b89">102,</ref><ref type="bibr" target="#b364">377]</ref>). Shallow architectures have been shown effective in solving many simple or well-constrained problems, but their limited modeling and representational power can cause difficulties when dealing with more complicated real-world applications involving natural signals such as human speech, natural sound and language, and natural image and visual scenes.</p><p>Human information processing mechanisms (e.g., vision and audition), however, suggest the need of deep architectures for extracting complex structure and building internal representation from rich sensory inputs. For example, human speech production and perception systems are both equipped with clearly layered hierarchical structures in transforming the information from the waveform level to the linguistic level <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr">74,</ref><ref type="bibr">75]</ref>. In a similar vein, the human visual system is also hierarchical in nature, mostly in the perception side but interestingly also in the "generation" side <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b113">126,</ref><ref type="bibr" target="#b274">287]</ref>). It is natural to believe that the state-of-the-art can be advanced in processing these types of natural signals if efficient and effective deep learning algorithms can be developed.</p><p>Historically, the concept of deep learning originated from artificial neural network research. (Hence, one may occasionally hear the discussion of "new-generation neural networks.") Feed-forward neural networks or MLPs with many hidden layers, which are often referred to as deep neural networks (DNNs), are good examples of the models with a deep architecture. Back-propagation (BP), popularized in 1980s, has been a well-known algorithm for learning the parameters of these networks. Unfortunately BP alone did not work well in practice then for learning networks with more than a small number of hidden layers (see a review and analysis in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b116">129]</ref>. The pervasive presence of local optima and other optimization challenges in the non-convex objective function of the deep networks are the main source of difficulties in the learning. BP is based on local gradient information, and starts usually at some random initial points. It often gets trapped in poor local optima when the batch-mode or even stochastic gradient descent BP algorithm is used. The severity increases significantly as the depth of the networks increases. This difficulty is partially responsible for steering away most of the machine learning and signal processing research from neural networks to shallow models that have convex loss functions (e.g., SVMs, CRFs, and MaxEnt models), for which the global optimum can be efficiently obtained at the cost of reduced modeling power, although there had been continuing work on neural networks with limited scale and impact (e.g., <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b74">87,</ref><ref type="bibr" target="#b155">168,</ref><ref type="bibr" target="#b199">212,</ref><ref type="bibr" target="#b250">263,</ref><ref type="bibr" target="#b291">304]</ref>.</p><p>The optimization difficulty associated with the deep models was empirically alleviated when a reasonably efficient, unsupervised learning algorithm was introduced in the two seminar papers <ref type="bibr" target="#b150">[163,</ref><ref type="bibr" target="#b151">164]</ref>. In these papers, a class of deep generative models, called deep belief network (DBN), was introduced. A DBN is composed of a stack of restricted Boltzmann machines (RBMs). A core component of the DBN is a greedy, layer-by-layer learning algorithm which optimizes DBN weights at time complexity linear to the size and depth of the networks. Separately and with some surprise, initializing the weights of an MLP with a correspondingly configured DBN often produces much better results than that with the random weights. As such, MLPs with many hidden layers, or deep neural networks (DNN), which are learned with unsupervised DBN pre-training followed by back-propagation fine-tuning is sometimes also called DBNs in the literature <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b247">260,</ref><ref type="bibr" target="#b245">258]</ref>. More recently, researchers have been more careful in distinguishing DNNs from DBNs <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b148">161]</ref>, and when DBN is used to initialize the training of a DNN, the resulting network is sometimes called the DBN-DNN <ref type="bibr" target="#b148">[161]</ref>.</p><p>Independently of the RBM development, in 2006 two alternative, non-probabilistic, non-generative, unsupervised deep models were published. One is an autoencoder variant with greedy layer-wise training much like the DBN training <ref type="bibr" target="#b27">[28]</ref>. Another is an energy-based model with unsupervised learning of sparse over-complete representations <ref type="bibr" target="#b284">[297]</ref>. They both can be effectively used to pre-train a deep neural network, much like the DBN.</p><p>In addition to the supply of good initialization points, the DBN comes with other attractive properties. First, the learning algorithm makes effective use of unlabeled data. Second, it can be interpreted as a probabilistic generative model. Third, the over-fitting problem, which is often observed in the models with millions of parameters such as DBNs, and the under-fitting problem, which occurs often in deep networks, can be effectively alleviated by the generative pre-training step. An insightful analysis on what kinds of speech information DBNs can capture is provided in <ref type="bibr" target="#b246">[259]</ref>.</p><p>Using hidden layers with many neurons in a DNN significantly improves the modeling power of the DNN and creates many closely optimal configurations. Even if parameter learning is trapped into a local optimum, the resulting DNN can still perform quite well since the chance of having a poor local optimum is lower than when a small number of neurons are used in the network. Using deep and wide neural networks, however, would cast great demand to the computational power during the training process and this is one of the reasons why it is not until recent years that researchers have started exploring both deep and wide neural networks in a serious manner.</p><p>Better learning algorithms and different nonlinearities also contributed to the success of DNNs. Stochastic gradient descend (SGD) algorithms are the most efficient algorithm when the training set is large and redundant as is the case for most applications <ref type="bibr" target="#b38">[39]</ref>. Recently, SGD is shown to be effective for parallelizing over many machines with an asynchronous mode <ref type="bibr" target="#b68">[69]</ref> or over multiple GPUs through pipelined BP <ref type="bibr" target="#b48">[49]</ref>. Further, SGD can often allow the training to jump out of local optima due to the noisy gradients estimated from a single or a small batch of samples. Other learning algorithms such as Hessian free <ref type="bibr" target="#b182">[195,</ref><ref type="bibr" target="#b225">238]</ref> or Krylov subspace methods <ref type="bibr" target="#b365">[378]</ref> have shown a similar ability.</p><p>For the highly non-convex optimization problem of DNN learning, it is obvious that better parameter initialization techniques will lead to better models since optimization starts from these initial models. What was not obvious, however, is how to efficiently and effectively initialize DNN parameters and how the use of large amounts of training data can alleviate the learning problem until more recently <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b87">100,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b150">163,</ref><ref type="bibr" target="#b151">164,</ref><ref type="bibr" target="#b148">161,</ref><ref type="bibr" target="#b310">323,</ref><ref type="bibr" target="#b363">376,</ref><ref type="bibr" target="#b401">414]</ref>. The DNN parameter initialization technique that attracted the most attention is the unsupervised pretraining technique proposed in <ref type="bibr" target="#b150">[163,</ref><ref type="bibr" target="#b151">164]</ref> discussed earlier.</p><p>The DBN pretraining procedure is not the only one that allows effective initialization of DNNs. An alternative unsupervised approach that performs equally well is to pretrain DNNs layer by layer by considering each pair of layers as a de-noising autoencoder regularized by setting a random subset of the input nodes to zero <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b363">376]</ref>. Another alternative is to use contractive autoencoders for the same purpose by favoring representations that are more robust to the input variations, i.e., penalizing the gradient of the activities of the hidden units with respect to the inputs <ref type="bibr" target="#b290">[303]</ref>. Further, Ranzato et al. <ref type="bibr" target="#b281">[294]</ref> developed the sparse encoding symmetric machine (SESM), which has a very similar architecture to RBMs as building blocks of a DBN. The SESM may also be used to effectively initialize the DNN training. In addition to unsupervised pretraining using greedy layer-wise procedures <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b151">164,</ref><ref type="bibr" target="#b282">295]</ref>, the supervised pretraining, or sometimes called discriminative pretraining, has also been shown to be effective <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b148">161,</ref><ref type="bibr" target="#b311">324,</ref><ref type="bibr" target="#b419">432]</ref> and in cases where labeled training data are abundant performs better than the unsupervised pretraining techniques. The idea of the discriminative pretraining is to start from a one-hidden-layer MLP trained with the BP algorithm. Every time when we want to add a new hidden layer we replace the output layer with a randomly initialized new hidden and output layer and train the whole new MLP (or DNN) using the BP algorithm. Different from the unsupervised pretraining techniques, the discriminative pretraining technique requires labels.</p><p>Researchers who apply deep learning to speech and vision analyzed what DNNs capture in speech and images. For example, <ref type="bibr" target="#b246">[259]</ref> applied a dimensionality reduction method to visualize the relationship among the feature vectors learned by the DNN. They found that the DNN's hidden activity vectors preserve the similarity structure of the feature vectors at multiple scales, and that this is especially true for the filterbank features. A more elaborated visualization method, based on a top-down generative process in the reverse direction of the classification network, was recently developed by Zeiler and Fergus <ref type="bibr" target="#b423">[436]</ref> for examining what features the deep convolutional networks capture from the image data. The power of the deep networks is shown to be their ability to extract appropriate features and do discrimination jointly <ref type="bibr" target="#b197">[210]</ref>.</p><p>As another way to concisely introduce the DNN, we can review the history of artificial neural networks using a "hype cycle," which is a graphic representation of the maturity, adoption and social application of specific technologies. The 2012 version of the hype cycles graph compiled by Gartner is shown in Figure <ref type="figure">2</ref>.1. It intends to show how a technology or application will evolve over time (according to five phases: technology trigger, peak of inflated expectations, trough of disillusionment, slope of enlightenment, and plateau of production), and to provide a source of insight to manage its deployment. Applying the Gartner hyper cycle to the artificial neural network development, we created Figure <ref type="figure">2</ref>.2 to align different generations of the neural network with the various phases designated in the hype cycle. The peak activities ("expectations" or "media hype" on the vertical axis) occurred in late 1980s and early 1990s, corresponding to the height of what is often referred to as the "second generation" of neural networks. The deep belief network (DBN) and a fast algorithm for training it were invented in 2006 <ref type="bibr" target="#b150">[163,</ref><ref type="bibr" target="#b151">164]</ref>. When the DBN was used to initialize the DNN, the learning became highly effective and this has inspired the subsequent fast growing research ("enlightenment" phase shown in Figure <ref type="figure">2</ref>.2). Applications of the DBN and DNN to industryscale speech feature extraction and speech recognition started in 2009 when leading academic and industrial researchers with both deep learning and speech expertise collaborated; see reviews in <ref type="bibr" target="#b76">[89,</ref><ref type="bibr" target="#b148">161]</ref>. This collaboration fast expanded the work of speech recognition using deep learning methods to increasingly larger successes <ref type="bibr" target="#b81">[94,</ref><ref type="bibr" target="#b148">161,</ref><ref type="bibr" target="#b310">323,</ref><ref type="bibr" target="#b401">414]</ref>, many of which will be covered in the remainder of this monograph. The height of the "plateau of productivity" phase, not yet reached in our opinion, is expected to be higher than that in the stereotypical curve (circled with a question mark in Figure <ref type="figure">2</ref>.2), and is marked by the dashed line that moves straight up.</p><p>We show in Figure <ref type="figure">2</ref>.3 the history of speech recognition, which has been compiled by NIST, organized by plotting the word error rate (WER) as a function of time for a number of increasingly difficult speech recognition tasks. Note all WER results were obtained using the GMM-HMM technology. When one particularly difficult task (Switchboard) is extracted from Figure <ref type="figure">2</ref>.3, we see a flat curve over many years using the GMM-HMM technology but after the DNN technology is used the WER drops sharply (marked by the red star in Figure <ref type="figure">2</ref>.4). In the next section, an overview is provided on the various architectures of deep learning, followed by more detailed expositions of a few widely studied architectures and methods and by selected applications in signal and information processing including speech and audio, natural language, information retrieval, vision, and multi-modal processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Three Classes of Deep Learning Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A three-way categorization</head><p>As described earlier, deep learning refers to a rather wide class of machine learning techniques and architectures, with the hallmark of using many layers of non-linear information processing that are hierarchical in nature. Depending on how the architectures and techniques are intended for use, e.g., synthesis/generation or recognition/ classification, one can broadly categorize most of the work in this area into three major classes:</p><p>1. Deep networks for unsupervised or generative learning, which are intended to capture high-order correlation of the observed or visible data for pattern analysis or synthesis purposes when no information about target class labels is available. Unsupervised feature or representation learning in the literature refers to this category of the deep networks. When used in the generative mode, may also be intended to characterize joint statistical distributions of the visible data and their associated classes when available and being treated as part of the visible data. In the latter case, the use of Bayes rule can turn this type of generative networks into a discriminative one for learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Deep networks for supervised learning, which are intended to directly provide discriminative power for pattern classification purposes, often by characterizing the posterior distributions of classes conditioned on the visible data. Target label data are always available in direct or indirect forms for such supervised learning. They are also called discriminative deep networks.</p><p>3. Hybrid deep networks, where the goal is discrimination which is assisted, often in a significant way, with the outcomes of generative or unsupervised deep networks. This can be accomplished by better optimization or/and regularization of the deep networks in category <ref type="bibr" target="#b1">(2)</ref>. The goal can also be accomplished when discriminative criteria for supervised learning are used to estimate the parameters in any of the deep generative or unsupervised deep networks in category (1) above.</p><p>Note the use of "hybrid" in (3) above is different from that used sometimes in the literature, which refers to the hybrid systems for speech recognition feeding the output probabilities of a neural network into an HMM <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b248">261]</ref>. By the commonly adopted machine learning tradition (e.g., Chapter 28 in <ref type="bibr" target="#b251">[264]</ref>, and Reference <ref type="bibr" target="#b82">[95]</ref>, it may be natural to just classify deep learning techniques into deep discriminative models (e.g., deep neural networks or DNNs, recurrent neural networks or RNNs, convolutional neural networks or CNNs, etc.) and generative/unsupervised models (e.g., restricted Boltzmann machine or RBMs, deep belief networks or DBNs, deep Boltzmann machines (DBMs), regularized autoencoders, etc.). This two-way classification scheme, however, misses a key insight gained in deep learning research about how generative or unsupervised-learning models can greatly improve the training of DNNs and other deep discriminative or supervised-learning models via better regularization or optimization. Also, deep networks for unsupervised learning may not necessarily need to be probabilistic or be able to meaningfully sample from the model (e.g., traditional autoencoders, sparse coding networks, etc.). We note here that more recent studies have generalized the traditional denoising autoencoders so that they can be efficiently sampled from and thus have become generative models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30]</ref>. Nevertheless, the traditional two-way classification indeed points to several key differences between deep networks for unsupervised and supervised learning. Compared between the two, deep supervised-learning models such as DNNs are usually more efficient to train and test, more flexible to construct, and more suitable for end-to-end learning of complex systems (e.g., no approximate inference and learning such as loopy belief propagation). On the other hand, the deep unsupervised-learning models, especially the probabilistic generative ones, are easier to interpret, easier to embed domain knowledge, easier to compose, and easier to handle uncertainty, but they are typically intractable in inference and learning for complex systems. These distinctions are retained also in the proposed three-way classification which is hence adopted throughout this monograph.</p><p>Below we review representative work in each of the above three categories, where several basic definitions are summarized in Table <ref type="table">3</ref>.1. Applications of these deep architectures, with varied ways of learning including supervised, unsupervised, or hybrid, are deferred to Sections 7-11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep networks for unsupervised or generative learning</head><p>Unsupervised learning refers to no use of task specific supervision information (e.g., target class labels) in the learning process. Many deep networks in this category can be used to meaningfully generate samples by sampling from the networks, with examples of RBMs, DBNs, DBMs, and generalized denoising autoencoders <ref type="bibr" target="#b22">[23]</ref>, and are thus generative models. Some networks in this category, however, cannot be easily sampled, with examples of sparse coding networks and the original forms of deep autoencoders, and are thus not generative in nature.</p><p>Among the various subclasses of generative or unsupervised deep networks, the energy-based deep models are the most common <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b200">213,</ref><ref type="bibr" target="#b255">268]</ref>. The original form of the deep autoencoder <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b87">100,</ref><ref type="bibr" target="#b151">164]</ref>, which we will give more detail about in Section 4, is a typical example Deep Learning: a class of machine learning techniques, where many layers of information processing stages in hierarchical supervised architectures are exploited for unsupervised feature learning and for pattern analysis/classification. The essence of deep learning is to compute hierarchical features or representations of the observational data, where the higher-level features or factors are defined from lower-level ones. The family of deep learning methods have been growing increasingly richer, encompassing those of neural networks, hierarchical probabilistic models, and a variety of unsupervised and supervised feature learning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep belief network (DBN):</head><p>probabilistic generative models composed of multiple layers of stochastic, hidden variables. The top two layers have undirected, symmetric connections between them. The lower layers receive top-down, directed connections from the layer above.</p><p>Boltzmann machine (BM): a network of symmetrically connected, neuron-like units that make stochastic decisions about whether to be on or off.</p><p>Restricted Boltzmann machine (RBM): a special type of BM consisting of a layer of visible units and a layer of hidden units with no visible-visible or hidden-hidden connections.</p><p>Deep neural network (DNN): a multilayer perceptron with many hidden layers, whose weights are fully connected and are often (although not always) initialized using either an unsupervised or a supervised pretraining technique. (In the literature prior to 2012, a DBN was often used incorrectly to mean a DNN.)</p><p>Deep autoencoder: a "discriminative" DNN whose output targets are the data input itself rather than class labels; hence an unsupervised learning model. When trained with a denoising criterion, a deep autoencoder is also a generative model and can be sampled from.</p><p>(Continued) Distributed representation: an internal representation of the observed data in such a way that they are modeled as being explained by the interactions of many hidden factors. A particular factor learned from configurations of other factors can often generalize well to new configurations. Distributed representations naturally occur in a "connectionist" neural network, where a concept is represented by a pattern of activity across a number of units and where at the same time a unit typically contributes to many concepts. One key advantage of such many-to-many correspondence is that they provide robustness in representing the internal structure of the data in terms of graceful degradation and damage resistance. Another key advantage is that they facilitate generalizations of concepts and relations, thus enabling reasoning abilities. of this unsupervised model category. Most other forms of deep autoencoders are also unsupervised in nature, but with quite different properties and implementations. Examples are transforming autoencoders <ref type="bibr" target="#b147">[160]</ref>, predictive sparse coders and their stacked version, and de-noising autoencoders and their stacked versions <ref type="bibr" target="#b363">[376]</ref>.</p><p>Specifically, in de-noising autoencoders, the input vectors are first corrupted by, for example, randomly selecting a percentage of the inputs and setting them to zeros or adding Gaussian noise to them. Then the parameters are adjusted for the hidden encoding nodes to reconstruct the original, uncorrupted input data using criteria such as mean square reconstruction error and KL divergence between the original inputs and the reconstructed inputs. The encoded representations transformed from the uncorrupted data are used as the inputs to the next level of the stacked de-noising autoencoder.</p><p>Another prominent type of deep unsupervised models with generative capability is the deep Boltzmann machine or DBM <ref type="bibr" target="#b118">[131,</ref><ref type="bibr" target="#b302">315,</ref><ref type="bibr" target="#b303">316,</ref><ref type="bibr" target="#b335">348]</ref>. A DBM contains many layers of hidden variables, and has no connections between the variables within the same layer. This is a special case of the general Boltzmann machine (BM), which is a network of symmetrically connected units that are on or off based on a stochastic mechanism. While having a simple learning algorithm, the general BMs are very complex to study and very slow to train. In a DBM, each layer captures complicated, higher-order correlations between the activities of hidden features in the layer below. DBMs have the potential of learning internal representations that become increasingly complex, highly desirable for solving object and speech recognition problems. Further, the high-level representations can be built from a large supply of unlabeled sensory inputs and very limited labeled data can then be used to only slightly fine-tune the model for a specific task at hand.</p><p>When the number of hidden layers of DBM is reduced to one, we have restricted Boltzmann machine (RBM). Like DBM, there are no hidden-to-hidden and no visible-to-visible connections in the RBM. The main virtue of RBM is that via composing many RBMs, many hidden layers can be learned efficiently using the feature activations of one RBM as the training data for the next. Such composition leads to deep belief network (DBN), which we will describe in more detail, together with RBMs, in Section 5.</p><p>The standard DBN has been extended to the factored higher-order Boltzmann machine in its bottom layer, with strong results obtained for phone recognition <ref type="bibr" target="#b63">[64]</ref> and for computer vision <ref type="bibr" target="#b283">[296]</ref>. This model, called the mean-covariance RBM or mcRBM, recognizes the limitation of the standard RBM in its ability to represent the covariance structure of the data. However, it is difficult to train mcRBMs and to use them at the higher levels of the deep architecture. Further, the strong results published are not easy to reproduce. In the architecture described by Dahl et al. <ref type="bibr" target="#b63">[64]</ref>, the mcRBM parameters in the full DBN are not finetuned using the discriminative information, which is used for fine tuning the higher layers of RBMs, due to the high computational cost. Subsequent work showed that when speaker adapted features are used, which remove more variability in the features, mcRBM was not helpful <ref type="bibr" target="#b246">[259]</ref>.</p><p>Another representative deep generative network that can be used for unsupervised (as well as supervised) learning is the sum-product network or SPN <ref type="bibr" target="#b112">[125,</ref><ref type="bibr" target="#b276">289]</ref>. An SPN is a directed acyclic graph with the observed variables as leaves, and with sum and product operations as internal nodes in the deep network. The "sum" nodes give mixture models, and the "product" nodes build up the feature hierarchy. Properties of "completeness" and "consistency" constrain the SPN in a desirable way. The learning of SPNs is carried out using the EM algorithm together with back-propagation. The learning procedure starts with a dense SPN. It then finds an SPN structure by learning its weights, where zero weights indicate removed connections. The main difficulty in learning SPNs is that the learning signal (i.e., the gradient) quickly dilutes when it propagates to deep layers. Empirical solutions have been found to mitigate this difficulty as reported in <ref type="bibr" target="#b276">[289]</ref>. It was pointed out in that early paper that despite the many desirable generative properties in the SPN, it is difficult to fine tune the parameters using the discriminative information, limiting its effectiveness in classification tasks. However, this difficulty has been overcome in the subsequent work reported in <ref type="bibr" target="#b112">[125]</ref>, where an efficient BP-style discriminative training algorithm for SPN was presented. Importantly, the standard gradient descent, based on the derivative of the conditional likelihood, suffers from the same gradient diffusion problem well known in the regular DNNs. The trick to alleviate this problem in learning SPNs is to replace the marginal inference with the most probable state of the hidden variables and to propagate gradients through this "hard" alignment only. Excellent results on small-scale image recognition tasks were reported by Gens and Domingo <ref type="bibr" target="#b112">[125]</ref>.</p><p>Recurrent neural networks (RNNs) can be considered as another class of deep networks for unsupervised (as well as supervised) learning, where the depth can be as large as the length of the input data sequence. In the unsupervised learning mode, the RNN is used to predict the data sequence in the future using the previous data samples, and no additional class information is used for learning. The RNN is very powerful for modeling sequence data (e.g., speech or text), but until recently they had not been widely used partly because they are difficult to train to capture long-term dependencies, giving rise to gradient vanishing or gradient explosion problems which were known in early 1990s <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b154">167]</ref>. These problems can now be dealt with more easily <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b72">85,</ref><ref type="bibr" target="#b267">280]</ref>. Recent advances in Hessian-free optimization <ref type="bibr" target="#b225">[238]</ref> have also partially overcome this difficulty using approximated second-order information or stochastic curvature estimates. In the more recent work <ref type="bibr" target="#b226">[239]</ref>, RNNs that are trained with Hessian-free optimization are used as a generative deep network in the character-level language modeling tasks, where gated connections are introduced to allow the current input characters to predict the transition from one latent state vector to the next. Such generative RNN models are demonstrated to be well capable of generating sequential text characters. More recently, Bengio et al. <ref type="bibr" target="#b21">[22]</ref> and Sutskever <ref type="bibr" target="#b343">[356]</ref> have explored variations of stochastic gradient descent optimization algorithms in training generative RNNs and shown that these algorithms can outperform Hessian-free optimization methods. Mikolov et al. <ref type="bibr" target="#b235">[248]</ref> have reported excellent results on using RNNs for language modeling. Most recently, Mesnil et al. <ref type="bibr" target="#b229">[242]</ref> and Yao et al. <ref type="bibr" target="#b390">[403]</ref> reported the success of RNNs in spoken language understanding. We will review this set of work in Section 8.</p><p>There has been a long history in speech recognition research where human speech production mechanisms are exploited to construct dynamic and deep structure in probabilistic generative models; for a comprehensive review, see the monograph by <ref type="bibr">Deng [76]</ref>. Specifically, the early work described in <ref type="bibr">[71,</ref><ref type="bibr">72,</ref><ref type="bibr" target="#b70">83,</ref><ref type="bibr" target="#b71">84,</ref><ref type="bibr" target="#b86">99,</ref><ref type="bibr" target="#b261">274]</ref> generalized and extended the conventional shallow and conditionally independent HMM structure by imposing dynamic constraints, in the form of polynomial trajectory, on the HMM parameters. A variant of this approach has been more recently developed using different learning techniques for time-varying HMM parameters and with the applications extended to speech recognition robustness <ref type="bibr" target="#b418">[431,</ref><ref type="bibr" target="#b403">416]</ref>. Similar trajectory HMMs also form the basis for parametric speech synthesis <ref type="bibr" target="#b215">[228,</ref><ref type="bibr" target="#b313">326,</ref><ref type="bibr" target="#b426">439,</ref><ref type="bibr" target="#b425">438]</ref>. Subsequent work added a new hidden layer into the dynamic model to explicitly account for the target-directed, articulatory-like properties in human speech generation <ref type="bibr" target="#b44">[45,</ref><ref type="bibr">73,</ref><ref type="bibr">74,</ref><ref type="bibr" target="#b70">83,</ref><ref type="bibr" target="#b83">96,</ref><ref type="bibr">75,</ref><ref type="bibr" target="#b77">90,</ref><ref type="bibr" target="#b218">231,</ref><ref type="bibr" target="#b219">232,</ref><ref type="bibr" target="#b220">233,</ref><ref type="bibr" target="#b238">251,</ref><ref type="bibr" target="#b269">282]</ref>. More efficient implementation of this deep architecture with hidden dynamics is achieved with non-recursive or finite impulse response (FIR) filters in more recent studies <ref type="bibr">[76,</ref><ref type="bibr" target="#b94">107,</ref><ref type="bibr" target="#b92">105]</ref>. The above deepstructured generative models of speech can be shown as special cases of the more general dynamic network model and even more general dynamic graphical models <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b33">34]</ref>. The graphical models can comprise many hidden layers to characterize the complex relationship between the variables in speech generation. Armed with powerful graphical modeling tool, the deep architecture of speech has more recently been successfully applied to solve the very difficult problem of single-channel, multi-talker speech recognition, where the mixed speech is the visible variable while the un-mixed speech becomes represented in a new hidden layer in the deep generative architecture <ref type="bibr" target="#b288">[301,</ref><ref type="bibr" target="#b378">391]</ref>. Deep generative graphical models are indeed a powerful tool in many applications due to their capability of embedding domain knowledge. However, they are often used with inappropriate approximations in inference, learning, prediction, and topology design, all arising from inherent intractability in these tasks for most real-world applications. This problem has been addressed in the recent work of Stoyanov et al. <ref type="bibr" target="#b339">[352]</ref>, which provides an interesting direction for making deep generative graphical models potentially more useful in practice in the future. An even more drastic way to deal with this intractability was proposed recently by Bengio et al. <ref type="bibr" target="#b29">[30]</ref>, where the need to marginalize latent variables is avoided altogether.</p><p>The standard statistical methods used for large-scale speech recognition and understanding combine (shallow) hidden Markov models for speech acoustics with higher layers of structure representing different levels of natural language hierarchy. This combined hierarchical model can be suitably regarded as a deep generative architecture, whose motivation and some technical detail may be found in Section 7 of the recent monograph <ref type="bibr" target="#b187">[200]</ref> on "Hierarchical HMM" or HHMM. Related models with greater technical depth and mathematical treatment can be found in <ref type="bibr" target="#b103">[116]</ref> for HHMM and <ref type="bibr" target="#b258">[271]</ref> for Layered HMM. These early deep models were formulated as directed graphical models, missing the key aspect of "distributed representation" embodied in the more recent deep generative networks of the DBN and DBM discussed earlier in this chapter. Filling in this missing aspect would help improve these generative models.</p><p>Finally, dynamic or temporally recursive generative models based on neural network architectures can be found in <ref type="bibr" target="#b348">[361]</ref> for human motion modeling, and in <ref type="bibr" target="#b331">[344,</ref><ref type="bibr" target="#b326">339]</ref> for natural language and natural scene parsing. The latter model is particularly interesting because the learning algorithms are capable of automatically determining the optimal model structure. This contrasts with other deep architectures such as DBN where only the parameters are learned while the architectures need to be pre-defined. Specifically, as reported in <ref type="bibr" target="#b331">[344]</ref>, the recursive structure commonly found in natural scene images and in natural language sentences can be discovered using a max-margin structure prediction architecture. It is shown that the units contained in the images or sentences are identified, and the way in which these units interact with each other to form the whole is also identified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Deep networks for supervised learning</head><p>Many of the discriminative techniques for supervised learning in signal and information processing are shallow architectures such as HMMs <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b114">127,</ref><ref type="bibr" target="#b134">147,</ref><ref type="bibr" target="#b173">186,</ref><ref type="bibr" target="#b175">188,</ref><ref type="bibr" target="#b277">290,</ref><ref type="bibr" target="#b381">394,</ref><ref type="bibr" target="#b405">418]</ref> and conditional random fields (CRFs) <ref type="bibr" target="#b138">[151,</ref><ref type="bibr" target="#b142">155,</ref><ref type="bibr" target="#b268">281,</ref><ref type="bibr" target="#b387">400,</ref><ref type="bibr" target="#b416">429,</ref><ref type="bibr" target="#b433">446]</ref>. A CRF is intrinsically a shallow discriminative architecture, characterized by the linear relationship between the input features and the transition features. The shallow nature of the CRF is made most clear by the equivalence established between the CRF and the discriminatively trained Gaussian models and HMMs <ref type="bibr" target="#b135">[148]</ref>. More recently, deep-structured CRFs have been developed by stacking the output in each lower layer of the CRF, together with the original input data, onto its higher layer <ref type="bibr" target="#b415">[428]</ref>. Various versions of deep-structured CRFs are successfully applied to phone recognition <ref type="bibr" target="#b397">[410]</ref>, spoken language identification <ref type="bibr" target="#b415">[428]</ref>, and natural language processing <ref type="bibr" target="#b415">[428]</ref>. However, at least for the phone recognition task, the performance of deep-structured CRFs, which are purely discriminative (non-generative), has not been able to match that of the hybrid approach involving DBN, which we will take on shortly.</p><p>Morgan <ref type="bibr" target="#b248">[261]</ref> gives an excellent review on other major existing discriminative models in speech recognition based mainly on the traditional neural network or MLP architecture using back-propagation learning with random initialization. It argues for the importance of both the increased width of each layer of the neural networks and the increased depth. In particular, a class of deep neural network models forms the basis of the popular "tandem" approach <ref type="bibr" target="#b249">[262]</ref>, where the output of the discriminatively learned neural network is treated as part of the observation variable in HMMs. For some representative recent work in this area, see <ref type="bibr" target="#b180">[193,</ref><ref type="bibr" target="#b270">283]</ref>.</p><p>In more recent work of <ref type="bibr" target="#b93">[106,</ref><ref type="bibr" target="#b97">110,</ref><ref type="bibr" target="#b205">218,</ref><ref type="bibr" target="#b353">366,</ref><ref type="bibr" target="#b364">377]</ref>, a new deep learning architecture, sometimes called deep stacking network (DSN), together with its tensor variant <ref type="bibr" target="#b167">[180,</ref><ref type="bibr" target="#b168">181]</ref> and its kernel version <ref type="bibr" target="#b89">[102]</ref>, are developed that all focus on discrimination with scalable, parallelizable, block-wise learning relying on little or no generative component. We will describe this type of discriminative deep architecture in detail in Section 6.</p><p>As discussed in the preceding section, recurrent neural networks (RNNs) have been used as a generative model; see also the neural predictive model <ref type="bibr" target="#b74">[87]</ref> with a similar "generative" mechanism. RNNs can also be used as a discriminative model where the output is a label sequence associated with the input data sequence. Note that such discriminative RNNs or sequence models were applied to speech a long time ago with limited success. In <ref type="bibr" target="#b16">[17]</ref>, an HMM was trained jointly with the neural networks, with a discriminative probabilistic training criterion. In <ref type="bibr" target="#b291">[304]</ref>, a separate HMM was used to segment the sequence during training, and the HMM was also used to transform the RNN classification results into label sequences. However, the use of the HMM for these purposes does not take advantage of the full potential of RNNs.</p><p>A set of new models and methods were proposed more recently in <ref type="bibr" target="#b120">[133,</ref><ref type="bibr" target="#b121">134,</ref><ref type="bibr" target="#b122">135,</ref><ref type="bibr" target="#b123">136]</ref> that enable the RNNs themselves to perform sequence classification while embedding the long-short-term memory into the model, removing the need for pre-segmenting the training data and for post-processing the outputs. Underlying this method is the idea of interpreting RNN outputs as the conditional distributions over all possible label sequences given the input sequences. Then, a differentiable objective function can be derived to optimize these conditional distributions over the correct label sequences, where the segmentation of the data is performed automatically by the algorithm. The effectiveness of this method has been demonstrated in handwriting recognition tasks and in a small speech task <ref type="bibr" target="#b122">[135,</ref><ref type="bibr" target="#b123">136]</ref> to be discussed in more detail in Section 7 of this monograph.</p><p>Another type of discriminative deep architecture is the convolutional neural network (CNN), in which each module consists of a convolutional layer and a pooling layer. These modules are often stacked up with one on top of another, or with a DNN on top of it, to form a deep model <ref type="bibr" target="#b199">[212]</ref>. The convolutional layer shares many weights, and the pooling layer subsamples the output of the convolutional layer and reduces the data rate from the layer below. The weight sharing in the convolutional layer, together with appropriately chosen pooling schemes, endows the CNN with some "invariance" properties (e.g., translation invariance). It has been argued that such limited "invariance" or equi-variance is not adequate for complex pattern recognition tasks and more principled ways of handling a wider range of invariance may be needed <ref type="bibr" target="#b147">[160]</ref>. Nevertheless, CNNs have been found highly effective and been commonly used in computer vision and image recognition <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b185">198,</ref><ref type="bibr" target="#b196">209,</ref><ref type="bibr" target="#b199">212,</ref><ref type="bibr" target="#b421">434]</ref>. More recently, with appropriate changes from the CNN designed for image analysis to that taking into account speech-specific properties, the CNN is also found effective for speech recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr">81,</ref><ref type="bibr" target="#b81">94,</ref><ref type="bibr" target="#b299">312</ref>]. We will discuss such applications in more detail in Section 7 of this monograph.</p><p>It is useful to point out that the time-delay neural network (TDNN) <ref type="bibr" target="#b189">[202,</ref><ref type="bibr" target="#b369">382]</ref> developed for early speech recognition is a special case and predecessor of the CNN when weight sharing is limited to one of the two dimensions, i.e., time dimension, and there is no pooling layer. It was not until recently that researchers have discovered that the timedimension invariance is less important than the frequency-dimension invariance for speech recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr">81]</ref>. A careful analysis on the underlying reasons is described in [81], together with a new strategy for designing the CNN's pooling layer demonstrated to be more effective than all previous CNNs in phone recognition.</p><p>It is also useful to point out that the model of hierarchical temporal memory (HTM) <ref type="bibr" target="#b113">[126,</ref><ref type="bibr" target="#b130">143,</ref><ref type="bibr" target="#b129">142]</ref> is another variant and extension of the CNN. The extension includes the following aspects: (1) Time or temporal dimension is introduced to serve as the "supervision" information for discrimination (even for static images); (2) Both bottom-up and top-down information flows are used, instead of just bottom-up in the CNN; and (3) A Bayesian probabilistic formalism is used for fusing information and for decision making.</p><p>Finally, the learning architecture developed for bottom-up, detection-based speech recognition proposed in <ref type="bibr" target="#b201">[214]</ref> and developed further since 2004, notably in <ref type="bibr" target="#b317">[330,</ref><ref type="bibr" target="#b319">332,</ref><ref type="bibr" target="#b414">427]</ref> using the DBN-DNN technique, can also be categorized in the discriminative or supervisedlearning deep architecture category. There is no intent and mechanism in this architecture to characterize the joint probability of data and recognition targets of speech attributes and of the higher-level phone and words. The most current implementation of this approach is based on the DNN, or neural networks with many layers using backpropagation learning. One intermediate neural network layer in the implementation of this detection-based framework explicitly represents the speech attributes, which are simplified entities from the "atomic" units of speech developed in the early work of <ref type="bibr" target="#b88">[101,</ref><ref type="bibr" target="#b342">355]</ref>. The simplification lies in the removal of the temporally overlapping properties of the speech attributes or articulatory-like features. Embedding such more realistic properties in the future work is expected to improve the accuracy of speech recognition further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hybrid deep networks</head><p>The term "hybrid" for this third category refers to the deep architecture that either comprises or makes use of both generative and discriminative model components. In the existing hybrid architectures published in the literature, the generative component is mostly exploited to help with discrimination, which is the final goal of the hybrid architecture. How and why generative modeling can help with discrimination can be examined from two viewpoints <ref type="bibr" target="#b101">[114]</ref>:</p><p>• The optimization viewpoint where generative models trained in an unsupervised fashion can provide excellent initialization points in highly nonlinear parameter estimation problems (The commonly used term of "pre-training" in deep learning has been introduced for this reason); and/or</p><p>• The regularization perspective where the unsupervised-learning models can effectively provide a prior on the set of functions representable by the model.</p><p>The study reported in <ref type="bibr" target="#b101">[114]</ref> provided an insightful analysis and experimental evidence supporting both of the viewpoints above.</p><p>The DBN, a generative, deep network for unsupervised learning discussed in Section 3.2, can be converted to and used as the initial model of a DNN for supervised learning with the same network structure, which is further discriminatively trained or fine-tuned using the target labels provided. When the DBN is used in this way we consider this DBN-DNN model as a hybrid deep model, where the model trained using unsupervised data helps to make the discriminative model effective for supervised learning. We will review details of the discriminative DNN for supervised learning in the context of RBM/DBN generative, unsupervised pre-training in Section 5.</p><p>Another example of the hybrid deep network is developed in <ref type="bibr" target="#b247">[260]</ref>, where the DNN weights are also initialized from a generative DBN but are further fine-tuned with a sequence-level discriminative criterion, which is the conditional probability of the label sequence given the input feature sequence, instead of the frame-level criterion of crossentropy commonly used. This can be viewed as a combination of the static DNN with the shallow discriminative architecture of CRF. It can be shown that such a DNN-CRF is equivalent to a hybrid deep architecture of DNN and HMM whose parameters are learned jointly using the full-sequence maximum mutual information (MMI) criterion between the entire label sequence and the input feature sequence. A closely related full-sequence training method designed and implemented for much larger tasks is carried out more recently with success for a shallow neural network <ref type="bibr" target="#b181">[194]</ref> and for a deep one <ref type="bibr" target="#b182">[195,</ref><ref type="bibr" target="#b340">353,</ref><ref type="bibr" target="#b361">374]</ref>. We note that the origin of the idea for joint training of the sequence model (e.g., the HMM) and of the neural network came from the early work of <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref>, where shallow neural networks were trained with small amounts of training data and with no generative pre-training.</p><p>Here, it is useful to point out a connection between the above pretraining/fine-tuning strategy associated with hybrid deep networks and the highly popular minimum phone error (MPE) training technique for the HMM (see <ref type="bibr" target="#b134">[147,</ref><ref type="bibr" target="#b277">290]</ref> for an overview). To make MPE training effective, the parameters need to be initialized using an algorithm (e.g., Baum-Welch algorithm) that optimizes a generative criterion (e.g., maximum likelihood). This type of methods, which uses maximumlikelihood trained parameters to assist in the discriminative HMM training can be viewed as a "hybrid" approach to train the shallow HMM model.</p><p>Along the line of using discriminative criteria to train parameters in generative models as in the above HMM training example, we here discuss the same method applied to learning other hybrid deep networks. In <ref type="bibr" target="#b190">[203]</ref>, the generative model of RBM is learned using the discriminative criterion of posterior class-label probabilities. Here the label vector is concatenated with the input data vector to form the combined visible layer in the RBM. In this way, RBM can serve as a stand-alone solution to classification problems and the authors derived a discriminative learning algorithm for RBM as a shallow generative model. In the more recent work by Ranzato et al. <ref type="bibr" target="#b285">[298]</ref>, the deep generative model of DBN with gated Markov random field (MRF) at the lowest level is learned for feature extraction and then for recognition of difficult image classes including occlusions. The generative ability of the DBN facilitates the discovery of what information is captured and what is lost at each level of representation in the deep model, as demonstrated in <ref type="bibr" target="#b285">[298]</ref>. A related study on using the discriminative criterion of empirical risk to train deep graphical models can be found in <ref type="bibr" target="#b339">[352]</ref>.</p><p>A further example of hybrid deep networks is the use of generative models of DBNs to pre-train deep convolutional neural networks (deep CNNs) <ref type="bibr" target="#b202">[215,</ref><ref type="bibr" target="#b203">216,</ref><ref type="bibr" target="#b204">217]</ref>. Like the fully connected DNN discussed earlier, pre-training also helps to improve the performance of deep CNNs over random initialization. Pre-training DNNs or CNNs using a set of regularized deep autoencoders <ref type="bibr" target="#b23">[24]</ref>, including denoising autoencoders, contractive autoencoders, and sparse autoencoders, is also a similar example of the category of hybrid deep networks.</p><p>The final example given here for hybrid deep networks is based on the idea and work of <ref type="bibr" target="#b131">[144,</ref><ref type="bibr" target="#b254">267]</ref>, where one task of discrimination (e.g., speech recognition) produces the output (text) that serves as the input to the second task of discrimination (e.g., machine translation). The overall system, giving the functionality of speech translation -translating speech in one language into text in another language -is a two-stage deep architecture consisting of both generative and discriminative elements. Both models of speech recognition (e.g., HMM) and of machine translation (e.g., phrasal mapping and non-monotonic alignment) are generative in nature, but their parameters are all learned for discrimination of the ultimate translated text given the speech data. The framework described in <ref type="bibr" target="#b131">[144]</ref> enables end-to-end performance optimization in the overall deep architecture using the unified learning framework initially published in <ref type="bibr" target="#b134">[147]</ref>. This hybrid deep learning approach can be applied to not only speech translation but also all speech-centric and possibly other information processing tasks such as speech information retrieval, speech understanding, cross-lingual speech/text understanding and retrieval, etc. (e.g., <ref type="bibr" target="#b75">[88,</ref><ref type="bibr" target="#b81">94,</ref><ref type="bibr" target="#b132">145,</ref><ref type="bibr" target="#b133">146,</ref><ref type="bibr" target="#b353">366,</ref><ref type="bibr" target="#b385">398]</ref>).</p><p>In the next three chapters, we will elaborate on three prominent types of models for deep learning, one from each of the three classes reviewed in this chapter. These are chosen to serve the tutorial purpose, given their simplicity of the architectural and mathematical descriptions. The three architectures described in the following three chapters may not be interpreted as the most representative and influential work in each of the three classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Introduction</head><p>The deep autoencoder is a special type of the DNN (with no class labels), whose output vectors have the same dimensionality as the input vectors. It is often used for learning a representation or effective encoding of the original data, in the form of input vectors, at hidden layers. Note that the autoencoder is a nonlinear feature extraction method without using class labels. As such, the features extracted aim at conserving and better representing information instead of performing classification tasks, although sometimes these two goals are correlated.</p><p>An autoencoder typically has an input layer which represents the original data or input feature vectors (e.g., pixels in image or spectra in speech), one or more hidden layers that represent the transformed feature, and an output layer which matches the input layer for reconstruction. When the number of hidden layers is greater than one, the autoencoder is considered to be deep. The dimension of the hidden layers can be either smaller (when the goal is feature compression) or larger (when the goal is mapping the feature to a higher-dimensional space) than the input dimension.</p><p>An autoencoder is often trained using one of the many backpropagation variants, typically the stochastic gradient descent method. Though often reasonably effective, there are fundamental problems when using back-propagation to train networks with many hidden layers. Once the errors get back-propagated to the first few layers, they become minuscule, and training becomes quite ineffective. Though more advanced back-propagation methods help with this problem to some degree, it still results in slow learning and poor solutions, especially with limited amounts of training data. As mentioned in the previous chapters, the problem can be alleviated by pre-training each layer as a simple autoencoder <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b150">163]</ref>. This strategy has been applied to construct a deep autoencoder to map images to short binary code for fast, content-based image retrieval, to encode documents (called semantic hashing), and to encode spectrogram-like speech features which we review below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Use of deep autoencoders to extract speech features</head><p>Here we review a set of work, some of which was published in <ref type="bibr" target="#b87">[100]</ref>, in developing an autoencoder for extracting binary speech codes from the raw speech spectrogram data in an unsupervised manner (i.e., no speech class labels). The discrete representations in terms of a binary code extracted by this model can be used in speech information retrieval or as bottleneck features for speech recognition.</p><p>A deep generative model of patches of spectrograms that contain 256 frequency bins and 1, 3, 9, or 13 frames is illustrated in Figure <ref type="figure" target="#fig_9">4</ref>.1. An undirected graphical model called a Gaussian-Bernoulli RBM is built that has one visible layer of linear variables with Gaussian noise and one hidden layer of 500 to 3000 binary latent variables. After learning the Gaussian-Bernoulli RBM, the activation probabilities of its hidden units are treated as the data for training another Bernoulli-Bernoulli RBM. These two RBM's can then be composed to form a deep belief net (DBN) in which it is easy to infer the states of the second layer of binary hidden units from the input in a single forward pass. The DBN used in this work is illustrated on the left side of The deep autoencoder with three hidden layers is formed by "unrolling" the DBN using its weight matrices. The lower layers of this deep autoencoder use the matrices to encode the input and the upper layers use the matrices in reverse order to decode the input. This deep autoencoder is then fine-tuned using error back-propagation to minimize the reconstruction error, as shown on the right side of Figure 4.1. After learning is complete, any variable-length spectrogram can be encoded and reconstructed as follows. First, N consecutive overlapping frames of 256-point log power spectra are each normalized to zero-mean and unit-variance across samples per feature to provide the input to the deep autoencoder. The first hidden layer then uses the logistic function to compute real-valued activations. These real values are fed to the next, coding layer to compute "codes." The real-valued activations of hidden units in the coding layer are quantized to be either zero or one with 0.5 as the threshold. These binary codes are then used to reconstruct the original spectrogram, where individual fixed-frame patches are reconstructed first using the two upper layers of network weights. Finally, the standard overlap-and-add technique in signal processing is used to reconstruct the full-length speech spectrogram from the outputs produced by applying the deep autoencoder to every possible window of N consecutive frames. We show some illustrative encoding and reconstruction examples below. At the top of Figure <ref type="figure" target="#fig_9">4</ref>.2 is the original, un-coded speech, followed by the speech utterances reconstructed from the binary codes (zero or one) at the 312 unit bottleneck code layer with encoding window lengths of N = 1, 3, 9, and 13, respectively. The lower reconstruction errors for N = 9 and N = 13 are clearly seen.</p><p>Encoding error of the deep autoencoder is qualitatively examined in comparison with the more traditional codes via vector quantization (VQ).   shown below the spectrograms, demonstrating that the autoencoder (red curve) is producing lower errors than the VQ coder (blue curve) throughout the entire span of the utterance. The final two spectrograms show detailed coding error distributions over both time and frequency bins. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Stacked denoising autoencoders</head><p>In early years of autoencoder research, the encoding layer had smaller dimensions than the input layer. However, in some applications, it is desirable that the encoding layer is wider than the input layer, in which case techniques are needed to prevent the neural network from learning the trivial identity mapping function. One of the reasons for using a    higher dimension in the hidden or encoding layers than the input layer is that it allows the autoencoder to capture a rich input distribution.</p><p>The trivial mapping problem discussed above can be prevented by methods such as using sparseness constraints, or using the "dropout" trick by randomly forcing certain values to be zero and thus introducing distortions at the input data <ref type="bibr" target="#b363">[376,</ref><ref type="bibr" target="#b362">375]</ref> or at the hidden layers <ref type="bibr" target="#b153">[166]</ref>. For  example, in the stacked denoising autoencoder detailed in <ref type="bibr" target="#b363">[376]</ref>, random noises are added to the input data. This serves several purposes. First, by forcing the output to match the original undistorted input data the model can avoid learning the trivial identity solution. Second, since the noises are added randomly, the model learned would be robust to the same kind of distortions in the test data. Third, since each distorted input sample is different, it greatly increases the training set size and thus can alleviate the overfitting problem.</p><p>It is interesting to note that when the encoding and decoding weights are forced to be the transpose of each other, such denoising autoencoder with a single sigmoidal hidden layer is strictly equivalent to a particular Gaussian RBM, but instead of training it by the technique of contrastive divergence (CD) or persistent CD, it is trained by a score matching principle, where the score is defined as the derivative of the log-density with respect to the input <ref type="bibr" target="#b362">[375]</ref>. Furthermore, Alain and Bengio <ref type="bibr" target="#b4">[5]</ref> generalized this result to any parameterization of the encoder and decoder with squared reconstruction error and Gaussian corruption noise. They show that as the amount of noise approaches zero, such models estimate the true score of the underlying data generating distribution. Finally, Bengio et al. <ref type="bibr" target="#b29">[30]</ref> show that any denoising autoencoder is a consistent estimator of the underlying data generating distribution within some family of distributions. This is true for any parameterization of the autoencoder, for any type of information-destroying corruption process with no constraint on the noise level except being positive, and for any reconstruction loss expressed as a conditional log-likelihood. The consistency of the estimator is achieved by associating the denoising autoencoder with a Markov chain whose stationary distribution is the distribution estimated by the model, and this Markov chain can be used to sample from the denoising autoencoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Transforming autoencoders</head><p>The deep autoencoder described above can extract faithful codes for feature vectors due to many layers of nonlinear processing. However, the code extracted in this way is transformation-variant. In other words, the extracted code would change in ways chosen by the learner when the input feature vector is transformed. Sometimes, it is desirable to have the code change predictably to reflect the underlying transformationinvariant property of the perceived content. This is the goal of the transforming autoencoder proposed in <ref type="bibr" target="#b149">[162]</ref> for image recognition.</p><p>The building block of the transforming autoencoder is a "capsule," which is an independent sub-network that extracts a single parameterized feature representing a single entity, be it visual or audio. A transforming autoencoder receives both an input vector and a target output vector, which is transformed from the input vector through a simple global transformation mechanism; e.g., translation of an image and frequency shift of speech (the latter due to the vocal tract length difference). An explicit representation of the global transformation is assumed known. The coding layer of the transforming autoencoder consists of the outputs of several capsules.</p><p>During the training phase, the different capsules learn to extract different entities in order to minimize the error between the final output and the target.</p><p>In addition to the deep autoencoder architectures described here, there are many other types of generative architectures in the literature, all characterized by the use of data alone (i.e., free of classification labels) to automatically derive higher-level features.</p><p>In this section, we present the most widely used hybrid deep architecture -the pre-trained deep neural network (DNN), and discuss the related techniques and building blocks including the RBM and DBN. We discuss the DNN example here in the category of hybrid deep networks before the examples in the category of deep networks for supervised learning (Section 6). This is partly due to the natural flow from the unsupervised learning models to the DNN as a hybrid model. The discriminative nature of artificial neural networks for supervised learning has been widely known, and thus would not be required for understanding the hybrid nature of the DNN that uses unsupervised pre-training to facilitate the subsequent discriminative fine tuning.</p><p>Part of the review in this chapter is based on recent publications in <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b148">161,</ref><ref type="bibr" target="#b399">412]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Restricted Boltzmann machines</head><p>An RBM is a special type of Markov random field that has one layer of (typically Bernoulli) stochastic hidden units and one layer of (typically Bernoulli or Gaussian) stochastic visible or observable units. RBMs can be represented as bipartite graphs, where all visible units are connected to all hidden units, and there are no visible-visible or hidden-hidden connections.</p><p>In an RBM, the joint distribution p(v, h; θ) over the visible units v and hidden units h, given the model parameters θ, is defined in terms of an energy function</p><formula xml:id="formula_1">E(v, h; θ) of p(v, h; θ) = exp(−E(v, h; θ)) Z ,</formula><p>where</p><formula xml:id="formula_2">Z = v h exp(−E(v, h; θ)</formula><p>) is a normalization factor or partition function, and the marginal probability that the model assigns to a visible vector v is</p><formula xml:id="formula_3">p(v; θ) = h exp(−E(v, h; θ)) Z</formula><p>For a Bernoulli (visible)-Bernoulli (hidden) RBM, the energy function is defined as</p><formula xml:id="formula_4">E(v, h; θ) = − I i=1 J j=1 w ij v i h j − I i=1 b i v i − J j=1 a j h j .</formula><p>where w ij represents the symmetric interaction term between visible unit v i and hidden unit h j , b i and a j the bias terms, and I and J are the numbers of visible and hidden units. The conditional probabilities can be efficiently calculated as</p><formula xml:id="formula_5">p(h j = 1|v; θ) = σ I i=1 w ij v i + a j , p(v i = 1|h; θ) = σ   J j=1 w ij h j +b i   , where σ(x) = 1/(1 + exp(−x)).</formula><p>Similarly, for a Gaussian (visible)-Bernoulli (hidden) RBM, the energy is</p><formula xml:id="formula_6">E(v, h; θ) = − I i=1 J j=1 w ij v i h j − 1 2 I i=1 (v i − b i ) 2 − J j=1 a j h j ,</formula><p>The corresponding conditional probabilities become</p><formula xml:id="formula_7">p(h j = 1|v; θ) = σ I i=1 w ij v i +a j , p(v i |h; θ) = N   J j=1 w ij h j + b i , 1   ,</formula><p>where v i takes real values and follows a Gaussian distribution with mean J j=1 w ij h j + b i and variance one. Gaussian-Bernoulli RBMs can be used to convert real-valued stochastic variables to binary stochastic variables, which can then be further processed using the Bernoulli-Bernoulli RBMs.</p><p>The above discussion used two of the most common conditional distributions for the visible data in the RBM -Gaussian (for continuous-valued data) and binomial (for binary data). More general types of distributions in the RBM can also be used. See <ref type="bibr" target="#b373">[386]</ref> for the use of general exponential-family distributions for this purpose.</p><p>Taking the gradient of the log likelihood log p(v; θ) we can derive the update rule for the RBM weights as:</p><formula xml:id="formula_8">∆w ij = E data (v i h j ) − E model (v i h j ),</formula><p>where E data (v i h j ) is the expectation observed in the training set (with h j sampled given v i according to the model), and E model (v i h j ) is that same expectation under the distribution defined by the model. Unfortunately, E model (v i h j ) is intractable to compute. The contrastive divergence (CD) approximation to the gradient was the first efficient method proposed to approximate this expected value, where E model (v i h j ) is replaced by running the Gibbs sampler initialized at the data for one or more steps. The steps in approximating E model (v i h j ) is summarized as follows: Here, (v 1 , h 1 ) is a sample from the model, as a very rough estimate of E model (v i h j ). The use of (v 1 , h 1 ) to approximate E model (v i h j ) gives rise to the algorithm of CD-1. The sampling process can be pictorially depicted in Figure <ref type="figure">5</ref>.1.</p><formula xml:id="formula_9">• Initialize v 0 at data • Sample h 0 ∼ p(h|v 0 ) • Sample v 1 ∼ p(v|h 0 ) • Sample h 1 ∼ p(h|v 1 )</formula><p>Note that CD-k generalizes this to more steps of the Markov chain. There are other techniques for estimating the log-likelihood gradient of RBMs, in particular the stochastic maximum likelihood or persistent contrastive divergence (PCD) <ref type="bibr" target="#b350">[363,</ref><ref type="bibr" target="#b393">406]</ref>. Both work better than CD when using the RBM as a generative model.</p><p>Careful training of RBMs is essential to the success of applying RBM and related deep learning techniques to solve practical problems. See Technical Report <ref type="bibr" target="#b146">[159]</ref> for a very useful practical guide for training RBMs.</p><p>The RBM discussed above is both a generative and an unsupervised model, which characterizes the input data distribution using hidden variables and there is no label information involved. However, when the label information is available, it can be used together with the data to form the concatenated "data" set. Then the same CD learning can be applied to optimize the approximate "generative" objective function related to data likelihood. Further, and more interestingly, a "discriminative" objective function can be defined in terms of conditional likelihood of labels. This discriminative RBM can be used to "fine tune" RBM for classification tasks <ref type="bibr" target="#b190">[203]</ref>.</p><p>Ranzato et al. <ref type="bibr" target="#b284">[297,</ref><ref type="bibr" target="#b282">295]</ref> proposed an unsupervised learning algorithm called sparse encoding symmetric machine (SESM), which is quite similar to RBM. They both have a symmetric encoder and decoder, and a logistic nonlinearity on the top of the encoder. The main difference is that whereas the RBM is trained using (very approximate) maximum likelihood, SESM is trained by simply minimizing the average energy plus an additional code sparsity term. SESM relies on the sparsity term to prevent flat energy surfaces, while RBM relies on an explicit contrastive term in the loss, an approximation of the log partition function. Another difference is in the coding strategy in that the code units are "noisy" and binary in the RBM, while they are quasibinary and sparse in SESM. The use of SESM in pre-training DNNs for speech recognition can be found in <ref type="bibr" target="#b271">[284]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Unsupervised layer-wise pre-training</head><p>Here we describe how to stack up RBMs just described to form a DBN as the basis for DNN's pre-training. Before delving into details, we first note that this procedure, proposed by Hinton and Salakhutdinov <ref type="bibr" target="#b150">[163]</ref> is a more general technique of unsupervised layer-wise pretraining. That is, not only RBMs can be stacked to form deep generative (or discriminative) networks, but other types of networks can also do the same, such as autoencoder variants as proposed by Bengio et al. <ref type="bibr" target="#b27">[28]</ref>.</p><p>Stacking a number of the RBMs learned layer by layer from bottom up gives rise to a DBN, an example of which is shown in Figure <ref type="figure">5</ref>.2. The stacking procedure is as follows. After learning a Gaussian-Bernoulli RBM (for applications with continuous features such as speech) or Bernoulli-Bernoulli RBM (for applications with nominal or binary features such as black-white image or coded text), we treat the activation probabilities of its hidden units as the data for training the Bernoulli-Bernoulli RBM one layer up. The activation probabilities of the secondlayer Bernoulli-Bernoulli RBM are then used as the visible data input for the third-layer Bernoulli-Bernoulli RBM, and so on. Some theoretical justification of this efficient layer-by-layer greedy learning strategy is given in <ref type="bibr" target="#b150">[163]</ref>, where it is shown that the stacking procedure above improves a variational lower bound on the likelihood of the training data under the composite model. That is, the greedy procedure above achieves approximate maximum likelihood learning. Note that this learning procedure is unsupervised and requires no class label.</p><p>When applied to classification tasks, the generative pre-training can be followed by or combined with other, typically discriminative, learning procedures that fine-tune all of the weights jointly to improve the performance of the network. This discriminative fine-tuning is performed by adding a final layer of variables that represent the desired outputs or labels provided in the training data. Then, the backpropagation algorithm can be used to adjust or fine-tune the network weights in the same way as for the standard feed-forward neural network. What goes to the top, label layer of this DNN depends on the application. For speech recognition applications, the top layer, denoted by "l 1 , l 2 , . . . , l j , . . . , l L ," in Figure <ref type="figure">5</ref>.2, can represent either syllables, phones, sub-phones, phone states, or other speech units used in the HMM-based speech recognition system.</p><p>The generative pre-training described above has produced better phone and speech recognition results than random initialization on a wide variety of tasks, which will be surveyed in Section 7. Further research has also shown the effectiveness of other pre-training strategies. As an example, greedy layer-by-layer training may be carried out with an additional discriminative term to the generative cost function at each level. And without generative pre-training, purely discriminative training of DNNs from random initial weights using the traditional stochastic gradient decent method has been shown to work very well when the scales of the initial weights are set carefully and the minibatch sizes, which trade off noisy gradients with convergence speed, used in stochastic gradient decent are adapted prudently (e.g., with an increasing size over training epochs). Also, randomization order in creating mini-batches needs to be judiciously determined. Importantly, it was found effective to learn a DNN by starting with a shallow neural network with a single hidden layer. Once this has been trained discriminatively (using early stops to avoid overfitting), a second hidden layer is inserted between the first hidden layer and the labeled softmax output units and the expanded deeper network is again trained discriminatively. This can be continued until the desired number of hidden layers is reached, after which a full backpropagation "fine tuning" is applied. This discriminative "pre-training" procedure is found to work well in practice <ref type="bibr" target="#b311">[324,</ref><ref type="bibr" target="#b406">419]</ref>, especially with a reasonably large amount of training data. When the amount of training data is increased even more, then some carefully designed random initialization methods can work well also without using the above pre-training schemes.</p><p>In any case, pre-training based on the use of RBMs to stack up in forming the DBN has been found to work well in most cases, regardless of a large or small amount of training data. It is useful to point out that there are other ways to perform pre-training in addition to the use of RBMs and DBNs. For example, denoising autoencoders have now been shown to be consistent estimators of the data generating distribution <ref type="bibr" target="#b29">[30]</ref>. Like RBMs, they are also shown to be generative models from which one can sample. Unlike RBMs, however, an unbiased estimator of the gradient of the training objective function can be obtained by the denoising autoencoders, avoiding the need for MCMC or variational approximations in the inner loop of training. Therefore, the greedy layer-wise pre-training may be performed as effectively by stacking the denoising autoencoders as by stacking the RBMs each as a single-layer learner.</p><p>Further, a general framework for layer-wise pre-training can be found in many deep learning papers; e.g., Section 2 of <ref type="bibr" target="#b20">[21]</ref>. This includes, as a special case, the use of RBMs as the single-layer building block as discussed in this section. The more general framework can cover the RBM/DBN as well as any other unsupervised feature extractor. It can also cover the case of unsupervised pre-training of the representation only followed by a separate stage of learning a classifier on top of the unsupervised, pre-trained features <ref type="bibr" target="#b202">[215,</ref><ref type="bibr" target="#b203">216,</ref><ref type="bibr" target="#b204">217]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Interfacing DNNs with HMMs</head><p>The pre-trained DNN as a prominent example of the hybrid deep networks discussed so far in this chapter is a static classifier with input vectors having a fixed dimensionality. However, many practical pattern recognition and information processing problems, including speech recognition, machine translation, natural language understanding, video processing and bio-information processing, require sequence recognition. In sequence recognition, sometimes called classification with structured input/output, the dimensionality of both inputs and outputs are variable.</p><p>The HMM, based on dynamic programing operations, is a convenient tool to help port the strength of a static classifier to handle dynamic or sequential patterns. Thus, it is natural to combine feed-forward neural networks and HMMs to bridge the gap between the static and sequence pattern recognition, as was done in the early days of neural networks for speech recognition <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b41">42]</ref>. A popular architecture to fulfill this role with the use of the DNN is shown in Figure <ref type="figure">5</ref>.3. This architecture has been successfully used in speech recognition experiments as reported in <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref>.</p><p>It is important to note that the unique elasticity of temporal dynamics of speech as elaborated in <ref type="bibr" target="#b44">[45,</ref><ref type="bibr">73,</ref><ref type="bibr">76,</ref><ref type="bibr" target="#b70">83]</ref> would require temporally  <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref>. [after <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref>, @IEEE]. correlated models more powerful than HMMs for the ultimate success of speech recognition. Integrating such dynamic models that have realistic co-articulatory properties with the DNN and possibly other deep learning models to form the coherent dynamic deep architecture is a challenging new research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Introduction</head><p>While the DNN just reviewed has been shown to be extremely powerful in connection with performing recognition and classification tasks including speech recognition and image classification, training a DNN has proven to be difficult computationally. In particular, conventional techniques for training DNNs at the fine tuning phase involve the utilization of a stochastic gradient descent learning algorithm, which is difficult to parallelize across machines. This makes learning at large scale nontrivial. For example, it has been possible to use one single, very powerful GPU machine to train DNN-based speech recognizers with dozens to a few hundreds or thousands of hours of speech training data with remarkable results. It is less clear, however, how to scale up this success with much more training data. See <ref type="bibr" target="#b68">[69]</ref> for recent work in this direction.</p><p>Here we describe a new deep learning architecture, the deep stacking network (DSN), which was originally designed with the learning scalability problem in mind. This chapter is based in part on the recent publications of <ref type="bibr" target="#b93">[106,</ref><ref type="bibr" target="#b97">110,</ref><ref type="bibr" target="#b167">180,</ref><ref type="bibr" target="#b168">181]</ref> with expanded discussions.</p><p>The central idea of the DSN design relates to the concept of stacking, as proposed and explored in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b379">392]</ref>, where simple modules of functions or classifiers are composed first and then they are "stacked" on top of each other in order to learn complex functions or classifiers. Various ways of implementing stacking operations have been developed in the past, typically making use of supervised information in the simple modules. The new features for the stacked classifier at a higher level of the stacking architecture often come from concatenation of the classifier output of a lower module and the raw input features. In <ref type="bibr" target="#b59">[60]</ref>, the simple module used for stacking was a conditional random field (CRF). This type of deep architecture was further developed with hidden states added for successful natural language and speech recognition applications where segmentation information is unknown in the training data <ref type="bibr" target="#b416">[429]</ref>. Convolutional neural networks, as in <ref type="bibr" target="#b172">[185]</ref>, can also be considered as a stacking architecture but the supervision information is typically not used until in the final stacking module.</p><p>The DSN architecture was originally presented in <ref type="bibr" target="#b93">[106]</ref> and was referred as deep convex network or DCN to emphasize the convex nature of a major portion of the algorithm used for learning the network. The DSN makes use of supervision information for stacking each of the basic modules, which takes the simplified form of multilayer perceptron. In the basic module, the output units are linear and the hidden units are sigmoidal nonlinear. The linearity in the output units permits highly efficient, parallelizable, and closed-form estimation (a result of convex optimization) for the output network weights given the hidden units' activities. Due to the closed-form constraints between the input and output weights, the input weights can also be elegantly estimated in an efficient, parallelizable, batch-mode manner, which we will describe in some detail in Section 6.3.</p><p>The name "convex" used in <ref type="bibr" target="#b93">[106]</ref> accentuates the role of convex optimization in learning the output network weights given the hidden units' activities in each basic module. It also points to the importance of the closed-form constraints, derived from the convexity, between the input and output weights. Such constraints make the learning of the remaining network parameters (i.e., the input network weights) much easier than otherwise, enabling batch-mode learning of the DSN that can be distributed over CPU clusters. And in more recent publications, the DSN was used when the key operation of stacking is emphasized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">A basic architecture of the deep stacking network</head><p>A DSN, as shown in Figure <ref type="figure">6</ref>.1, includes a variable number of layered modules, wherein each module is a specialized neural network consisting of a single hidden layer and two trainable sets of weights. In Figure <ref type="figure">6</ref>.1, only four such modules are illustrated, where each module is shown with a separate color. In practice, up to a few hundreds of modules have been efficiently trained and used in image and speech classification experiments.</p><p>The lowest module in the DSN comprises a linear layer with a set of linear input units, a hidden nonlinear layer with a set of nonlinear units, and a second linear layer with a set of linear output units. A sigmoidal nonlinearity is typically used in the hidden layer. However, other nonlinearities can also be used. If the DSN is utilized in connection with recognizing an image, the input units can correspond to a number of pixels (or extracted features) in the image, and can be assigned values based at least in part upon intensity values, RGB values, or the like corresponding to the respective pixels. If the DSN is utilized in connection with speech recognition, the set of input units may correspond to samples of speech waveform, or the extracted features from speech waveforms, such as power spectra or cepstral coefficients. The output units in the linear output layer represent the targets of classification. For instance, if the DSN is configured to perform digit recognition, then the output units may be representative of the values 0, 1, 2, 3, and so forth up to 9 with a 0-1 coding scheme. If the DSN is configured to perform speech recognition, then the output units may be representative of phones, HMM states of phones, or context-dependent HMM states of phones.</p><p>The lower-layer weight matrix, which we denote by W , connects the linear input layer and the hidden nonlinear layer. The upper-layer weight matrix, which we denote by U , connects the nonlinear hidden layer with the linear output layer. The weight matrix U can be determined through a closed-form solution given the weight matrix W when the mean square error training criterion is used. As indicated above, the DSN includes a set of serially connected, overlapping, and layered modules, wherein each module has the same architecture -a linear input layer followed by a nonlinear hidden layer, which is connected to a linear output layer. Note that the output units of a lower module are a subset of the input units of an adjacent higher module in the DSN. More specifically, in a second module that is directly above the lowest module in the DSN, the input units can include the output units of the lowest module and optionally the raw input feature.</p><p>This pattern of including output units in a lower module as a portion of the input units in an adjacent higher module and thereafter learning a weight matrix that describes connection weights between hidden units and linear output units via convex optimization can continue for many modules. A resultant learned DSN may then be deployed in connection with an automatic classification task such as frame-level speech phone or state classification. Connecting the DSN's output to an HMM or any dynamic programming device enables continuous speech recognition and other forms of sequential pattern recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">A method for learning the DSN weights</head><p>Here, we provide some technical details on how the use of linear output units in the DSN facilitates the learning of the DSN weights. A single module is used to illustrate the advantage for simplicity reasons. First, it is clear that the upper layer weight matrix U can be efficiently learned once the activity matrix H over all training samples in the hidden layer is known. Let's denote the training vectors by X = [x 1 , . . . , x i , . . . , x N ], in which each vector is denoted by x i = [x 1i , . . . , x ji , . . . , x Di ] T where D is the dimension of the input vector, which is a function of the block, and N is the total number of training samples. Denote by L the number of hidden units and by C the dimension of the output vector. Then the output of a DSN block is y i = U T h i where h i = σ(W T x i ) is the hidden-layer vector for sample i, U is an L × C weight matrix at the upper layer of a block. W is a D × L weight matrix at the lower layer of a block, and σ(•) is a sigmoid function. Bias terms are implicitly represented in the above formulation if x i and h i are augmented with ones.</p><p>Given target vectors in the full training set with a total of N samples, T = [t 1 , . . . , t i , . . . , t N ], where each vector is</p><formula xml:id="formula_10">t i = [t 1i , • • • , t ji , . . . , t Ci ] T ,</formula><p>the parameters U and W are learned so as to minimize the average of the total square error below:</p><formula xml:id="formula_11">E = 1 2 i y i − t i 2 = 1 2 Tr[(Y − T )(Y − T ) T ]</formula><p>where the output of the network is</p><formula xml:id="formula_12">y i = U T h i = U T σ(W T x i ) = G i (UW )</formula><p>which depends on both weight matrices, as in the standard neural net.</p><p>Assuming H = [h 1 , . . . , h i , . . . , h N ] is known, or equivalently, W is known. Then, setting the error derivative with respective to U to zero gives</p><formula xml:id="formula_13">U = (HH T ) −1 HT T = F(W ), where h i = σ(W T x i ).</formula><p>This provides an explicit constraint between U and W which were treated independently in the conventional backpropagation algorithm. Now, given the equality constraint U = F (W ), let's use Lagrangian multiplier method to solve the optimization problem in learning W Optimizing the Lagrangian:</p><formula xml:id="formula_14">E = 1 2 i G i (U , W ) − t i 2 + λ U − F(W )</formula><p>we can derive batch-mode gradient descent learning algorithm where the gradient takes the following form <ref type="bibr" target="#b93">[106,</ref><ref type="bibr" target="#b400">413]</ref>:</p><formula xml:id="formula_15">∂E ∂W = 2X[H T • (1 − H ) T • [H † (HT T )(TH † ) − T T (TH † )]],</formula><p>where</p><formula xml:id="formula_16">H † = H T (H H T ) −1 is pseudo-inverse of H and symbol • denotes element-wise multiplication.</formula><p>Compared with conventional backpropagation, the above method has less noise in gradient computation due to the exploitation of the explicit constraint U = F (W ). As such, it was found experimentally that, unlike backpropagation, batch training is effective, which aids parallel learning of the DSN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">The tensor deep stacking network</head><p>The above DSN architecture has recently been generalized to its tensorized version, which we call the tensor DSN (TDSN) <ref type="bibr" target="#b167">[180,</ref><ref type="bibr" target="#b168">181]</ref>. It has the same scalability as the DSN in terms of parallelizability in learning, but it generalizes the DSN by providing higher-order feature interactions missing in the DSN.</p><p>The architecture of the TDSN is similar to that of the DSN in the way that stacking operation is carried out. That is, modules of the TDSN are stacked up in a similar way to form a deep architecture. The differences between the TDSN and the DSN lie mainly in how each module is constructed. In the DSN, we have one set of hidden units forming a hidden layer, as denoted at the left panel of Figure <ref type="figure">6</ref>.2. In contrast, each module of a TDSN contains two independent hidden layers, denoted as "Hidden 1" and "Hidden 2" in the middle and right panels of Figure <ref type="figure">6</ref>.2. As a result of this difference, the upper-layer weights, denoted by "U" in Figure <ref type="figure">6</ref>.2, changes from a matrix (a two dimensional array) in the DSN to a tensor (a three dimensional array) in the TDSN, shown as a cube labeled by "U" in the middle panel.</p><p>The tensor U has a three-way connection, one to the prediction layer and the remaining to the two separate hidden layers. An equivalent form of this TDSN module is shown in the right panel of Figure <ref type="figure">6</ref>.2, where the implicit hidden layer is formed by expanding the two separate hidden layers into their outer product. The resulting large vector contains all possible pair-wise products for the two sets of hidden-layer vectors. This turns tensor U into a matrix again whose dimensions are (1) size of the prediction layer; and (2) product of the two hidden layers' sizes. Such equivalence enables the same convex optimization for learning U developed for the DSN to be applied to learning tensor U. Importantly, higher-order hidden feature interactions are enabled in the TDSN via the outer product construction for the large, implicit hidden layer. Stacking the TDSN modules to form a deep architecture pursues in a similar way to the DSN by concatenating various vectors. Two examples are shown in Figures 6.3 and 6.4. Note stacking by concatenating hidden layers with input (Figure <ref type="figure">6</ref>.4) would be difficult for the DSN since its hidden layer tends to be too large for practical purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">The Kernelized deep stacking network</head><p>The DSN architecture has also recently been generalized to its kernelized version, which we call the kernel-DSN (K-DSN) <ref type="bibr" target="#b89">[102,</ref><ref type="bibr" target="#b158">171]</ref>. The motivation of the extension is to increase the size of the hidden units in each DSN module, yet without increasing the size of the free parameters to learn. This goal can be easily accomplished using the kernel trick, resulting in the K-DSN which we describe below. In the DSN architecture reviewed above optimizing the weight matrix U given the hidden layers' outputs in each module is a convex optimization problem. However, the problem of optimizing weight matrix W and thus the whole network is nonconvex. In a recent extension of DSN, a tensor structure was imposed, shifting most of the nonconvex learning burden for W to the convex optimization of U <ref type="bibr" target="#b167">[180,</ref><ref type="bibr" target="#b168">181]</ref>. In the new K-DSN extension, we completely eliminate nonconvex learning for W using the kernel trick.</p><p>To derive the K-DSN architecture and the associated learning algorithm, we first take the bottom module of DSN as an example and generalize the sigmoidal hidden layer h i = σ(W T x i ) in the DSN module into a generic nonlinear mapping function G(X) from the raw input feature X, with high dimensionality in G(X) (possibly infinite) determined only implicitly by a kernel function to be chosen. Second, we formulate the constrained optimization problem of</p><formula xml:id="formula_17">minimize 1 2 Tr[EE T ] + C 2 U T U subject to T − U T G(X) = E.</formula><p>Third, we make use of dual representations of the above constrained optimization problem to obtain U = G T a, where vector a takes the following form:</p><formula xml:id="formula_18">a = (CI + K ) −1 T and K = G(X )G T (X) is a symmetric kernel matrix with elements K nm = g T (x n )g(x m ).</formula><p>Finally, for each new input vector x in the test or dev set, we obtain the K-DSN (bottom) module's prediction as</p><formula xml:id="formula_19">y(x) = U T g(x) = a T G(X)g(x) = k T (x)(CI + K ) −1 T ,</formula><p>where the kernel vector k(x) is so defined that its elements have values of k n (x) = k(x n , x) in which x n is a training sample and x is the current test sample.</p><p>For lth module in K-DCN where l ≥ 2, the kernel matrix is modified to</p><formula xml:id="formula_20">K = G([X |Y (l−1) | Y (l−2) | . . . Y (1) ])G T ([X |Y (l−1) |Y (l−2) | . . . Y (1) ]).</formula><p>The key advantages of K-DSN can be analyzed as follows. First, unlike DSN which needs to compute hidden units' output, the K-DSN does not need to explicitly compute hidden units' output G(X) or G([X |Y (l−1) |Y (l−2) | . . . Y (1) ]). When Gaussian kernels are used, kernel trick equivalently gives us an infinite number of hidden units without the need to compute them explicitly. Further, we no longer need to learn the lower-layer weight matrix W in DSN as described in <ref type="bibr" target="#b89">[102]</ref> and the kernel parameter (e.g., the single variance parameter σ in the Gaussian kernel) makes K-DSN much less subject to overfitting than DSN. Figure <ref type="figure">6</ref>.5 illustrates the basic architecture of a K-DSN using the Gaussian kernel and using three modules.</p><p>The entire K-DSN with Gaussian kernels is characterized by two sets of module-dependent hyper-parameters: σ (l) and C (l) the kernel smoothing parameter and regularization parameter, respectively. While both parameters are intuitive and their tuning (via line search or leaveone-out cross validation) is straightforward for a single bottom module, tuning the full network with all the modules is more difficult. For example, if the bottom module is tuned too well, then adding more modules would not benefit much. In contrast, when the lower modules are loosely tuned (i.e., relaxed from the results obtained from straightforward methods), the overall K-DSN often performs much better. The experimental results reported by Deng et al. <ref type="bibr" target="#b89">[102]</ref> are obtained using a set of empirically determined tuning schedules to adaptively regularize the K-DSN from bottom to top modules.</p><p>The K-DSN described here has a set of highly desirable properties from the machine learning and pattern recognition perspectives. It combines the power of deep learning and kernel learning in a principled way and unlike the basic DSN there is no longer nonconvex optimization problem involved in training the K-DSN. The computation steps make the K-DSN easier to scale up for parallel computing in distributed servers than the DSN and tensor-DSN. There are many fewer parameters in the K-DSN to tune than in the DSN, T-DSN, and DNN, and there is no need for pre-training. It is found in the study of <ref type="bibr" target="#b89">[102]</ref> that regularization plays a much more important role in the K-DSN than in the basic DSN and Tensor-DSN. Further, effective regularization schedules developed for learning the K-DSN weights can be motivated by intuitive insight from useful optimization tricks such as the heuristic in Rprop or resilient backpropagation algorithm <ref type="bibr" target="#b289">[302]</ref>.</p><p>However, as inherent in any kernel method, the scalability becomes an issue also for the K-DSN as the training and testing samples become very large. A solution is provided in the study by Huang et al. <ref type="bibr" target="#b158">[171]</ref>, based on the use of random Fourier features, which possess the strong theoretical property of approximating the Gaussian kernel while rendering efficient computation in both training and evaluation of the K-DSN with large training samples. It is empirically demonstrated that just like the conventional K-DSN exploiting rigorous Gaussian kernels, the use of random Fourier features also enables successful stacking of kernel modules to form a deep architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selected Applications in Speech</head><p>and Audio Processing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Acoustic modeling for speech recognition</head><p>As discussed in Section 2, speech recognition is the very first successful application of deep learning methods at an industry scale. This success is a result of close academic-industrial collaboration, initiated at Microsoft Research, with the involved researchers identifying and acutely attending to the industrial need for large-scale deployment <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b76">89,</ref><ref type="bibr" target="#b96">109,</ref><ref type="bibr" target="#b148">161,</ref><ref type="bibr" target="#b310">323,</ref><ref type="bibr" target="#b401">414]</ref>. It is also a result of carefully exploiting the strengths of the deep learning and the then-state-of-the-art speech recognition technology, including notably the highly efficient decoding techniques. Speech recognition has long been dominated by the GMM-HMM method, with an underlying shallow or flat generative model of contextdependent GMMs and HMMs (e.g., <ref type="bibr" target="#b79">[92,</ref><ref type="bibr" target="#b80">93,</ref><ref type="bibr" target="#b174">187,</ref><ref type="bibr" target="#b280">293]</ref>). Neural networks once were a popular approach but had not been competitive with the GMM-HMM <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b74">87,</ref><ref type="bibr" target="#b248">261,</ref><ref type="bibr" target="#b369">382]</ref>. Generative models with deep hidden dynamics likewise have also not been clearly competitive (e.g., <ref type="bibr" target="#b44">[45,</ref><ref type="bibr">73,</ref><ref type="bibr" target="#b95">108,</ref><ref type="bibr" target="#b269">282]</ref>).</p><p>Deep learning and the DNN started making their impact in speech recognition in 2010, after close collaborations between academic and industrial researchers; see reviews in <ref type="bibr" target="#b76">[89,</ref><ref type="bibr" target="#b148">161]</ref>. The collaborative work started in phone recognition tasks <ref type="bibr" target="#b76">[89,</ref><ref type="bibr" target="#b87">100,</ref><ref type="bibr" target="#b122">135,</ref><ref type="bibr" target="#b123">136,</ref><ref type="bibr" target="#b244">257,</ref><ref type="bibr" target="#b247">260,</ref><ref type="bibr" target="#b245">258,</ref><ref type="bibr" target="#b296">309,</ref><ref type="bibr" target="#b298">311,</ref><ref type="bibr" target="#b321">334]</ref>, demonstrating the power of hybrid DNN architectures discussed in Section 5 and of subsequent new architectures with convolutional and recurrent structure. The work also showed the importance of raw speech features of spectrogram -back from the long-popular MFCC features toward but not yet reaching the raw speech-waveform level <ref type="bibr" target="#b170">[183,</ref><ref type="bibr" target="#b314">327]</ref>. The collaboration continued to large vocabulary tasks with more convincing, highly positive results <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b81">94,</ref><ref type="bibr" target="#b76">89,</ref><ref type="bibr" target="#b148">161,</ref><ref type="bibr" target="#b186">199,</ref><ref type="bibr" target="#b182">195,</ref><ref type="bibr" target="#b210">223,</ref><ref type="bibr" target="#b310">323,</ref><ref type="bibr" target="#b340">353,</ref><ref type="bibr" target="#b386">399,</ref><ref type="bibr" target="#b401">414]</ref>. The success in large vocabulary speech recognition is in large part attributed to the use of a very large DNN output layer structured in the same way as the GMM-HMM speech units (senones), motivated partially by the speech researchers' desires to take advantage of the context-dependent phone modeling techniques that have been proven to work well in the GMM-HMM framework, and to keep the change of the already highly efficient decoder software's infrastructure developed for the GMM-HMM systems to a minimum. In the meantime, this body of work also demonstrated the possibility to reduce the need for the DBNlike pre-training in effective learning of DNNs when a large amount of labeled data is available. A combination of three factors helped to quickly spread the success of deep learning in speech recognition to the entire speech industry and academia: (1) significantly lowered errors compared with the then-state-of-the-art GMM-HMM systems;</p><p>(2) minimal decoder changes required to deploy the new DNN-based speech recognizer due to the use of senones as the DNN output; and (3) reduced system complexity empowered by the DNN's strong modeling power. By the ICASSP-2013 timeframe, at least 15 major speech recognition groups worldwide confirmed experimentally the success of DNNs with very large tasks and with the use of raw speech spectral features other than MFCCs. The most notable groups include major industrial speech labs worldwide: Microsoft <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b76">89,</ref><ref type="bibr" target="#b81">94,</ref><ref type="bibr" target="#b311">324,</ref><ref type="bibr" target="#b386">399,</ref><ref type="bibr" target="#b417">430]</ref>, IBM <ref type="bibr" target="#b182">[195,</ref><ref type="bibr" target="#b296">309,</ref><ref type="bibr" target="#b298">311,</ref><ref type="bibr" target="#b294">307,</ref><ref type="bibr" target="#b304">317]</ref>, Google <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b137">150,</ref><ref type="bibr" target="#b171">184,</ref><ref type="bibr" target="#b210">223]</ref>, iFlyTek, and Baidu. Their results represent a new state-of-the-art in speech recognition widely deployed in these companies' voice products and services with extensive media coverage in recent years.</p><p>In the remainder of this chapter, we review a wide range of speech recognition work based on deep learning methods according to several major themes expressed in the section titles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Back to primitive spectral features of speech</head><p>Deep learning, also referred as representation learning or (unsupervised) feature learning, sets an important goal of automatic discovery of powerful features from raw input data independent of application domains. For speech feature learning and for speech recognition, this goal is condensed to the use of primitive spectral or possibly waveform features. Over the past 30 years or so, largely "hand-crafted" transformations of speech spectrogram have led to significant accuracy improvements in the GMM-based HMM systems, despite the known loss of information from the raw speech data. The most successful transformation is the non-adaptive cosine transform, which gave rise to Mel-frequency cepstral coefficients (MFCC) features. The cosine transform approximately de-correlates feature components, which is important for the use of GMMs with diagonal covariance matrices. However, when GMMs are replaced by deep learning models such as DNNs, deep belief nets (DBNs), or deep autoencoders, such de-correlation becomes irrelevant due to the very strength of the deep learning methods in modeling data correlation. As discussed in detail in Section 4, early work of <ref type="bibr" target="#b87">[100]</ref> demonstrated this strength and in particular the benefit of spectrograms over MFCCs in effective coding of bottleneck speech features using autoencoders in an unsupervised manner.</p><p>The pipeline from speech waveforms (raw speech features) to MFCCs and their temporal differences goes through intermediate stages of log-spectra and then (Mel-warped) filter-banks, with learned parameters based on the data. An important character of deep learning is to move away from separate design of feature representations and of classifiers. This idea of jointly learning classifier and feature transformation for speech recognition was already explored in early studies on the GMM-HMM based systems; e.g., <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b286">299]</ref>. However, greater speech recognition performance gain is obtained only recently in the recognizers empowered by deep learning methods. For example, Mohamed et al. <ref type="bibr" target="#b246">[259]</ref>, Li et al. <ref type="bibr" target="#b208">[221]</ref>, and Deng et al. <ref type="bibr" target="#b81">[94]</ref> showed significantly lowered speech recognition errors using large-scale DNNs when moving from the MFCC features back to more primitive (Mel-scaled) filter-bank features. These results indicate that DNNs can learn a better transformation than the original fixed cosine transform from the Mel-scaled filter-bank features.</p><p>Compared with MFCCs, "raw" spectral features not only retain more information, but also enable the use of convolution and pooling operations to represent and handle some typical speech variability -e.g., vocal tract length differences across speakers, distinct speaking styles causing formant undershoot or overshoot, etc. -expressed explicitly in the frequency domain. For example, the convolutional neural network (CNN) can only be meaningfully and effectively applied to speech recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b81">94]</ref> when spectral features, instead of MFCC features, are used.</p><p>More recently, Sainath et al. <ref type="bibr" target="#b294">[307]</ref> went one step further toward raw features by learning the parameters that define the filter-banks on power spectra. That is, rather than using Mel-warped filter-bank features as the input features as in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b208">221]</ref>, the weights corresponding to the Mel-scale filters are only used to initialize the parameters, which are subsequently learned together with the rest of the deep network as the classifier. The overall architecture of the jointly learned feature generator and classifier is shown in Figure <ref type="figure">7</ref>.1. Substantial speech recognition error reduction is reported in <ref type="bibr" target="#b294">[307]</ref>.</p><p>It has been shown that not only learning the spectral aspect of the features are beneficial for speech recognition, learning the temporal aspect of the features is also helpful <ref type="bibr" target="#b319">[332]</ref>. Further, Yu et al. <ref type="bibr" target="#b413">[426]</ref> carefully analyzed the properties of different layers in the DNN as the layer-wise extracted features starting from the lower raw filter-bank features. They found that the improved speech recognition accuracy achieved by the DNNs partially attributes to DNN's ability to extract discriminative internal representations that are robust to the many sources of variability in speech signals. They also show that these representations become increasingly insensitive to small perturbations in the input at higher layers, which helps to achieve better speech recognition accuracy.</p><p>To the extreme end, deep learning would promote to use the lowest level of raw features of speech, i.e., speech sound waveforms, for speech recognition, and learn the transformation automatically. As an initial attempt toward this goal the study carried out by Jaitly and Hinton <ref type="bibr" target="#b170">[183]</ref> makes use of speech sound waves as the raw input feature to an RBM with a convolutional structure as the classifier. With the use of rectified linear units in the hidden layer <ref type="bibr" target="#b117">[130]</ref>, it is possible, to a limited extent, to automatically normalize the amplitude variation in the waveform signal. Although the final results are disappointing, the work shows that much work is needed along this direction. For example, just as demonstrated by Sainath et al. <ref type="bibr" target="#b294">[307]</ref> that the use of raw spectra as features requires additional attention in normalization than MFCCs, the use of speech waveforms demands even more attention in normalization <ref type="bibr" target="#b314">[327]</ref>. This is true for both GMM-based and deep learning based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">The DNN-HMM architecture versus use of DNN-derived features</head><p>Another major theme in the recent studies reported in the literature on applying deep learning methods to speech recognition is two disparate ways of using the DNN: (1) Direct applications of the DNN-HMM architecture as discussed in Section 5.3 to perform speech recognition; and (2) The use of DNNs to extract or derive features, which are then fed into a separate sequence classifier. In the speech recognition literature <ref type="bibr" target="#b41">[42]</ref>, a system, in which a neural network's output is directly used to estimate the emission probabilities of an HMM, is often called an ANN/HMM hybrid system. This should be distinguished from the use of "hybrid" in Section 5 and throughout this monograph, where a hybrid of unsupervised pre-training and of supervised fine tuning is exploited to learn the parameters of DNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2.1">The DNN-HMM architecture as a recognizer</head><p>An early DNN-HMM architecture <ref type="bibr" target="#b244">[257]</ref> was presented at the NIPS Workshop <ref type="bibr" target="#b96">[109]</ref>, developed, analyzed, and assisted by University of Toronto and MSR speech researchers. In this work, a five-layer DNN (called the DBN in the paper) was used to replace the Gaussian mixture models in the GMM-HMM system, and the monophone state was used as the modeling unit. Although monophones are generally accepted as a weaker phonetic representation than triphones, the DNN-HMM approach with monophones was shown to achieve higher phone recognition accuracy than the state-of-the-art triphone GMM-HMM systems. Further, the DNN results were found to be slightly superior to the then-best-performing single system based on the generative hidden trajectory model (HTM) in the literature <ref type="bibr" target="#b92">[105,</ref><ref type="bibr" target="#b95">108]</ref> evaluated on the same, commonly used TIMIT task by many speech researchers <ref type="bibr" target="#b94">[107,</ref><ref type="bibr" target="#b95">108,</ref><ref type="bibr" target="#b261">274,</ref><ref type="bibr" target="#b300">313]</ref>. At MSR, Redmond, the error patterns produced by these two separate systems (the DNN vs. the HTM) were carefully analyzed and found to be very different, reflecting distinct core capabilities of the two approaches and igniting intensive further studies on the DNN-HMM approach described below.</p><p>MSR and University of Toronto researchers <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b401">414]</ref> extended the DNN-HMM system from the monophone phonetic representation of the DNN outputs to the triphone or context-dependent counterpart and from phone recognition to large vocabulary speech recognition. Experiments conducted at MSR on the 24-hour and 48-hour Bing mobile voice search datasets collected under the real usage scenario demonstrate that the context-dependent DNN-HMM significantly outperforms the state-of-the-art GMM-HMM system. Three factors, in addition to the use of the DNN, contribute to the success: the use of tied triphones as the DNN modeling units, the use of the best available tri-phone GMM-HMM to generate the tri-phone state alignment, and the effective exploitation of a long window of input features. Experiments also indicate that the decoding time of a five-layer DNN-HMM is almost the same as that of the state-of-the-art triphone GMM-HMM.</p><p>The success was quickly extended to large vocabulary speech recognition tasks with hundreds and even thousands of hours of training set and with thousands of tri-phone states, including the Switchboard and Broadcast News databases, and Google's voice search and YouTube tasks <ref type="bibr" target="#b81">[94,</ref><ref type="bibr" target="#b148">161,</ref><ref type="bibr" target="#b171">184,</ref><ref type="bibr" target="#b296">309,</ref><ref type="bibr" target="#b298">311,</ref><ref type="bibr" target="#b311">324]</ref>. For example, on the Switchboard benchmark, the context-dependent DNN-HMM (CD-DNN-HMM) is shown to cut error by one third compared to the state-of-the-art GMM-HMM system <ref type="bibr" target="#b310">[323]</ref>. As a summary, we show in Table <ref type="table">7</ref>.1 some quantitative recognition error rates in relatively early literature produced by the basic DNN-HMM architecture in comparison with those by the previous state-of-the-art systems based on the generative models. (More advanced architectures have produced better results than shown here). Note from sub-tables A to D, the training data are increased approximately one order of magnitude from one task to the next. Not only the computation scales up well (i.e., almost linearly) with the training size, but most importantly the relative error rate reduction increases substantially with increasing amounts of training data -from approximately 10% to 20%, and then to 30%. This set of results highlight the strongly desirable properties of the DNN-based methods, despite the conceptual simplicity of the overall DNN-HMM architecture and some known weaknesses. developed over the past 20 some years may not be directly applicable to the new systems although similar techniques have been recently developed for DNN-HMMs. To remedy this problem, the "tandem" approach, developed originally by Hermansky et al. <ref type="bibr" target="#b141">[154]</ref>, has been adopted, where the output of the neural networks in the form of posterior probabilities for phone classes, are used, often in conjunction with the acoustic features to form new augmented input features, in a separate GMM-HMM system.</p><p>This tandem approach is used by Vinyals and Ravuri <ref type="bibr" target="#b366">[379]</ref> where a DNN's outputs are extracted to serve as the features for mismatched noisy speech. It is reported that DNNs outperform the neural networks with a single hidden layer under the clean condition, but the gains slowly diminish as the noise level is increased. Furthermore, using MFCCs in conjunction with the posteriors computed from DNNs outperforms using the DNN features alone in low to moderate noise conditions with the tandem architecture. Comparisons of such tandem approach with the direct DNN-HMM approach are made by Tüske et al. <ref type="bibr" target="#b355">[368]</ref> and Imseng et al. <ref type="bibr" target="#b169">[182]</ref>.</p><p>An alternative way of extracting the DNN features is to use the "bottleneck" layer, which is narrower than other layers in the DNN, to restrict the capacity of the network. Then, such bottleneck features are fed to a GMM-HMM system, often in conjunction with the original acoustic features and some dimensionality reduction techniques. The bottleneck features derived from the DNN are believed to capture information complementary to conventional acoustic features derived from the short-time spectra of the input. A speech recognizer based on the above bottleneck feature approach is built by Yu and Seltzer <ref type="bibr" target="#b412">[425]</ref>, with the overall architecture shown in Figure <ref type="figure">7</ref>.2. Several variants of the DNN-based bottleneck-feature approach have been explored; see details in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b124">137,</ref><ref type="bibr" target="#b188">201,</ref><ref type="bibr" target="#b272">285,</ref><ref type="bibr" target="#b295">308,</ref><ref type="bibr" target="#b355">368]</ref>.</p><p>Yet another method to derive the features from the DNN is to feed its top-most hidden layer as the new features for a separate speech recognizer. In <ref type="bibr" target="#b386">[399]</ref>, a GMM-HMM is used as such a recognizer, and the high-dimensional, DNN-derived features are subject to dimensionality reduction before feeding them into the recognizer. More recently, a recurrent neural network (RNN) is used as the "backend" recognizer receiving the high-dimensional, DNN-derived features as the input without dimensionality reduction <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b72">85]</ref>. These studies also show that the use of the top-most hidden layer of the DNN as features is better than other hidden layers and also better than the output layer in terms of recognition accuracy for the RNN sequence classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">Noise robustness by deep learning</head><p>The study of noise robustness in speech recognition has a long history, mostly before the recent rise of deep learning. One major contributing factor to the often observed brittleness of speech recognition technology is the inability of the standard GMM-HMM-based acoustic model to accurately model noise-distorted speech test data that differs in character from the training data, which may or may not be distorted by noise. A wide range of noise-robust techniques developed over past 30 years can be analyzed and categorized using five different criteria: (1) feature-domain versus model-domain processing, (2) the use of prior knowledge about the acoustic environment distortion, (3) the use of explicit environment-distortion models, (4) deterministic versus uncertainty processing, and (5) the use of acoustic models trained jointly with the same feature enhancement or model adaptation process used in the testing stage. See a comprehensive review in <ref type="bibr" target="#b207">[220]</ref> and some additional review literature or original work in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr">82,</ref><ref type="bibr" target="#b106">119,</ref><ref type="bibr" target="#b127">140,</ref><ref type="bibr" target="#b217">230,</ref><ref type="bibr" target="#b357">370,</ref><ref type="bibr" target="#b391">404,</ref><ref type="bibr" target="#b418">431,</ref><ref type="bibr" target="#b431">444]</ref>.</p><p>Many of the model-domain techniques developed for GMM-HMMs (e.g., model-domain noise robustness techniques surveyed by Li et al. <ref type="bibr" target="#b207">[220]</ref> and Gales <ref type="bibr" target="#b106">[119]</ref>) are not directly applicable to the new deep learning models for speech recognition. The feature-domain techniques, however, can be directly applied to the DNN system. A detailed investigation of the use of DNNs for noise robust speech recognition in the feature domain was reported by Seltzer et al. <ref type="bibr" target="#b312">[325]</ref>, who applied the C-MMSE <ref type="bibr" target="#b402">[415]</ref> feature enhancement algorithm on the input feature used in the DNN. By processing both the training and testing data with the same algorithm, any consistent errors or artifacts introduced by the enhancement algorithm can be learned by the DNN-HMM recognizer. This study also successfully explored the use of the noise aware training paradigm for training the DNN, where each observation was augmented with an estimate of the noise. Strong results were obtained on the Aurora4 task. More recently, Kashiwagi et al. <ref type="bibr" target="#b178">[191]</ref> applied the SPLICE feature enhancement technique [82] to a DNN speech recognizer. In that study the DNN's output layer was determined on clean data instead of on noisy data as in the study reported by Seltzer et al. <ref type="bibr" target="#b312">[325]</ref>.</p><p>Besides DNN, other deep architectures have also been proposed to perform feature enhancement and noise-robust speech recognition. For example, Mass et al. <ref type="bibr" target="#b222">[235]</ref> applied a deep recurrent auto encoder neural network to remove noise in the input features for robust speech recognition. The model was trained on stereo (noisy and clean) speech features to predict clean features given noisy input, similar to the SPLICE setup but using a deep model instead of a GMM. Vinyals and Ravuri <ref type="bibr" target="#b366">[379]</ref> investigated the tandem approaches to noise-robust speech recognition, where DNNs were trained directly with noisy speech to generate posterior features. Finally, Rennie et al. <ref type="bibr" target="#b287">[300]</ref> explored the use of a version of the RBM, called the factorial hidden RBM, for noise-robust speech recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.4">Output representations in the DNN</head><p>Most deep learning methods for speech recognition and other information processing applications have focused on learning representations from input acoustic features without paying attention to output representations. The recent 2013 NIPS Workshop on Learning Output Representations (http://nips.cc/Conferences/2013/Program/ event.php?ID=3714) was dedicated to bridging this gap. For example, the Deep Visual-Semantic Embedding Model described in <ref type="bibr" target="#b104">[117]</ref>, to be discussed more in Section 11) exploits continuous-valued output representations obtained from the text embeddings to assist in the branch of the deep network for classifying images. For speech recognition, the importance of designing effective linguistic representations for the output layers of deep networks is highlighted in <ref type="bibr">[79]</ref>.</p><p>Most current DNN systems use a high-dimensional output representation to match the context-dependent phonetic states in the HMMs. For this reason, the output layer evaluation can cost 1/3 of the total computation time. To improve the decoding speed, techniques such as low-rank approximation is typically applied to the output layer. In <ref type="bibr" target="#b297">[310]</ref> and <ref type="bibr" target="#b384">[397]</ref>, the DNN with high-dimensional output layer was trained first. The singular value decomposition (SVD)-based dimension reduction technique was then performed on the large output-layer matrix. The resulting matrices are further combined and as the result the original large weight matrix is approximated by a product of two much smaller matrices. This technique in essence converts the original large output layer to two layers -a bottleneck linear layer and a nonlinear output layer -both with smaller weight matrices. The converted DNN with reduced dimensionality is further refined. The experimental results show that no speech recognition accuracy reduction was observed even when the size is cut to half, while the run-time computation is significantly reduced.</p><p>The output representations for speech recognition can benefit from the structured design of the symbolic or phonological units of speech as presented in <ref type="bibr">[79]</ref>. The rich phonological structure of symbolic nature in human speech has been well known for many years. Likewise, it has also been well understood for a long time that the use of phonetic or its finer state sequences, even with contextual dependency, in engineering speech recognition systems, is inadequate in representing such rich structure <ref type="bibr" target="#b73">[86,</ref><ref type="bibr" target="#b260">273,</ref><ref type="bibr" target="#b342">355]</ref>, and thus leaving a promising open direction to improve the speech recognition systems' performance. Basic theories about the internal structure of speech sounds and their relevance to speech recognition technology in terms of the specification, design, and learning of possible output representations of the underlying speech model for speech target sequences are surveyed in [76] and more recently in <ref type="bibr">[79]</ref>.</p><p>There has been a growing body of deep learning work in speech recognition with their focus placed on designing output representations related to linguistic structure. In <ref type="bibr" target="#b370">[383,</ref><ref type="bibr" target="#b371">384]</ref>, a limitation of the output representation design, based on the context-dependent phone units as proposed in <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref>, is recognized and a solution is offered. The root cause of this limitation is that all context-dependent phone states within a cluster created by the decision tree share the same set of parameters and this reduces its resolution power for fine-grained states during the decoding phase. The solution proposed formulates output representations of the context-dependent DNN as an instance of the canonical state modeling technique, making use of broad phonetic classes. First, triphones are clustered into multiple sets of shorter biphones using broad phone contexts. Then, the DNN is trained to discriminate the bi-phones within each set. Logistic regression is used to transform the canonical states into the detailed triphone state output probabilities. That is, the overall design of the output representation of the context-dependent DNN is hierarchical in nature, solving both the data sparseness and low-resolution problems at the same time.</p><p>Related work on designing the output linguistic representations for speech recognition can be found in <ref type="bibr" target="#b184">[197]</ref> and in <ref type="bibr" target="#b228">[241]</ref>. While the designs are in the context of GMM-HMM-based speech recognition systems, they both can be extended to deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.5">Adaptation of the DNN-based speech recognizers</head><p>The DNN-HMM is an advanced version of the artificial neural network and HMM "hybrid" system developed in 1990s, for which several adaptation techniques have been developed. Most of these techniques are based on linear transformation of the network weights of either input or output layers. A number of exploratory studies on DNN adaptation made use of the same or related linear transformation methods <ref type="bibr" target="#b210">[223,</ref><ref type="bibr" target="#b388">401,</ref><ref type="bibr" target="#b389">402]</ref>. However, compared with the earlier narrower and shallower neural network systems, the DNN-HMM has significantly more parameters due to wider and deeper hidden layers used and the much larger output layer designed to model context dependent phones and states. This difference casts special challenges to adapting the DNN-HMM, especially when the adaptation data is small. Here we discuss representative recent studies on overcoming such challenges in adapting the large-sized DNN weights in distinct ways.</p><p>Yu et al. <ref type="bibr" target="#b417">[430]</ref> proposed a regularized adaptation technique for DNNs. It adapts the DNN weights conservatively by forcing the distribution estimated from the adapted model to be close to that estimated from those before the adaptation. This constraint is realized by adding Kullback-Leibler divergence (KLD) regularization to the adaptation criterion. This type of regularization is shown to be equivalent to a modification of the target distribution in the conventional backpropagation algorithm and thus the training of the DNN remains largely unchanged. The new target distribution is derived to be a linear interpolation of the distribution estimated from the model before adaptation and the ground truth alignment of the adaptation data. This interpolation prevents overtraining by keeping the adapted model from straying too far from the speaker-independent model. This type of adaptation differs from L2 regularization, which constrains the model parameters themselves rather than the output probabilities.</p><p>In <ref type="bibr" target="#b317">[330]</ref>, adaptation of the DNN was applied not on the conventional network weights but on the hidden activation functions. In this way, the main limitation of current adaptation techniques based on adaptable linear transformation of the network weights in either the input or the output layer is effectively overcome, since the new method only needs to adapt a more limited number of hidden activation functions.</p><p>Several studies were carried out on unsupervised or semi-supervised adaptation of DNN acoustic models with different types of input features with success <ref type="bibr" target="#b210">[223,</ref><ref type="bibr" target="#b392">405]</ref>.</p><p>Most recently, Saon et al. <ref type="bibr" target="#b304">[317]</ref> explored a new and highly effective method in adapting DNNs for speech recognition. The method combined I-vector features with fMLLR (feature-domain max-likelihood linear regression) features as the input into a DNN. I-vectors or (speaker) identity vectors are commonly used for speaker verification and speaker recognition applications, as they encapsulate relevant information about a speaker's identity in a low-dimensional feature vector. The fMLLR is an effective adaptation technique developed for GMM-HMM systems. Since I-vectors do not obey locality in frequency, they must be combined carefully with the fMLLR features that obey locality. The architecture of the multi-scale CNN-DNN was shown to be effective for the combination of these two different types of features. During both training and decoding, the speaker-specific I-vector was appended to the frame-based fMLLR features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.6">Better architectures and nonlinear units</head><p>Over recent years, since the success of the (fully-connected) DNN-HMM hybrid system was demonstrated in <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b96">109,</ref><ref type="bibr" target="#b148">161,</ref><ref type="bibr" target="#b244">257,</ref><ref type="bibr" target="#b245">258,</ref><ref type="bibr" target="#b295">308,</ref><ref type="bibr" target="#b296">309,</ref><ref type="bibr" target="#b311">324,</ref><ref type="bibr" target="#b416">429]</ref>, many new architectures and nonlinear units have been proposed and evaluated for speech recognition. Here we provide an overview of this progress, extending the overview provided in <ref type="bibr" target="#b76">[89]</ref>.</p><p>The tensor version of the DNN is reported by Yu et al. <ref type="bibr" target="#b408">[421,</ref><ref type="bibr" target="#b409">422]</ref>, which extends the conventional DNN by replacing one or more of its layers with a double-projection layer and a tensor layer. In the doubleprojection layer, each input vector is projected into two nonlinear subspaces. In the tensor layer, two subspace projections interact with each other and jointly predict the next layer in the overall deep architecture. An approach is developed to map the tensor layers to the conventional sigmoid layers so that the former can be treated and trained in a similar way to the latter. With this mapping the tensor version of the DNN can be treated as the DNN augmented with double-projection layers so that the backpropagation learning algorithm can be cleanly derived and relatively easily implemented.</p><p>A related architecture to the above is the tensor version of the DSN described in Section 6, also usefully applied to speech classification and recognition <ref type="bibr" target="#b167">[180,</ref><ref type="bibr" target="#b168">181]</ref>. The same approach applies to mapping the tensor layers (i.e., the upper layer in each of the many modules in the DSN context) to the conventional sigmoid layers. Again, this mapping simplifies the training algorithm so that it becomes not so far apart from that for the DSN.</p><p>As discussed in Section 3.2, the concept of convolution in time was originated in the TDNN (time-delay neural network) as a shallow neural network <ref type="bibr" target="#b189">[202,</ref><ref type="bibr" target="#b369">382]</ref> developed during early days of speech recognition. Only recently and when deep architectures (e.g. deep convolutional neural network or deep CNN) were used, it has been found that frequency-dimension weight sharing is more effective for high-performance phone recognition, when the HMM is used to handle the time variability, than time-domain weight sharing as in the previous TDNN in which the HMM was not used <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr">81]</ref>. These studies also show that designing the pooling scheme in the deep CNN to properly trade-off between invariance to vocal tract length and discrimination among speech sounds, together with a regularization technique of "dropout" <ref type="bibr" target="#b153">[166]</ref>, leads to even better phone recognition performance. This set of work further points to the direction of trading-off between trajectory discrimination and invariance expressed in the whole dynamic pattern of speech defined in mixed time and frequency domains using convolution and pooling. Moreover, the most recent studies reported in <ref type="bibr" target="#b293">[306,</ref><ref type="bibr" target="#b294">307,</ref><ref type="bibr" target="#b299">312]</ref> show that CNNs also benefit large vocabulary continuous speech recognition. They further demonstrate that multiple convolutional layers provide even more improvement when the convolutional layers use a large number of convolution kernels or feature maps. In particular, Sainath et al. <ref type="bibr" target="#b293">[306]</ref> extensively explored many variants of the deep CNN. In combination with several novel methods the deep CNN is shown to produce state of the art results in a few large vocabulary speech recognition tasks.</p><p>In addition to the DNN, CNN, and DSN, as well as their tensor versions, other deep models have also been developed and reported in the literature for speech recognition. For example, the deep-structured CRF, which stacks many layers of CRFs, have been usefully applied to the task of language identification <ref type="bibr" target="#b416">[429]</ref>, phone recognition <ref type="bibr" target="#b397">[410]</ref>, sequential labeling in natural language processing <ref type="bibr" target="#b415">[428]</ref>, and confidence calibration in speech recognition <ref type="bibr" target="#b410">[423]</ref>. More recently, Demuynck and Triefenbach <ref type="bibr" target="#b69">[70]</ref> developed the deep GMM architecture, where the aspects of DNNs that lead to strong performance are extracted and applied to build hierarchical GMMs. They show that by going "deep and wide" and feeding windowed probabilities of a lower layer of GMMs to a higher layer of GMMs, the performance of the deep-GMM system can be made comparable to a DNN. One advantage of staying in the GMM space is that the decades of work in GMM adaptation and discriminative learning remains applicable.</p><p>Perhaps the most notable deep architecture among all is the recurrent neural network (RNN) as well as its stacked or deep versions <ref type="bibr" target="#b122">[135,</ref><ref type="bibr" target="#b123">136,</ref><ref type="bibr" target="#b140">153,</ref><ref type="bibr" target="#b266">279,</ref><ref type="bibr" target="#b364">377]</ref>. While the RNN saw its early success in phone recognition <ref type="bibr" target="#b291">[304]</ref>, it was not easy to duplicate due to the intricacy in training, let alone to scale up for larger speech recognition tasks. Learning algorithms for the RNN have been dramatically improved since then, and much better results have been obtained recently using the RNN <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b121">134,</ref><ref type="bibr" target="#b222">235]</ref>, especially when the bi-directional LSTM (long short-term memory) is used <ref type="bibr" target="#b122">[135,</ref><ref type="bibr" target="#b123">136]</ref>. The basic information flow in the bi-directional RNN and a cell of LSTM is shown in Figures 7.  Learning the RNN parameters is known to be difficult due to vanishing or exploding gradients <ref type="bibr" target="#b267">[280]</ref>. Chen and Deng <ref type="bibr" target="#b47">[48]</ref> and Deng and  Chen <ref type="bibr" target="#b72">[85]</ref> developed a primal-dual training method that formulates the learning of the RNN as a formal optimization problem, where cross entropy is maximized subject to the condition that the infinity norm of the recurrent matrix of the RNN is less than a fixed value to guarantee the stability of RNN dynamics. Experimental results on phone recognition demonstrate: (1) the primal-dual technique is highly effective in learning RNNs, with superior performance to the earlier heuristic method of truncating the size of the gradient; (2) The use of a DNN to compute high-level features of speech data to feed into the RNN gives much higher accuracy than without using the DNN; and (3) The accuracy drops progressively as the DNN features are extracted from higher to lower hidden layers of the DNN.</p><p>A special case of the RNN is reservoir models or echo state networks, where the output layers are fixed to be linear instead of nonlinear as in the regular RNN, and where the recurrent matrices are carefully designed but not learned. The input matrices are also fixed and not learned, due partly to the difficulty of learning. Only the weight matrices between the hidden and output layers are learned. Since the output layer is linear, the learning is very efficient and with global optimum achievable by a closed-form solution. But due to the fact that many parameters are not learned, the hidden layer needs to be very large in order to obtain good results. Triefenbach et al. <ref type="bibr" target="#b352">[365]</ref> applied such models to phone recognition, with reasonably good accuracy obtained.</p><p>Palangi et al. <ref type="bibr" target="#b263">[276]</ref> presented an improved version of the reservoir model by learning both the input and recurrent matrices which were fixed in the previous model that makes use of the linear output (or readout) units to simplify the learning of only the output matrix in the RNN. Rather, a special technique is devised that takes advantage of the linearity in the output units in the reservoir model to learn the input and recurrent matrices. Compared with the backpropagation through time (BPTT) algorithm commonly used in learning the general RNNs, the proposed technique makes use of the linearity in the output units to provide constraints among various matrices in the RNN, enabling the computation of the gradients as the learning signal in an analytical form instead of by recursion as in the BPTT.</p><p>In addition to the recent innovations in better architectures of deep learning models for speech recognition reviewed above, there is also a growing body of work on developing and implementing better nonlinear units. Although sigmoidal and tanh functions are the most commonly used nonlinear types in DNNs their limitations are well known. For example, it is slow to learn the whole network due to weak gradients when the units are close to saturation in both directions. Jaitly and Hinton <ref type="bibr" target="#b170">[183]</ref> appear to be the first to apply the rectified linear units (ReLU) in the DNNs to speech recognition to overcome the weakness of the sigmoidal units. ReLU refers to the units in a neural network that use the activation function of f (x) = max(0, x). Dahl et al. <ref type="bibr" target="#b64">[65]</ref> and Mass et al. <ref type="bibr" target="#b221">[234]</ref> successfully applied ReLU to large vocabulary speech recognition, with the best accuracy obtained when combining ReLU with the "Dropout" regularization technique.</p><p>Another new type of DNN units demonstrated more recently to be useful for speech recognition is the "maxout" units, which were used for forming the deep maxout network as described in <ref type="bibr" target="#b231">[244]</ref>. A deep maxout network consists of multiple layers which generate hidden activations via the maximum or "maxout" operation over a fixed number of weighted inputs called a "group." This is the same operation as the max pooling used in the CNN as discussed earlier for both speech recognition and computer vision. The maximal value within each group is taken as the output from the previous layer. Most recently, Zhang et al. <ref type="bibr" target="#b428">[441]</ref> generalize the above "maxout" units to two new types. The "soft-maxout" type of units replace the original max operation with the soft-max function. The second, p-norm type of units used the nonlinearity of y = x p . It is shown experimentally that the p-norm units with p = 2 perform consistently better than the maxout, tanh, and ReLU units. In Gulcehre et al. <ref type="bibr" target="#b125">[138]</ref>, techniques that automatically learn the p-norm was proposed and investigated.</p><p>Finally, Srivastava et al. <ref type="bibr" target="#b337">[350]</ref> propose yet another new type of nonlinear units, called winner-take-all units. Here, local competition among neighboring neurons are incorporated into the otherwise regular feedforward architecture, which is then trained via backpropagation with different gradients than the normal one. Winner-take-all is an interesting new form of nonlinearity, and it forms groups of (typically two) neurons where all the neurons in a group are made zero-valued except the one with the largest value. Experiments show that the network does not forget as much as networks with standard sigmoidal nonlinearity. This new type of nonlinear units are yet to be evaluated in speech recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.7">Better optimization and regularization</head><p>Another area where significant advances are made recently in applying deep learning to acoustic model for speech recognition is on optimization criteria and methods, as well as on the related regularization techniques to help prevent overfitting during the deep network training.</p><p>One of the early studies on DNNs for speech recognition, conducted at Microsoft Research and reported in <ref type="bibr" target="#b247">[260]</ref>, first recognizes the mismatch between the desired error rate and the cross-entropy training criterion in the conventional DNN training. The solution is provided by replacing the frame-based, cross-entropy training criterion with the fullsequence-based maximum mutual information optimization objective, in a similar way to defining the training objective for the shallow neural network interfaced with an HMM <ref type="bibr" target="#b181">[194]</ref>. Equivalently, this amounts to putting the model of conditional random field (CRF) at the top of the DNN, replacing the original softmax layer which naturally leads to cross entropy. (Note the DNN was called the DBN in the paper). This new sequential discriminative learning technique is developed to jointly optimize the DNN weights, CRF transition weights, and bi-phone language model. Importantly, the speech task is defined in TIMIT, with the use of a simple bi-phone-gram "language" model. The simplicity of the bi-gram language model enables the full-sequence training to carry out without the need to use lattices, drastically reducing the training complexity.</p><p>As another way to motivate the full-sequence training method of <ref type="bibr" target="#b247">[260]</ref>, we note that the earlier DNN phone recognition experiments made use of the standard frame-based objective function in static pattern classification, cross-entropy, to optimize the DNN weights. The transition parameters and language model scores were obtained from an HMM and were trained independently of the DNN weights. However, it has been known during the long history of the HMM research that sequence classification criteria can be very helpful in improving speech and phone recognition accuracy. This is because the sequence classification criteria are more directly correlated with the performance measure (e.g., the overall word or phone error rate) than frame-level criteria. More specifically, the use of frame-level cross entropy to train the DNN for phone sequence recognition does not explicitly take into account the fact that the neighboring frames have smaller distances between the assigned probability distributions over phone class labels. To overcome this deficiency, one can optimize the conditional probability of the whole sequence of labels, given the whole visible feature utterance or equivalently the hidden feature sequence extracted by DNN. To optimize the log conditional probability on the training data, the gradient can be taken over the activation parameters, transition parameters and lower-layer weights, and then pursue back-propagation of the error defined at the sentence level. We remark that in a much earlier study <ref type="bibr" target="#b199">[212]</ref>, combining a neural network with a CRF-like structure was done, where the mathematical formulation appears to include CRFs as a special case. Also, the benefit of using the full-sequence classification criteria was shown earlier on shallow neural networks in <ref type="bibr" target="#b181">[194,</ref><ref type="bibr" target="#b278">291]</ref>.</p><p>In implementing the above full-sequence learning algorithm for the DNN system as described in <ref type="bibr" target="#b247">[260]</ref>, the DNN weights are initialized using the frame-level cross entropy as the objective. The transition parameters are initialized from the combination of the HMM transition matrices and the "bi-phone language" model scores, and are then further optimized by tuning the transition features while fixing the DNN weights before the joint optimization. Using joint optimization with careful scheduling to reduce overfitting, it is shown that the full-sequence training outperforms the DNN trained with frame-level cross entropy by approximately 5% relative <ref type="bibr" target="#b247">[260]</ref>. Without the effort to reduce overfitting, it is found that the DNN trained with MMI is much more prone to overfitting than that trained with frame-level cross entropy. This is because the correlations across frames in speech tend to be different among the training, development, and test data. Importantly, such differences do not show when frame-based objective functions are used for training.</p><p>For large vocabulary speech recognition where more complex language models are in use, the optimization methods for full-sequence training of the DNN-HMM are much more sophisticated. Kingsbury et al. <ref type="bibr" target="#b182">[195]</ref> reported the first success of such training using parallel, second-order, Hessian-free optimization techniques, which are carefully implemented for large vocabulary speech recognition. Sainath et al. <ref type="bibr" target="#b292">[305]</ref> improved and speeded up the Hessian-free techniques by reducing the number of Krylov subspace solver iterations <ref type="bibr" target="#b365">[378]</ref>, which are used for implicit estimation of the Hessian. They also use sampling methods to decrease the amount of training data to speed up the training. While the batch-mode, second-order Hessian-free techniques prove successful for full-sequence training of large-scale DNN-HMM systems, the success of the first-order stochastic gradient descent methods is also reported recently <ref type="bibr" target="#b340">[353]</ref>. It is found that heuristics are needed to handle the problem of lattice sparseness. That is, the DNN must be adjusted to the updated numerator lattices by additional iterations of frame-based cross-entropy training. Further, artificial silence arcs need to be added to the denominator lattices, or the maximum mutual information objective function needs to be smoothed with the frame-based cross entropy objective. The conclusion is that for large vocabulary speech recognition tasks with sparse lattices, the implementation of the sequence training requires much greater engineering skills than the small tasks such as reported in <ref type="bibr" target="#b247">[260]</ref>, although the objective function as well as the gradient derivation are essentially the same. Similar conclusions are reached by Vesely et al. <ref type="bibr" target="#b361">[374]</ref> when carrying out full-sequence training of DNN-HMMs for large-vocabulary speech recognition. However, different heuristics from <ref type="bibr" target="#b340">[353]</ref> are shown to be effective in the training. Separately, Wiesler et al. <ref type="bibr" target="#b377">[390]</ref> investigated the Hessian-free optimization method for training the DNN with the cross-entropy objective and empirically analyzed the properties of the method. And finally, Dognin and Goel <ref type="bibr" target="#b100">[113]</ref> combined stochastic average gradient and Hessian-free optimization for sequence training of deep neural networks with success in that the training procedure converges in about half the time compared with the full Hessian-free sequence training.</p><p>For large DNN-HMM systems with either frame-level or sequencelevel optimization objectives, speeding up the training is essential to take advantage of large amounts of training data and of large model sizes. In addition to the methods described above, Dean et al. <ref type="bibr" target="#b68">[69]</ref> reported the use of the asynchronous stochastic gradient descent (ASGD) method, the adaptive gradient descent (Adagrad) method, and the large-scale limited-memory BFGS (L-BFGS) method for very large vocabulary speech recognition. Sainath et al. <ref type="bibr" target="#b299">[312]</ref> provided a review of a wide range of optimization methods for speeding up the training of DNN-based systems for large speech recognition tasks.</p><p>In addition to the advances described above focusing on optimization with the fully supervised learning paradigm, where all training data contain the label information, the semi-supervised training paradigm is also exploited for learning DNN-HMM systems for speech recognition. Liao et al. <ref type="bibr" target="#b210">[223]</ref> reported the exploration of using semisupervised training on the DNN-HMM system for the very challenging task of recognizing YouTube speech. The main technique is based on the use of "island of confidence" filtering heuristics to select useful training segments. Separately, semi-supervised training of DNNs is explored by Vesely et al. <ref type="bibr" target="#b361">[374]</ref>, where self-training strategies are used as the basis for data selection using both the utterance-level and frame-level confidences. Frame-selection based on per-frame confidences derived from confusion in a lattice is found beneficial. Huang et al. <ref type="bibr" target="#b163">[176]</ref> reported another variant of semi-supervised training technique in which multisystem combination and confidence recalibration is applied to select the training data. Further, Thomas et al. <ref type="bibr" target="#b349">[362]</ref> overcome the problem of lacking sufficient training data for acoustic modeling in a number of low-resource scenarios. They make use of transcribed multilingual data and semi-supervised training to build the proposed feature front-ends for subsequent speech recognition.</p><p>Finally, we see important progress in deep learning based speech recognition in recent years with the introduction of new regularization methods based on "dropout" originally proposed by Hinton et al. <ref type="bibr" target="#b153">[166]</ref>. Overfitting is very common in DNN training and co-adaptation is prevalent within the DNN with multiple activations adapting together to explain input acoustic data. Dropout is a technique to limit coadaptation. It operates as follows. On each training instance, each hidden unit is randomly omitted with a fixed probability (e.g., p = 0.5). Then, decoding is done normally except with straightforward scaling of the DNN weights (by a factor of 1 − p). Alternatively, the scaling of the DNN weights can be done during training [by a factor of 1/(1 − p)] rather than in decoding. The benefits of dropout regularization for training DNNs are to make a hidden unit in the DNN act strongly by itself without relying on others, and to serve a way to do model averaging of different networks. These benefits are most pronounced when the training data is limited, or when the DNN size is disproportionally large with respect to the size of the training data. Dahl et al. <ref type="bibr" target="#b64">[65]</ref> applied dropout in conjunction with the ReLU units and to only the top few layers of a fully-connected DNN. Seltzer and Yu <ref type="bibr" target="#b312">[325]</ref> applied it to noise robust speech recognition. <ref type="bibr">Deng et al. [81]</ref>, on the other hand, applied dropout to all layers of a deep convolutional neural network, including both the top fully connected DNN layers and the bottom locally connected CNN layer and the pooling layer. It is found that the dropout rate need to be substantially smaller for the convolutional layer.</p><p>Subsequent work on applying dropout includes the study by Miao and Metze <ref type="bibr" target="#b230">[243]</ref>, where DNN-based speech recognition is constrained by low resources with sparse training data. Most recently, Sainath et al. <ref type="bibr" target="#b293">[306]</ref> combined dropout with a number of novel techniques described in this section (including the use of deep CNNs, Hessian-free sequence learning, the use of ReLU units, and the use of joint fMLLR and filterbank features, etc.) to obtain state of the art results on several large vocabulary speech recognition tasks.</p><p>As a summary, the initial success of deep learning methods for speech analysis and recognition reported around 2010 has come a long way over the past three years. An explosive growth in the work and publications on this topic has been observed, and huge excitement has been ignited within the speech recognition community. We expect that the growth in the research on deep learning based speech recognition will continue, at least in the near future. It is also fair to say that the continuing large-scale success of deep learning in speech recognition as surveyed in this chapter (up to the ASRU-2013 time frame) is a key stimulant to the large-scale exploration and applications of the deep learning methods to other areas, which we will survey in Sections 8-11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Speech synthesis</head><p>In addition to speech recognition, the impact of deep learning has recently spread to speech synthesis, aimed to overcome the limitations of the conventional approach in statistical parametric synthesis based on Gaussian-HMM and decision-tree-based model clustering. The goal of speech synthesis is to generate speech sounds directly from text and possibly with additional information. The first set of papers appeared at ICASSP, May 2013, where four different deep learning approaches are reported to improve the traditional HMM-based statistical parametric speech synthesis systems built based on "shallow" speech models, which we briefly review here after providing appropriate background information.</p><p>Statistical parametric speech synthesis emerged in the mid-1990s, and is currently the dominant technology in speech synthesis. See a recent overview in <ref type="bibr" target="#b351">[364]</ref>. In this approach, the relationship between texts and their acoustic realizations are modeled using a set of stochastic generative acoustic models. Decision tree-clustered contextdependent HMMs with a Gaussian distribution as the output of an HMM state are the most popular generative acoustic model used. In such HMM-based speech synthesis systems, acoustic features including the spectra, excitation and segment durations of speech are modeled simultaneously within a unified context-dependent HMM framework. At the synthesis time, a text analysis module extracts a sequence of contextual factors including phonetic, prosodic, linguistic, and grammatical descriptions from an input text to be synthesized. Given the sequence of contextual factors, a sentence-level context-dependent HMM corresponding to the input text is composed, where its model parameters are determined by traversing the decision trees. The acoustic features are predicted so as to maximize their output probabilities from the sentence HMM under the constraints between static and dynamic features. Finally, the predicted acoustic features are sent to a waveform synthesis module to reconstruct the speech waveforms. It has been known for many years that the speech sounds generated by this standard approach are often muffled compared with natural speech. The inadequacy of acoustic modeling based on the shallowstructured HMM is conjectured to be one of the reasons. Several very recent studies have adopted deep learning approaches to overcome such deficiency. One significant advantage of deep learning techniques is their strong ability to represent the intrinsic correlation or mapping relationship among the units of a high-dimensional stochastic vector using a generative (e.g., the RBM and DBN discussed in Section 3.2) or discriminative (e.g., the DNN discussed in Section 3.3) modeling framework. The deep learning techniques are thus expected to help the acoustic modeling aspect of speech synthesis in overcoming the limitations of the conventional shallow modeling approach.</p><p>A series of studies are carried out recently on ways of overcoming the above limitations using deep learning methods, inspired partly by the intrinsically hierarchical processes in human speech production and the successful applications of a number of deep learning methods in speech recognition as reviewed earlier in this chapter. In Ling et al. <ref type="bibr" target="#b214">[227,</ref><ref type="bibr" target="#b216">229]</ref>, the RBM and DBN as generative models are used to replace the traditional Gaussian models, achieving significant quality improvement, in both subjective and objective measures, of the synthesized voice. In the approach developed in <ref type="bibr" target="#b177">[190]</ref>, the DBN as a generative model is used to represent joint distribution of linguistic and acoustic features. Both the decision trees and Gaussian models are replaced by the DBN. The method is very similar to that used for generating digit images by the DBN, where the issue of temporal sequence modeling specific to speech (non-issue for image) is by-passed via the use of the relatively large, syllable-sized units in speech synthesis. On the other hand, in contrast to the generative deep models (RBMs and DBNs) exploited above, the study reported in <ref type="bibr" target="#b422">[435]</ref> makes use of the discriminative model of the DNN to represent the conditional distribution of the acoustic features given the linguistic features. Finally, in <ref type="bibr" target="#b102">[115]</ref>, the discriminative model of the DNN is used as a feature extractor that summarizes high-level structure from the raw acoustic features. Such DNN features are then used as the input for the second stage for the prediction of prosodic contour targets from contextual features in the full speech synthesis system.</p><p>The application of deep learning to speech synthesis is in its infancy, and much more work is expected from that community in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Audio and music processing</head><p>Similar to speech recognition but to a less extent, in the area of audio and music processing, deep learning has also become of intense interest but only quite recently. As an example, the first major event of deep learning for speech recognition took place in 2009, followed by a series of events including a comprehensive tutorial on the topic at ICASSP-2012 and with the special issue at IEEE Transactions on Audio, Speech, and Language Processing, the premier publication for speech recognition, in the same year. The first major event of deep learning for audio and music processing appears to be the special session at ICASSP-2014, titled Deep Learning for Music <ref type="bibr" target="#b13">[14]</ref>.</p><p>In the general field of audio and music processing, the impacted areas by deep learning include mainly music signal processing and music information retrieval <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b128">141,</ref><ref type="bibr" target="#b164">177,</ref><ref type="bibr" target="#b165">178,</ref><ref type="bibr" target="#b166">179,</ref><ref type="bibr" target="#b306">319]</ref>. Deep learning presents a unique set of challenges in these areas. Music audio signals are time series where events are organized in musical time, rather than in real time, which changes as a function of rhythm and expression. The measured signals typically combine multiple voices that are synchronized in time and overlapping in frequency, mixing both short-term and long-term temporal dependencies. The influencing factors include musical tradition, style, composer and interpretation. The high complexity and variety give rise to the signal representation problems well-suited to the high levels of abstraction afforded by the perceptually and biologically motivated processing techniques of deep learning.</p><p>In the early work on audio signals as reported by Lee et al. <ref type="bibr" target="#b202">[215]</ref> and their follow-up work, the convolutional structure is imposed on the RBM while building up a DBN. Convolution is made in time by sharing weights between hidden units in an attempt to detect the same "invariant" feature over different times. Then a max-pooling operation is performed where the maximal activations over small temporal neighborhoods of hidden units are obtained, inducing some local temporal invariance. The resulting convolutional DBN is applied to audio as well as speech data for a number of tasks including music artist and genre classification, speaker identification, speaker gender classification, and phone classification, with promising results presented.</p><p>The RNN has also been recently applied to music processing applications <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>, where the use of ReLU hidden units instead of logistic or tanh nonlinearities are explored in the RNN. As reviewed in Section 7.2, ReLU units compute y = max(x, 0), and lead to sparser gradients, less diffusion of credit and blame in the RNN, and faster training. The RNN is applied to the task of automatic recognition of chords from audio music, an active area of research in music information retrieval. The motivation of using the RNN architecture is its power in modeling dynamical systems. The RNN incorporates an internal memory, or hidden state, represented by a self-connected hidden layer of neurons. This property makes them well suited to model temporal sequences, such as frames in a magnitude spectrogram or chord labels in a harmonic progression. When well trained, the RNN is endowed with the power to predict the output at the next time step given the previous ones. Experimental results show that the RNN-based automatic chord recognition system is competitive with existing state-ofthe-art approaches <ref type="bibr" target="#b262">[275]</ref>. The RNN is capable of learning basic musical properties such as temporal continuity, harmony and temporal dynamics. It can also efficiently search for the most musically plausible chord sequences when the audio signal is ambiguous, noisy or weakly discriminative.</p><p>A recent review article by Humphrey et al. <ref type="bibr" target="#b166">[179]</ref> provides a detailed analysis on content-based music informatics, and in particular on why the progress is decelerating throughout the field. The analysis concludes that hand-crafted feature design is sub-optimal and unsustainable, that the power of shallow architectures is fundamentally limited, and that short-time analysis cannot encode musically meaningful structure. These conclusions motivate the use of deep learning methods aimed at automatic feature learning. By embracing feature learning, it becomes possible to optimize a music retrieval system's internal feature representation or discovering it directly, since deep architectures are especially well-suited to characterize the hierarchical nature of music. Finally, we review the very recent work by van den Oord, et al. <ref type="bibr" target="#b358">[371]</ref> on content-based music recommendation using deep learning methods. Automatic music recommendation has become an increasingly significant and useful technique in practice. Most recommender systems rely on collaborative filtering, suffering from the cold start problem where it fails when no usage data is available. Thus, collaborative filtering is not effective for recommending new and unpopular songs. Deep learning methods power the latent factor model for recommendation, which predicts the latent factors from music audio when they cannot be obtained from usage data. A traditional approach using a bag-of-words representation of the audio signals is compared with deep CNNs with rigorous evaluation made. The results show highly sensible recommendations produced by the predicted latent factors using deep CNNs. The study demonstrates that a combination of convolutional neural networks and richer audio features lead to such promising results for content-based music recommendation.</p><p>Like speech recognition and speech synthesis, much more work is expected from the music and audio signal processing community in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selected Applications in Language Modeling and Natural Language Processing</head><p>Research in language, document, and text processing has seen increasing popularity recently in the signal processing community, and has been designated as one of the main focus areas by the IEEE Signal Processing Society's Speech and Language Processing Technical Committee. Applications of deep learning to this area started with language modeling (LM), where the goal is to provide a probability to any arbitrary sequence of words or other linguistic symbols (e.g., letters, characters, phones, etc.). Natural language processing (NLP) or computational linguistics also deals with sequences of words or other linguistic symbols, but the tasks are much more diverse (e.g., translation, parsing, text classification, etc.), not focusing on providing probabilities for linguistic symbols. The connection is that LM is often an important and very useful component of NLP systems. Applications to NLP is currently one of the most active areas in deep learning research, and deep learning is also considered as one promising direction by the NLP research community. However, the intersection between the deep learning and NLP researchers is so far not nearly as large as that for the application areas of speech or vision. This is partly because the hard evidence for the superiority of deep learning over the current state of the art NLP methods has not been as strong as speech or visual object recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Language modeling</head><p>Language models (LMs) are crucial part of many successful applications, such as speech recognition, text information retrieval, statistical machine translation and other tasks of NLP. Traditional techniques for estimating the parameters in LMs are based on N-gram counts. Despite known weaknesses of N -grams and huge efforts of research communities across many fields, N -grams remained the state-of-the-art until neural network and deep learning based methods were shown to significantly lower the perplexity of LMs, one common (but not ultimate) measure of the LM quality, over several standard benchmark tasks <ref type="bibr" target="#b232">[245,</ref><ref type="bibr" target="#b234">247,</ref><ref type="bibr" target="#b235">248]</ref>.</p><p>Before we discuss neural network based LMs, we note the use of hierarchical Bayesian priors in building up deep and recursive structure for LMs <ref type="bibr" target="#b161">[174]</ref>. Specifically, Pitman-Yor process is exploited as the Bayesian prior, from which a deep (four layers) probabilistic generative model is built. It offers a principled approach to LM smoothing by incorporating the power-law distribution for natural language. As discussed in Section 3, this type of prior knowledge embedding is more readily achievable in the generative probabilistic modeling setup than in the discriminative neural network based setup. The reported results on LM perplexity reduction are not nearly as strong as that achieved by the neural network based LMs, which we discuss next.</p><p>There has been a long history <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b420">433]</ref> of using (shallow) feed-forward neural networks in LMs, called the NNLM. The use of DNNs in the same way for LMs appeared more recently in <ref type="bibr" target="#b7">[8]</ref>. An LM is a function that captures the salient statistical characteristics of the distribution of sequences of words in natural language. It allows one to make probabilistic predictions of the next word given preceding ones. An NNLM is one that exploits the neural network's ability to learn distributed representations in order to reduce the impact of the curse of dimensionality. The original NNLM, with a feed-forward neural network structure works as follows: the input of the N-gram NNLM is formed by using a fixed length history of N − 1 words. Each of the previous N − 1 words is encoded using the very sparse 1-of-V coding, where V is the size of the vocabulary. Then, this 1-of-V orthogonal representation of words is projected linearly to a lower dimensional space, using the projection matrix shared among words at different positions in the history. This type of continuous-space, distributed representation of words is called "word embedding," very different from the common symbolic or localist presentation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. After the projection layer, a hidden layer with nonlinear activation function, which is either a hyperbolic tangent or a logistic sigmoid, is used. An output layer of the neural network then follows the hidden layer, with the number of output units equal to the size of the full vocabulary. After the network is trained, the output layer activations represent the "N -gram" LM's probability distribution.</p><p>The main advantage of NNLMs over the traditional counting-based N -gram LMs is that history is no longer seen as exact sequence of N −1 words, but rather as a projection of the entire history into some lower dimensional space. This leads to a reduction of the total number of parameters in the model that have to be trained, resulting in automatic clustering of similar histories. Compared with the class-based N -gram LMs, the NNLMs are different in that they project all words into the same low dimensional space, in which there can be many degrees of similarity between words. On the other hand, NNLMs have much larger computational complexity than N -gram LMs.</p><p>Let's look at the strengths of the NNLMs again from the viewpoint of distributed representations. A distributed representation of a symbol is a vector of features which characterize the meaning of the symbol. Each element in the vector participates in representing the meaning. With an NNLM, one relies on the learning algorithm to discover meaningful, continuous-valued features. The basic idea is to learn to associate each word in the dictionary with a continuous-valued vector representation, which in the literature is called a word embedding, where each word corresponds to a point in a feature space. One can imagine that each dimension of that space corresponds to a semantic or grammatical characteristic of words. The hope is that functionally similar words get to be closer to each other in that space, at least along some directions. A sequence of words can thus be transformed into a sequence of these learned feature vectors. The neural network learns to map that sequence of feature vectors to the probability distribution over the next word in the sequence. The distributed representation approach to LMs has the advantage that it allows the model to generalize well to sequences that are not in the set of training word sequences, but that are similar in terms of their features, i.e., their distributed representation. Because neural networks tend to map nearby inputs to nearby outputs, the predictions corresponding to word sequences with similar features are mapped to similar predictions.</p><p>The above ideas of NNLMs have been implemented in various studies, some involving deep architectures. The idea of structuring hierarchically the output of an NNLM in order to handle large vocabularies was introduced in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b249">262]</ref>. In <ref type="bibr" target="#b239">[252]</ref>, the temporally factored RBM was used for language modeling. Unlike the traditional N -gram model, the factored RBM uses distributed representations not only for context words but also for the words being predicted. This approach is generalized to deeper structures as reported in <ref type="bibr" target="#b240">[253]</ref>. Subsequent work on NNLM with "deep" architectures can be found in <ref type="bibr" target="#b192">[205,</ref><ref type="bibr" target="#b194">207,</ref><ref type="bibr" target="#b195">208,</ref><ref type="bibr" target="#b232">245,</ref><ref type="bibr" target="#b234">247,</ref><ref type="bibr" target="#b235">248]</ref>. As an example, Le et al. <ref type="bibr" target="#b194">[207]</ref> describes an NNLM with structured output layer (SOUL-NNLM) where the processing depth in the LM is focused in the neural network's output representation. Figure <ref type="figure" target="#fig_31">8</ref>.1 illustrates the SOUL-NNLM architecture with hierarchical structure in the output layers of the neural network, which shares the same architecture with the conventional NNLM up to the hidden layer. The hierarchical structure for the network's output vocabulary is in the form of a clustering tree, shown to the right of Figure <ref type="figure" target="#fig_31">8</ref>.1, where each word belongs to only one class and ends in a single leaf node of the tree. As a result of the hierarchical structure, the SOUL-NNLM enables the training of the NNLM with a full, very large vocabulary. This gives advantages over the traditional NNLM which requires shortlists of words in order to carry out the efficient computation in training.</p><p>As another example neural-network-based LMs, the work described in <ref type="bibr" target="#b234">[247,</ref><ref type="bibr" target="#b235">248]</ref> and <ref type="bibr" target="#b232">[245]</ref> makes use of RNNs to build large scale language models, called RNNLMs. The main difference between the feed-forward and the recurrent architecture for LMs is different ways of representing the word history. For feed-forward NNLM, the history is still just previous several words. But for the RNNLM, an effective representation of history is learned from the data during training. The hidden layer of RNN represents all previous history and not just N − 1 previous words, thus the model can theoretically represent long context patterns. A further important advantage of the RNNLM over the feed-forward counterpart is the possibility to represent more advanced patterns in the word sequence. For example, patterns that rely on words that could have occurred at variable positions in the history can be encoded much more efficiently with the recurrent architecture. That is, the RNNLM can simply remember some specific word in the state of the hidden layer, while the feed-forward NNLM would need to use parameters for each specific position of the word in the history.</p><p>The RNNLM is trained using the algorithm of back-propagation through time; see details in <ref type="bibr" target="#b232">[245]</ref>, which provided   <ref type="bibr" target="#b234">[247,</ref><ref type="bibr" target="#b235">248]</ref> and <ref type="bibr" target="#b232">[245]</ref>.</p><p>A separate work on applying RNN to an LM with the unit of characters instead of words can be found in <ref type="bibr" target="#b140">[153,</ref><ref type="bibr" target="#b344">357]</ref>. Many interesting properties such as predicting long-term dependencies (e.g., making open and closing quotes in a paragraph) are demonstrated. However, the usefulness of characters instead of words as units in practical applications is not clear because the word is such a powerful representation for natural language. Changing words to characters in LMs may limit most practical application scenarios and the training become more difficult. Word-level models currently remain superior.</p><p>In the most recent work, Mnih and Teh <ref type="bibr" target="#b242">[255]</ref> and Mnih and Kavukcuoglu <ref type="bibr" target="#b241">[254]</ref> have developed a fast and simple training algorithm for NNLMs. Despite their superior performance, NNLMs have been used less widely than standard N -gram LMs due to the much longer training time. The reported algorithm makes use of a method called noise-contrastive estimation or NCE <ref type="bibr" target="#b126">[139]</ref> to achieve much faster training for NNLMs, with time complexity independent of the vocabulary size; hence a flat instead of tree-structured output layer in the NNLM is used. The idea behind NCE is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise. That is, to estimate parameters in a density model of observed data, we can learn to discriminate between samples from the data distribution and samples from a known noise distribution. As an important special case, NCE is particularly attractive for unnormalized distributions (i.e., free from partition functions in the denominator). In order to apply NCE to train NNLMs efficiently, Mnih and Teh <ref type="bibr" target="#b242">[255]</ref> and Mnih and Kavukcuoglu <ref type="bibr" target="#b241">[254]</ref> first formulate the learning problem as one which takes the objective function as the distribution of the word in terms of a scoring function. The NNLM then can be viewed as a way to quantify the compatibility between the word history and a candidate next word using the scoring function. The objective function for training the NNLM thus becomes exponentiation of the scoring function, normalized by the same constant over all possible words. Removing the costly normalization factor, NCE is shown to speed up the NNLM training over an order of magnitude.</p><p>A similar concept to NCE is used in the recent work of <ref type="bibr" target="#b237">[250]</ref>, which is called negative sampling. This is applied to a simplified version of an NNLM, for the purpose of constructing word embedding instead of computing probabilities of word sequences. Word embedding is an important concept for NLP applications, which we discuss next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Natural language processing</head><p>Machine learning has been a dominant tool in NLP for many years. However, the use of machine learning in NLP has been mostly limited to numerical optimization of weights for human designed representations and features from the text data. The goal of deep or representation learning is to automatically develop features or representations from the raw text material appropriate for a wide range of NLP tasks.</p><p>Recently, neural network based deep learning methods have been shown to perform well on various NLP tasks such as language modeling, machine translation, part-of-speech tagging, named entity recognition, sentiment analysis, and paraphrase detection. The most attractive aspect of deep learning methods is their ability to perform these tasks without external hand-designed resources or time-intensive feature engineering. To this end, deep learning develops and makes use an important concept called "embedding," which refers to the representation of symbolic information in natural language text at word-level, phrase-level, and even sentence-level in terms of continuous-valued vectors.</p><p>The early work highlighting the importance of word embedding came from <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b354">[367]</ref>, and <ref type="bibr" target="#b62">[63]</ref>, although the original form came from <ref type="bibr" target="#b25">[26]</ref> as a side product of language modeling. Raw symbolic word representations are transformed from the sparse vectors via 1-of-V coding with a very high dimension (i.e., the vocabulary size V or its square or even its cubic) into low-dimensional, real-valued vectors via a neural network and then used for processing by subsequent neural network layers. The key advantage of using the continuous space to represent words (or phrases) is its distributed nature, which enables sharing or grouping the representations of words with a similar meaning. Such sharing is not possible in the original symbolic space, constructed by 1-of-V coding with a very high dimension, for representing words. Unsupervised learning is used where "context" of the word is used as the learning signal in neural networks. Excellent tutorials were recently given by Socher et al. <ref type="bibr" target="#b325">[338,</ref><ref type="bibr" target="#b327">340]</ref> to explain how the neural network is trained to perform word embedding. More recent work proposes new ways of learning word embeddings that better capture the semantics of words by incorporating both local and global document contexts and better account for homonymy and polysemy by learning multiple embeddings per word <ref type="bibr" target="#b156">[169]</ref>. Also, there is strong evidence that the use of RNNs can also provide empirically good performance in learning word embeddings <ref type="bibr" target="#b232">[245]</ref>. While the use of NNLMs, whose aim is to predict the future words in context, also induces word embeddings as its by-product, much simpler ways of achieving the embeddings are possible without the need to do word prediction. As shown by Collobert and Weston <ref type="bibr" target="#b61">[62]</ref>, the neural networks used for creating word embeddings need much smaller output units than the huge size typically required for NNLMs.</p><p>In the same early paper on word embedding, Collobert and Weston <ref type="bibr" target="#b61">[62]</ref> developed and employed a convolutional network as the common model to simultaneously solve a number of classic problems including part-of-speech tagging, chunking, named entity tagging, semantic role identification, and similar word identification. More recent work reported in <ref type="bibr" target="#b60">[61]</ref> further developed a fast, purely discriminative approach for parsing based on the deep recurrent convolutional architecture. Collobert et al. <ref type="bibr" target="#b62">[63]</ref> provide a comprehensive review on ways of applying unified neural network architectures and related deep learning algorithms to solve NLP problems from "scratch," meaning that no traditional NLP methods are used to extract features. The theme of this line of work is to avoid task-specific, "man-made" feature engineering while providing versatility and unified features constructed automatically from deep learning applicable to all natural language processing tasks. The systems described in <ref type="bibr" target="#b62">[63]</ref> automatically learn internal representations or word embedding from vast amounts of mostly unlabeled training data while performing a wide range of NLP tasks.</p><p>The recent work by Mikolov et al. <ref type="bibr" target="#b233">[246]</ref> derives word embeddings by simplifying the NNLM described in Section 8.1. It is found that the NNLM can be successfully trained in two steps. First, continuous word vectors are learned using a simple model which eliminates the nonlinearity in the upper neural network layer and share the projection layer for all words. And second, the N -gram NNLM is trained on top of the word vectors. So, after removing the second step in the NNLM, the simple model is used to learn word embeddings, where the simplicity allows the use of very large amount of data. This gives rise to a word embedding model called Continuous Bag-of-Words Model (CBOW), as shown in Figure <ref type="figure" target="#fig_31">8</ref>.3a. Further, since the goal is no longer computing probabilities of word sequences as in LMs, the word embedding system here is made more effective by not only to predict the current word based on the context but also to perform inverse prediction known as "Skip-gram" model, as shown in Figure <ref type="figure" target="#fig_31">8</ref>.3b. In the follow-up work <ref type="bibr" target="#b237">[250]</ref> by the same authors, this word embedding system including the Skip-gram model is extended by a much faster learning method called negative sampling, similar to NCE discussed in Section 8.1.</p><p>In parallel with the above development, Mnih and Kavukcuoglu <ref type="bibr" target="#b241">[254]</ref> demonstrate that NCE training of lightweight word embedding models is a highly efficient way of learning high-quality word representations, much like the somewhat earlier lightweight LMs developed by Mnih and Teh <ref type="bibr" target="#b242">[255]</ref> described in Section 8.1. Consequently, results that used to require very considerable hardware and software infrastructure can now be obtained on a single desktop with minimal programming effort and using less time and data. This most recent work also shows that for representation learning, only five noise samples in NCE can be sufficient for obtaining strong results for word embedding, much fewer than that required for LMs. The authors also used an "inversed language model" for computing word embeddings, similar to the way in which the Skip-gram model is used in <ref type="bibr" target="#b237">[250]</ref>.</p><p>Huang et al. <ref type="bibr" target="#b156">[169]</ref> recognized the limitation of the earlier work on word embeddings in that these models were built with only local context and one representation per word. They extended the local context models to one that can incorporate global context from full sentences or the entire document. This extended models accounts for homonymy and polysemy by learning multiple embeddings for each word. An illustration of this model is shown in Figure <ref type="figure" target="#fig_31">8</ref>.4. In the earlier work by the same research group <ref type="bibr" target="#b331">[344]</ref>, a recursive neural network with local context was developed to build a deep architecture. The network, despite missing global context, was already shown to be capable of successful merging of natural language words based on the learned semantic transformations of their original features. This deep learning approach provided an excellent performance on natural language parsing. The same approach was also demonstrated to be reasonably successful in parsing natural scene images. In related studies, a similar recursive deep architecture is used for paraphrase detection <ref type="bibr" target="#b333">[346]</ref>, and for predicting sentiment distributions from text <ref type="bibr" target="#b332">[345]</ref>.</p><p>We now turn to selected applications of deep learning methods including the use of neural network architectures and word embeddings to practically useful NLP tasks. Machine translation is one of such tasks, pursued by NLP researchers for many years based typically on shallow statistical models. The work described in <ref type="bibr" target="#b307">[320]</ref> are perhaps the first comprehensive report on the successful application of neuralnetwork-based language models with word embeddings, trained on a GPU, for large machine translation tasks. They address the problem of high computation complexity, and provide a solution that allows training 500 million words with 20 hours. Strong results are reported, with perplexity down from 71 to 60 in LMs and the corresponding BLEU score gained by 1.8 points using the neural-network-based language models with word embeddings compared with the best back-off LM.</p><p>A more recent study on applying deep learning methods to machine translation appears in <ref type="bibr" target="#b108">[121,</ref><ref type="bibr" target="#b110">123]</ref>, where the phrase-translation component, rather than the LM component in the machine translation system is replaced by the neural network models with semantic word embeddings. As shown in Figure <ref type="figure" target="#fig_31">8</ref>.5 for the architecture of this approach, a pair of source (denoted by f ) and target (denoted by e) phrases are projected into continuous-valued vector representations in a lowdimensional latent semantic space (denoted by the two y vectors).Then their translation score is computed by the distance between the pair in this new space. The projection is performed by two deep neural networks (not shown here) whose weights are learned on parallel training data. The learning is aimed to directly optimize the quality of endto-end machine translation results. Experimental evaluation has been performed on two standard Europarl translation tasks used by the NLP community, English-French and German-English. The results show that the new semantic-based phrase translation model significantly improves the performance of a state-of-the-art phrase-based statistical machine translation system, leading to a gain close to 1.0 BLEU point.</p><p>A related approach to machine translation was developed by Schwenk <ref type="bibr" target="#b307">[320]</ref>. The estimation of the translation model probabilities of a phrase-based machine translation system is carried out using neural networks. The translation probability of phrase pairs is learned using continuous-space representations induced by neural networks. A simplification is made that decomposes the translation probability of a phrase or a sentence to a product of n-gram probabilities as in a standard n-gram language model. No joint representations of a phrase in the source language and the translated version in the target language are exploited as in the approach reported by Gao et al. <ref type="bibr" target="#b109">[122,</ref><ref type="bibr" target="#b110">123]</ref>.</p><p>Yet another deep learning approach to machine translation appeared in <ref type="bibr" target="#b236">[249]</ref>. As in other approaches, a corpus of words in one language are compared with the same corpus of words translated into another, and words and phrases in such bilingual data that share similar statistical properties are considered equivalent. A new technique is proposed that automatically generates dictionaries and phrase tables that convert one language into another. It does not rely on versions of the same document in different languages. Instead, it uses data mining techniques to model the structure of a source language and then compares it to the structure of the target language. The technique is shown to translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It is based on vector-valued word embeddings as discussed earlier in this chapter and it learns a linear mapping between vector spaces of source and target languages.</p><p>An earlier study on applying deep learning techniques with DBNs was provided in <ref type="bibr" target="#b98">[111]</ref> to attack a machine transliteration problem, a much easier task than machine translation. This type of deep architectures and learning may be generalized to the more difficult machine translation problem but no follow-up work has been reported. As another early NLP application, Sarikaya et al. <ref type="bibr" target="#b305">[318]</ref> applied DNNs (called DBNs in the paper) to perform a natural language call-routing task. The DNNs use unsupervised learning to discover multiple layers of features that are then used to optimize discrimination. Unsupervised feature discovery is found to make DBNs far less prone to overfitting than the neural networks initialized with random weights. Unsupervised learning also makes it easier to train neural networks with many hidden layers. DBNs are found to produce better classification results than several other widely used learning techniques, e.g., maximum entropy and boosting based classifiers.</p><p>One most interesting NLP task recently tackled by deep learning methods is that of knowledge base (ontology) completion, which is instrumental in question-answering and many other NLP applications. An early work in this space came from <ref type="bibr" target="#b36">[37]</ref>, where a process is introduced to automatically learn structured distributed embeddings of knowledge bases. The proposed representations in the continuousvalued vector space are compact and can be efficiently learned from large-scale data of entities and relations. A specialized neural network architecture, a generalization of "Siamese" network, is used. In the follow-up work that focuses on multi-relational data <ref type="bibr" target="#b35">[36]</ref>, the semantic matching energy model is proposed to learn vector representations for both entities and relations. More recent work <ref type="bibr" target="#b327">[340]</ref> adopts an alternative approach, based on the use of neural tensor networks, to attack the problem of reasoning over a large joint knowledge graph for relation classification. The knowledge graph is represented as triples of a relation between two entities, and the authors aim to develop a neural network model suitable for inference over such relationships. The model they presented is a neural tensor network, with one layer only. The network is used to represent entities in a fixed-dimensional vectors, which are created separately by averaging pre-trained word embedding vectors. It then learn the tensor with the newly added relationship element that describes the interactions among all the latent components in each of the relationships. The neural tensor network can be visualized in Figure <ref type="figure" target="#fig_31">8</ref>.6, where each dashed box denotes one of the two slices of the tensor. Experimentally, the paper <ref type="bibr" target="#b327">[340]</ref> shows that this tensor model can effectively classify unseen relationships in WordNet and FreeBase.</p><p>As the final example of deep learning applied successfully to NLP, we discuss here sentiment analysis applications based on recursive deep  <ref type="bibr" target="#b327">[340]</ref>, with two relationships shown as two slices in the tensor. The tensor is denoted by W [1:2] . The network contains a bilinear tensor layer that directly relates the two entity vectors (shown as e1 and e2) across three dimensions. Each dashed box denotes one of the two slices of the tensor. [after <ref type="bibr" target="#b327">[340]</ref>, @NIPS]. models published recently by Socher et al. <ref type="bibr" target="#b334">[347]</ref>. Sentiment analysis is a task that is aimed to estimate the positive or negative opinion by an algorithm based on input text information. As we discussed earlier in this chapter, word embeddings in the semantic space achieved by neural network models have been very useful but it is difficult for them to express the meaning of longer phrases in a principled way. For sentiment analysis with the input data from typically many words and phrases, the embedding model requires the compositionality properties. To this end, Socher et al. <ref type="bibr" target="#b334">[347]</ref> developed the recursive neural tensor network, where each layer is constructed similarly to that of the neural tensor network described in <ref type="bibr" target="#b327">[340]</ref> with an illustration shown in Figure <ref type="figure" target="#fig_31">8</ref>.6. The recursive construction of the full network exhibiting properties of compositionality follows that of <ref type="bibr" target="#b331">[344]</ref> for the regular, non-tensor network. When trained on a carefully constructed sentiment analysis database, the recursive neural tensor network is shown to outperform all previous methods on several metrics. The new model pushes the state of the art in single sentence positive/negative classification accuracy from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bagof-features baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selected Applications in Information Retrieval</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">A brief introduction to information retrieval</head><p>Information retrieval (IR) is a process whereby a user enters a query into the automated computer system that contains a collection of many documents with the goal of obtaining a set of most relevant documents. Queries are formal statements of information needs, such as search strings in web search engines. In IR, a query does not uniquely identify a single document in the collection. Instead, several documents may match the query with different degrees of relevancy.</p><p>A document, sometimes called an object as a more general term which may include not only a text document but also an image, audio (music or speech), or video, is an entity that contains information and represented as an entry in a database. In this section, we limit the "object" to only text documents. User queries in IR are matched against the documents' representation stored in the database. Documents themselves often are not kept or stored directly in the IR system. Rather, they are represented in the system by metadata. Typical IR systems compute a numeric score on how well each document in the database matches the query, and rank the objects according to this value. The top-ranking documents from the system are then shown to the user. The process may then be iterated if the user wishes to refine the query.</p><p>Based partly on <ref type="bibr" target="#b223">[236]</ref>, common IR methods consist of several categories:</p><p>• Boolean retrieval, where a document either matches a query or does not. • Algebraic approaches to retrieval, where models are used to represent documents and queries as vectors, matrices, or tuples. The similarity of the query vector and document vector is represented as a scalar value. This value can be used to produce a list of documents that are rank-ordered for a query. Common models and methods include vector space model, topic-based vector space model, extended Boolean model, and latent semantic analysis. • Probabilistic approaches to retrieval, where the process of IR is treated as a probabilistic inference. Similarities are computed as probabilities that a document is relevant for a given query, and the probability value is then used as the score in ranking documents. Common models and methods include binary independence model, probabilistic relevance model with the BM25 relevance function, methods of inference with uncertainty, probabilistic, language modeling, http://en.wikipedia.org/wiki/ Uncertain_inference and the technique of latent Dirichlet allocation. • Feature-based approaches to retrieval, where documents are viewed as vectors of values of feature functions. Principled methods of "learning to rank" are devised to combine these features into a single relevance score. Feature functions are arbitrary functions of document and query, and as such Feature-based approaches can easily incorporate almost any other retrieval model as just yet another feature.</p><p>Deep learning applications to IR are rather recent. The approaches in the literature so far belong mostly to the category of feature-based approaches. The use of deep networks is mainly for extracting semantically meaningful features for subsequent document ranking stages. We will review selected studies in the recent literature in the remainder of this section below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Semantic hashing with deep autoencoders for document indexing and retrieval</head><p>Here we discuss the "semantic hashing" approach for the application of deep autoencoders to document indexing and retrieval as published in <ref type="bibr" target="#b146">[159,</ref><ref type="bibr" target="#b301">314]</ref>. It is shown that the hidden variables in the final layer of a DBN not only are easy to infer after using an approximation based on feed-forward propagation, but they also give a better representation of each document, based on the word-count features, than the widely used latent semantic analysis and the traditional TF-IDF approach for information retrieval. Using the compact code produced by deep autoencoders, documents are mapped to memory addresses in such a way that semantically similar text documents are located at nearby addresses to facilitate rapid document retrieval. The mapping from a word-count vector to its compact code is highly efficient, requiring only a matrix multiplication and a subsequent sigmoid function evaluation for each hidden layer in the encoder part of the network.</p><p>A deep generative model of DBN is exploited for the above purpose as discussed in <ref type="bibr" target="#b152">[165]</ref>. Briefly, the lowest layer of the DBN represents the word-count vector of a document and the top layer represents a learned binary code for that document. The top two layers of the DBN form an undirected associative memory and the remaining layers form a Bayesian (also called belief) network with directed, top-down connections. This DBN, composed of a set of stacked RBMs as we reviewed in Section 5, produces a feed-forward "encoder" network that converts word-count vectors to compact codes. By composing the RBMs in the opposite order, a "decoder" network is constructed that maps compact code vectors into reconstructed word-count vectors. Combining the encoder and decoder, one obtains a deep autoencoder (subject to further fine-tuning as discussed in Section 4) for document coding and subsequent retrieval.</p><p>After the deep model is trained, the retrieval process starts with mapping each query into a 128-bit binary code by performing a forward pass through the model with thresholding. Then the Hamming distance between the query binary code and all the documents' 128-bit binary codes, especially those of the "neighboring" documents defined in the semantic space, are computed extremely efficiently. The efficiency is accomplished by looking up the neighboring bit vectors in the hash table. The same idea as discussed here for coding text documents for information retrieval has been explored for audio document retrieval and speech feature coding problems with some initial exploration reported in <ref type="bibr" target="#b87">[100]</ref>, discussed in Section 4 in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Deep-structured semantic modeling (DSSM) for document retrieval</head><p>Here we discuss the more advanced and recent approach to large-scale document retrieval (Web search) based on a specialized deep architecture, called deep-structured semantic model or deep semantic similarity model (DSSM), as published in <ref type="bibr" target="#b159">[172]</ref>, and its convolutional version (C-DSSM), as published in <ref type="bibr" target="#b315">[328]</ref>. Modern search engines retrieve Web documents mainly by matching keywords in documents with those in a search query. However, lexical matching can be inaccurate due to the fact that a concept is often expressed using different vocabularies and language styles in documents and queries. Latent semantic models are able to map a query to its relevant documents at the semantic level where lexical-matching often fails <ref type="bibr" target="#b223">[236]</ref>. These models address the language discrepancy between Web documents and search queries by grouping different terms that occur in a similar context into the same semantic cluster. Thus, a query and a document, represented as two vectors in the lower-dimensional semantic space, can still have a high similarity even if they do not share any term. Probabilistic topic models such as probabilistic latent semantic models and latent Dirichlet allocation models have been proposed for semantic matching to partially overcome such difficulties. However, the improvement on IR tasks has not been as significant as originally expected because of two main factors: (1) most state-of-the-art latent semantic models are based on linear projection, and thus are inadequate in capturing effectively the complex semantic properties of documents; and (2) these models are often trained in an unsupervised manner using an objective function that is only loosely coupled with the evaluation metric for the retrieval task. In order to improve semantic matching for IR, two lines of research have been conducted to extend the above latent semantic models. The first is the semantic hashing approach reviewed in Section 9.1 above in this section based on the use of deep autoencoders <ref type="bibr" target="#b152">[165,</ref><ref type="bibr" target="#b301">314]</ref>. While the hierarchical semantic structure embedded in the query and the document can be extracted via deep learning, the deep learning approach used for their models still adopts an unsupervised learning method where the model parameters are optimized for the re-construction of the documents rather than for differentiating the relevant documents from the irrelevant ones for a given query. As a result, the deep neural network models do not significantly outperform strong baseline IR models that are based on lexical matching. In the second line of research, click-through data, which consists of a list of queries and the corresponding clicked documents, is exploited for semantic modeling so as to bridge the language discrepancy between search queries and Web documents in recent studies <ref type="bibr" target="#b107">[120,</ref><ref type="bibr" target="#b111">124]</ref>. These models are trained on click-through data using objectives that tailor to the document ranking task. However, these click-through-based models are still linear, suffering from the issue of expressiveness. As a result, these models need to be combined with the keyword matching models (such as BM25) in order to obtain a significantly better performance than baselines.</p><p>The DSSM approach reported in <ref type="bibr" target="#b159">[172]</ref> aims to combine the strengths of the above two lines of work while overcoming their weaknesses. It uses the DNN architecture to capture complex semantic properties of the query and the document, and to rank a set of documents for a given query. Briefly, a nonlinear projection is performed first to map the query and the documents to a common semantic space. Then, the relevance of each document given the query is calculated as the cosine similarity between their vectors in that semantic space. The DNNs are trained using the click-through data such that the conditional likelihood of the clicked document given the query is maximized. Different from the previous latent semantic models that are learned in an unsupervised fashion, the DSSM is optimized directly for Web document ranking, and thus gives superior performance. Furthermore, to deal with large vocabularies in Web search applications, a new word hashing method is developed, through which the high-dimensional term vectors of queries or documents are projected to low-dimensional letter based n-gram vectors with little information loss.</p><p>Figure <ref type="figure">9</ref>.1 illustrates the DNN part in the DSSM architecture. The DNN is used to map high-dimensional sparse text features into lowdimensional dense features in a semantic space. The first hidden layer, with 30k units, accomplishes word hashing. The word-hashed features are then projected through multiple layers of non-linear projections. The final layer's neural activities in this DNN form the feature in the semantic space.</p><p>To show the computational steps in the various layers of the DNN in Figure <ref type="figure">9</ref>.1, we denote x as the input term vector, y as the output vector, l i , i = 1, . . . , N − 1, as the intermediate hidden layers, W i as the ith projection matrix, and b i as the ith bias vector, we have</p><formula xml:id="formula_21">l 1 = W 1 x, l i = f (W i l i−1 + b i ), i &gt; 1 y = f (W N l N −1 + b N ),</formula><p>where tanh function is used at the output layer and the hidden layers l i , i = 2, . . . , N − 1:</p><formula xml:id="formula_22">f (x) = 1 − e −2x 1 + e −2x .</formula><p>The semantic relevance score between a query Q and a document D can then be computed as the consine distance</p><formula xml:id="formula_23">R(Q, D) = cosine(y Q , y D ) = y T Q y D y Q y D ,</formula><p>where y Q and y D are the concept vectors of the query and the document, respectively. In Web search, given the query, the documents can be sorted by their semantic relevance scores.</p><p>Learning of the DNN weights W i and b i shown in Figure <ref type="figure">9</ref>.1 is an important contribution of the study of <ref type="bibr" target="#b159">[172]</ref>. Compared with the DNNs used in speech recognition where the targets or labels of the training data are readily available, the DNN in the DSSM does not have such label information well defined. That is, rather than using the common cross entropy or mean square errors as the training objective function, IR-centric loss functions need to be developed in order to train the DNN weights in the DSSM using the available data such as click-through logs.</p><p>The click-through logs consist of a list of queries and their clicked documents. A query is typically more relevant to the documents that are clicked on than those that are not. This weak supervision information can be exploited to train the DSSM. More specifically, the weight matrices in the DSSM, W i , is learned to maximize the posterior probability of the clicked documents given the queries</p><formula xml:id="formula_24">P (D | Q) = exp(γR(Q, D)) D ∈D exp(γR(Q, D ))</formula><p>defined on the semantic relevance score R(Q, D) between the Query (Q) and the Document (D), where γ is a smoothing factor set empirically on a held-out data set, and D denotes the set of candidate documents to be ranked. Ideally, D should contain all possible documents, as in the maximum mutual information training for speech recognition where all possible negative candidates may be considered <ref type="bibr" target="#b134">[147]</ref>. However in this case D is of Web scale and thus is intractable in practice. In the implementation of DSSM learning described in <ref type="bibr" target="#b159">[172]</ref>, a subset of the negative candidates are used, following the common practice adopted in MCE (Minimum Classification Error) training in speech recognition <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b105">118,</ref><ref type="bibr" target="#b404">417,</ref><ref type="bibr" target="#b405">418]</ref>. In other words, for each query and clicked-document pair, denoted by (QD + ) where Q is a query and D + is the clicked document, the set of D is approximated by including D + and only four randomly selected unclicked documents, denoted by D − j ; j = 1, . . . , 4}. In the study reported in <ref type="bibr" target="#b159">[172]</ref>, no significant difference was found when different sampling strategies were used to select the unclicked documents.</p><p>With the above simplification the DSSM parameters are estimated to maximize the approximate likelihood of the clicked documents given the queries across the training set</p><formula xml:id="formula_25">L(Λ) = log (Q,D + ,D − j ) P (D + | Q),</formula><p>where Λ denotes the parameter set of the DNN weights {W i } in the DSSM. In Figure <ref type="figure">9</ref>.2, we show the overall DSSM architecture that contains several DNNs. All these DNNs share the same weights but take different documents (one positive and several negatives) as inputs when training the DSSM parameters. Details of the gradient computation of this approximate loss function with respect to the DNN weights tied across documents and queries can be found in <ref type="bibr" target="#b159">[172]</ref> and are not elaborated here.</p><p>Most recently, the DSSM described above has been extended to its convolutional version, or C-DSSM <ref type="bibr" target="#b315">[328]</ref>. In the C-DSSM, semantically similar words within context are projected to vectors that are close to each other in the contextual feature space through a convolutional structure. The overall semantic meaning of a sentence is found to be determined by a few key words in the sentence, and thus the C-DSSM uses an additional max pooling layer to extract the most salient local features to form a fixed-length global feature vector. The global feature vector is then fed to the remaining nonlinear DNN layer(s) to map it to a point in the shared semantic space. The main motivation for using the convolutional structure in the C-DSSM is its ability to map a variable-length word sequence to a lowdimensional vector in a latent semantic space. Unlike most previous models that treat a query or a document as a bag of words, a query or a document in the C-DSSM is viewed as a sequence of words with contextual structures. By using the convolutional structure, local contextual information at the word n-gram level is modeled first. Then, salient local features in a word sequence are combined to form a global feature vector. Finally, the high-level semantic information of the word sequence is extracted to form a global vector representation. Like the DSSM just described, the C-DSSM is also trained on click-through data by maximizing the conditional likelihood of the clicked documents given a query using the back-propagation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4">Use of deep stacking networks for information retrieval</head><p>In parallel with the IR studies reviewed above, the deep stacking network (DSN) discussed in Section 6 has also been explored recently for IR with insightful results <ref type="bibr" target="#b75">[88]</ref>. The experimental results suggest that the classification error rate using the binary decision of "relevant" versus "non-relevant" from the DSN, which is closely correlated with the DSN training objective, is also generally correlated well with the NDCG (normalized discounted cumulative gain) as the most common IR quality measure. The exception is found in the region of high IR quality.</p><p>As described in Section 6, the simplicity of the DSN's training objective, the mean square error (MSE), drastically facilitates its successful applications to image recognition, speech recognition, and speech understanding. The MSE objective and classification error rate have been shown to be well correlated in these speech or image applications. For information retrieval (IR) applications, however, the inconsistency between the MSE objective and the desired objective (e.g., NDCG) is much greater than that for the above classification-focused applications. For example, the NDCG as a desirable IR objective function is a highly non-smooth function of the parameters to be learned, with a very different nature from the nonlinear relationship between MSE and classification error rate. Thus, it is of interest to understand to what extent the NDCG is reasonably well correlated with classification rate or MSE where the relevance level in IR is used as the DSN prediction target. Further, can the advantage of learning simplicity in the DSN be applied to improve IR quality measures such as the NDCG? Our experimental results presented in <ref type="bibr" target="#b75">[88]</ref> provide largely positive answers to both of the above questions. In addition, special care that need to be taken in implementing DSN learning algorithms when moving from classification to IR applications are addressed.</p><p>The IR task in the experiments of <ref type="bibr" target="#b75">[88]</ref> is the sponsored search related to ad placement. In addition to the organic web search results, commercial search engines also provide supplementary sponsored results in response to the user's query. The sponsored search results are selected from a database pooled by advertisers who bid to have their ads displayed on the search result pages. Given an input query, the search engine will retrieve relevant ads from the database, rank them, and display them at the proper place on the search result page; e.g., at the top or right hand side of the web search results. Finding relevant ads to a query is quite similar to common web search. For instance, although the documents come from a constrained database, the task resembles typical search ranking that targets on predicting document relevance to the input query. The experiments conducted for this task are the first with the use of deep learning techniques (based on the DSN architecture) on the ad-related IR problem. The preliminary results from the experiments are the close correlation between the MSE as the DSN training objective with the NDCG as the IR quality measure over a wide NDCG range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selected Applications in Object Recognition and Computer Vision</head><p>Over the past two years or so, tremendous progress has been made in applying deep learning techniques to computer vision, especially in the field of object recognition. The success of deep learning in this area is now commonly accepted by the computer vision community. It is the second area in which the application of deep learning techniques is successful, following the speech recognition area as we reviewed and analyzed in Sections 2 and 7.</p><p>Excellent surveys on the recent progress of deep learning for computer vision are available in the NIPS-2013 tutorial (https:// nips.cc/Conferences/2013/Program/event.php?ID=4170 with video recording at http://research.microsoft.com/apps/video/default.aspx? id=206976&amp;l=i) and slides at http://cs.nyu.edu/∼fergus/presentations/ nips2013_final.pdf, and also in the CVPR-2012 tutorial (http://cs.nyu. edu/∼fergus/tutorials/deep_learning_cvpr12). The reviews provided in this section below are based partly on these tutorials, in connection with the earlier deep learning material in this monograph. Another excellent source which this section draws from is the most recent Ph.D. thesis on the topic of deep learning for computer vision <ref type="bibr" target="#b421">[434]</ref>.</p><p>Over many years, object recognition in computer vision has been relying on hand-designed features such as SIFT (scale invariant feature transform) and HOG (histogram of oriented gradients), akin to the reliance of speech recognition on hand-designed features such as MFCC and PLP. However, features like SIFT and HOG only capture low-level edge information. The design of features to effectively capture mid-level information such as edge intersections or high-level representation such as object parts becomes much more difficult. Deep learning aims to overcome such challenges by automatically learning hierarchies of visual features in both unsupervised and supervised manners directly from data. The review below categorizes the many deep learning methods applied to computer vision into two classes: (1) unsupervised feature learning where the deep learning is used to extract features only, which may be subsequently fed to relatively simple machine learning algorithm for classification or other tasks; and (2) supervised learning methods where end-to-end learning is adopted to jointly optimize feature extractor and classifier components of the full system when large amounts of labeled training data are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">Unsupervised or generative feature learning</head><p>When labeled data are relatively scarce, unsupervised learning algorithms have been shown to learn useful visual feature hierarchies. In fact, prior to the demonstration of remarkable successes of CNN architectures with supervised learning in the 2012 ImageNet competition, much of the work in applying deep learning methods to computer vision had been on unsupervised feature learning. The original unsupervised deep autoencoder that exploits DBN pre-training was developed and demonstrated by Hinton and Salakhutdinov <ref type="bibr" target="#b151">[164]</ref> with success on the image recognition and dimensionality reduction (coding) tasks of MNIST with only 60,000 samples in the training set; see details of this task in http://yann.lecun.com/exdb/mnist/ and an analysis in <ref type="bibr">[78]</ref>. It is interesting to note that the gain of coding efficiency using the DBNbased autoencoder on the image data over the conventional method of principal component analysis as demonstrated in <ref type="bibr" target="#b151">[164]</ref> is very similar to the gain reported in <ref type="bibr" target="#b87">[100]</ref> and described in Section 4 of this monograph on the speech data over the traditional technique of vector quantization. Also, Nair and Hinton <ref type="bibr" target="#b252">[265]</ref> developed a modified DBN where the top-layer model uses a third-order Boltzmann machine. This type of DBN is applied to the NORB database -a three-dimensional object recognition task. An error rate close to the best published result on this task is reported. In particular, it is shown that the DBN substantially outperforms shallow models such as SVMs. In <ref type="bibr" target="#b345">[358]</ref>, two strategies to improve the robustness of the DBN are developed. First, sparse connections in the first layer of the DBN are used as a way to regularize the model. Second, a probabilistic de-noising algorithm is developed. Both techniques are shown to be effective in improving robustness against occlusion and random noise in a noisy image recognition task. DBNs have also been successfully applied to create compact but meaningful representations of images <ref type="bibr" target="#b347">[360]</ref> for retrieval purposes. On this large collection image retrieval task, deep learning approaches also produced strong results. Further, the use of a temporally conditional DBN for video sequence and human motion synthesis were reported in <ref type="bibr" target="#b348">[361]</ref>. The conditional RBM and DBN make the RBM and DBN weights associated with a fixed time window conditioned on the data from previous time steps. The computational tool offered in this type of temporal DBN and the related recurrent networks may provide the opportunity to improve the DBN-HMMs towards efficient integration of temporalcentric human speech production mechanisms into DBN-based speech production model.</p><p>Deep learning methods have a rich family, including hierarchical probabilistic and generative models (neural networks or otherwise). One most recent example of this type developed and applied to facial expression datasets is the stochastic feed-forward neural networks that can be learned efficiently and that can induce a rich multiple-mode distribution in the output space not possible with the standard, deterministic neural networks <ref type="bibr" target="#b346">[359]</ref>. In Figure <ref type="figure">10</ref>.1, we show the architecture of a typical stochastic feed-forward neural network with four hidden layers with mixed deterministic and stochastic neurons (left) used to model multi-mode distributions illustrated on the right. The stochastic network here is a deep, directed graphical model, where the generation process starts from input x, a neural face, and generates the output y, the facial expression. In face expression classification experiments, the learned unsupervised hidden features generated from this stochastic network are appended to the image pixels and helped to obtain superior accuracy to the baseline classifier based on the conditional RBM/DBN <ref type="bibr" target="#b348">[361]</ref>.</p><p>Perhaps the most notable work in the category of unsupervised deep feature learning for computer vision (prior to the recent surge of the work on CNNs) is that of <ref type="bibr" target="#b196">[209]</ref>, a nine-layer locally connected sparse autoencoder with pooling and local contrast normalization. The model has one billion connections, trained on the dataset with 10 million images downloaded from the Internet. The unsupervised feature learning methods allow the system to train a face detector without having to label images as containing a face or not. And the control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation.</p><p>Another set of popular studies on unsupervised deep feature learning for computer vision are based on deep sparse coding models <ref type="bibr" target="#b213">[226]</ref>. This type of deep models produced state-of-the-art accuracy results on the ImageNet object recognition tasks prior to the rise of the CNN architectures armed with supervised learning to perform joint feature learning and classification, which we turn to now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">Supervised feature learning and classification</head><p>The origin of the applications of deep learning to object recognition tasks can be traced to the convolutional neural networks (CNNs) in the early 90s; see a comprehensive overview in <ref type="bibr" target="#b199">[212]</ref>. The CNNbased architectures in the supervised learning mode have captured intense interest in computer vision since October 2012 shortly after the ImageNet competition results were released (http://www.imagenet.org/challenges/LSVRC/2012/). This is mainly due to the huge recognition accuracy gain over competing approaches when large amounts of labeled data are available to efficiently train large CNNs using GPU-like high-performance computing platforms. Just like DNNbased deep learning methods have outperformed previous state-ofthe-art approaches in speech recognition in a series of benchmark tasks including phone recognition, large-vocabulary speech recognition, noise-robust speech recognition, and multi-lingual speech recognition, CNN-based deep learning methods have demonstrated the same in a set of computer vision benchmark tasks including category-level object recognition, object detection, and semantic segmentation.</p><p>The basic architecture of the CNN described in <ref type="bibr" target="#b199">[212]</ref> is shown in Figure <ref type="figure">10</ref>.1. To incorporate the relative invariance of the spatial relationship in typical image pixels with respect to the location, the CNN uses a convolutional layer with local receptive fields and with tied filter weights, much like 2-dimensional FIR filters in image processing. The output of the FIR filters is then passed through a nonlinear activation function to create activation maps, followed by another nonlinear pooling (labeled as "subsampling" in Figure <ref type="figure">10</ref>.2) layer that reduces the data rate while providing invariance to slightly different input images. The output of the pooling layer is fed to a few fully connected layers as in the DNN discussed in earlier chapters. The whole architecture above is also called the deep CNN in the literature.</p><p>Deep models with convolution structure such as CNNs have been found effective and have been in use in computer vision and image recognition since 90s <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b172">185,</ref><ref type="bibr" target="#b179">192,</ref><ref type="bibr" target="#b185">198,</ref><ref type="bibr" target="#b199">212]</ref>. The most notable advance was achieved in the 2012 ImageNet LSVRC competition, in which The original convolutional neural network that is composed of multiple alternating convolution and pooling layers followed by fully connected layers.</p><p>[after <ref type="bibr" target="#b199">[212]</ref>, @IEEE].</p><p>the task is to train a model with 1.2 million high-resolution images to classify unseen images to one of the 1000 different image classes. On the test set consisting of 150k images, the deep CNN approach described in <ref type="bibr" target="#b185">[198]</ref> achieved the error rates considerably lower than the previous state-of-the-art. Very large deep-CNNs are used, consisting of 60 million weights, and 650,000 neurons, and five convolutional layers together with max-pooling layers. Additional two fully-connected layers as in the DNN described previously are used on top of the CNN layers. Although all the above structures were developed separately in earlier work, their best combination accounted for major part of the success. See the overall architecture of the deep CNN system in Figure <ref type="figure">10</ref>.3. Two additional factors contribute to the final success. The first is a powerful regularization technique called "dropout"; see details in <ref type="bibr" target="#b153">[166]</ref> and a series of further analysis and improvement in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b227">240,</ref><ref type="bibr" target="#b368">381,</ref><ref type="bibr" target="#b372">385]</ref>. In particular, Warde-Farley et al. <ref type="bibr" target="#b372">[385]</ref> analyzed the disentangling effects of dropout and showed that it helps because different members of the bag share parameters. Applications of the same "dropout" techniques are also successful for some speech recognition tasks <ref type="bibr" target="#b64">[65,</ref><ref type="bibr">81]</ref>. The second factor is the use of non-saturating neurons or rectified linear units (ReLU) that compute f (x) = max(x, 0), which significantly speeds up the overall training process especially with efficient GPU implementation. This deep-CNN system achieved a winning top-5 test error rate of 15.3% using extra training data from ImageNet Fall 2011 release, or 16.4% using only supplied training data in ImageNet-2012, significantly lower than 26.2% achieved by the second-best system which combines scores from many classifiers using a set of handcrafted features such as SIFT and Fisher vectors. See details in http:// www.image-net.org/challenges/LSVRC/2012/oxford_vgg.pdf about the best competing method. It is noted, however, that the Fishervector-encoding approach has recently been extended by Simonyan et al. <ref type="bibr" target="#b316">[329]</ref> via stacking in multiple layers to form deep Fisher networks, which achieve competitive results with deep CNNs at a smaller computational learning cost.</p><p>The state of the art performance demonstrated in <ref type="bibr" target="#b185">[198]</ref> using the deep-CNN approach is further improved by another significant margin during 2013, using a similar approach but with bigger models and larger amounts of training data. A summary of top-5 test error rates from 11 top-performing teams participating in the 2013 Ima-geNet ILSVRC competition is shown in Figure <ref type="figure">10</ref>.4, with the best result of the 2012 competition shown to the right most as the baseline.</p><p>Here we see rapid error reduction on the same task from the lowest pre-2012 error rate of 26.2% (non-neural networks) to 15.3% in 2012 and further to 11.2% in 2013, both achieved with deep-CNN technology. It is also interesting to observe that all major entries in the 2013 ImageNet ILSVRC competition is based on deep learning approaches. For example, the Adobe system shown in Figure <ref type="figure">10</ref>.4 is based on the deep-CNN reported in <ref type="bibr" target="#b185">[198]</ref> including the use of dropout. The network architecture is modified to include more filters and connections. At test time, image saliency is used to obtain 9 crops from original images, which are combined with the standard five multiview crops. The NUS system uses a non-parametric, adaptive method to combine the outputs from multiple shallow and deep experts, including deep-CNN, kernel, and GMM methods. The VGG system is described in <ref type="bibr" target="#b316">[329]</ref> and uses a combination of the deep Fisher vector network and the deep-CNN. The ZF system is based on a combination of a large CNN with a range of different architectures. The choice of architectures was assisted by visualization of model features using a deconvolutional network as described by Zeiler et al. <ref type="bibr" target="#b424">[437]</ref>, Zeiler and Fergus <ref type="bibr" target="#b422">[435,</ref><ref type="bibr" target="#b423">436]</ref>, and Zeiler ( <ref type="bibr" target="#b421">[434]</ref>). The CognitiveVision system uses an image classification scheme based on a DNN architecture. The method is inspired by cognitive psychophysics about how the human vision system first learns to classify the basic-level categories and then learns to classify categories at the subordinate level for fine-grained object recognition. Finally, the best-performing system called Clarifai in Figure <ref type="figure">10</ref>.4 is based on a large and deep CNN with dropout regularization. It augments the amount of training data by down-sampling images to 256 pixels. The system contains a total of 65M parameters. Multiple such models were averaged together to further boost performance. The main novelty is to use the visualization technique based on the deconvolutional networks as described in <ref type="bibr" target="#b421">[434,</ref><ref type="bibr" target="#b424">437]</ref> to identify what makes the deep model perform well, based on which a powerful deep architecture was chosen. See more details of these systems in http://www.imagenet.org/challenges/LSVRC/2013/results.php.</p><p>While the deep CNN has demonstrated remarkable classification performance on object recognition tasks, there has been no clear understanding of why they perform so well until recently. Zeiler and Fergus <ref type="bibr" target="#b422">[435,</ref><ref type="bibr" target="#b423">436]</ref> conducted research to address just this issue, and then used the gained understanding to further improve the CNN systems, which yielded excellent performance as shown in Figure <ref type="figure">10</ref>.4 with labels "ZF" and "Clarifai." A novel visualization technique is developed that gives insight into the function of intermediate feature layers of the deep CNN. The technique also sheds light onto the operation of the full network acting as a classifier. The visualization technique is based on a deconvolutional network, which maps the neural activities in intermediate layers of the original convolutional network back to the input pixel space. This allows the researchers to examine what input pattern originally caused a given activation in the feature maps. Figure <ref type="figure">10</ref>.5 (the top portion) illustrates how a deconvolutional network is attached to each of its layers, thereby providing a closed loop back to image pixels as the input to the original CNN. The information flow in this closed loop is as follows. First, an input image is presented to the deep CNN in a feed-forward manner so that the features at all layers are computed. To examine a given CNN activation, all other activations in the layer are set to zero and the feature maps are passed as input to the attached deconvolutional network's layer. Then, successive operations, opposite to the feed-forward computation in the CNN, are carried out including unpooling, rectifying, and filtering. This allows the reconstruction of the activity in the layer beneath that gave rise to the chosen activation. These operations are repeated until input layer is reached. During unpooling, non-invertibility of the max pooling operation in the CNN is resolved by an approximate inverse, where the locations of the maxima within each pooling region are recorded in a set of "switch" variables. These switches are used to place the reconstructions from the layer above into appropriate locations, preserving the structure of the stimulus. This procedure is shown at the bottom portion of Figure <ref type="figure">10</ref>.5.</p><p>In addition to the deep-CNN architecture described above, the DNN architecture has also been shown to be highly successful in a number of computer vision tasks <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref>. We have not found in the literature on direct comparisons among the CNN, DNN, and other related architectures on the identical tasks.</p><p>Finally, the most recent study on supervised learning for computer vision shows that the deep CNN architecture is not only successful for object/image classification discussed earlier in this section but also successful for objection detection in the whole images <ref type="bibr" target="#b115">[128]</ref>. The detection task is substantially more complex than the classification task.</p><p>As a brief summary of this chapter, deep learning has made huge inroads into computer vision, soon after its success in speech recognition discussed in Section 7. So far, it is the supervised learning paradigm based on the deep CNN architecture and the related classification techniques that are making the greatest impact, showcased by the ImageNet competition results from 2012 and 2013. These methods can be used for not only object recognition but also many other computer vision tasks. There has been some debate as to the reasons for the success of these CNN-based deep learning methods, and about their limitations. Many questions are still open as to how these methods can be tailored to certain computer vision applications and how to scale up the models and training data. Finally, we discussed a number of studies on unsupervised and generative approaches of deep learning to computer vision and image modeling problems in the earlier part of this chapter. Their performance has not been competitive with the supervised learning approach on object recognition tasks with ample training data. To achieve long term and ultimate success in computer vision, it is likely that unsupervised learning will be needed. To this end, many open problems in unsupervised feature learning and deep learning need to be addressed and much more research need to be carried out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selected Applications in Multimodal and Multi-task Learning</head><p>Multi-task learning is a machine learning approach that learns to solve several related problems at the same time, using a shared representation. It can be regarded as one of the two major classes of transfer learning or learning with knowledge transfer, which focuses on generalizations across distributions, domains, or tasks. The other major class of transfer learning is adaptive learning, where knowledge transfer is carried out in a sequential manner, typically from a source task to a target task <ref type="bibr" target="#b82">[95]</ref>. Multi-modal learning is a closely related concept to multi-task learning, where the learning domains or "tasks" cut across several modalities for human-computer interactions or other applications embracing a mixture of textual, audio/speech, touch, and visual information sources. The essence of deep learning is to automate the process of discovering effective features or representations for any machine learning task, including automatically transferring knowledge from one task to another concurrently. Multi-task learning is often applied to conditions where no or very little training data are available for the target task domain, and hence is sometimes called zero-shot or one-shot learning. It is evident that difficult multi-task leaning naturally fits the paradigm of deep learning or representation learning where the shared representations and statistical strengths across tasks (e.g., those involving separate modalities of audio, image, touch, and text) is expected to greatly facilitate many machine learning scenarios under low-or zero-resource conditions. Before deep learning methods were adopted, there had been numerous efforts in multi-modal and multi-task learning. For example, a prototype called MiPad for multi-modal interactions involving capturing, leaning, coordinating, and rendering a mix of speech, touch, and visual information was developed and reported in <ref type="bibr" target="#b162">[175,</ref><ref type="bibr" target="#b90">103]</ref>. And in <ref type="bibr" target="#b341">[354,</ref><ref type="bibr" target="#b430">443]</ref>, mixed sources of information from multiple-sensory microphones with separate bone-conductive and airborn paths were exploited to de-noise speech. These early studies all used shallow models and learning methods and achieved worse than desired performance. With the advent of deep learning, it is hopeful that the difficult multi-modal learning problems can be solved with eventual success to enable a wide range of practical applications. In this chapter, we will review selected applications in this area, organized according to different combinations of more than one modalities or learning tasks. Much of the work reviewed here is on-going research, and readers should expect follow-up publications in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.1">Multi-modalities: Text and image</head><p>The underlying mechanism for potential effectiveness of multi-modal learning involving text and image is the common semantics associated with the text and image. The relationship between the text and image may come, for example, from the text annotations of an image (as the training data for a multi-modal learning system). If the related text and image share the same representation in a common semantic space, the system can generalize to the unseen situation where either text or image is unavailable. It can thus be naturally used for zero-shot learning for image or text. In other words, multi-modality learning can use text information to help image/visual recognition, and vice versa. Exploiting text information for image/visual recognition constitutes most of the work done in this space, which we review in this section below.</p><p>The deep architecture, called DeViSE (deep visual-semantic embedding) and developed by Frome et al. <ref type="bibr" target="#b104">[117]</ref>, is a typical example of the multi-modal learning where text information is used to improve the image recognition system, especially for performing zero-shot learning. Image recognition systems are often limited in their ability to scale to large number of object categories, due in part to the increasing difficulty of acquiring sufficient training data with text labels as the number of image categories grows. The multi-modal DeViSE system is aimed to leverage text data to train the image models. The joint model is trained to identify image classes using both labeled image data and the semantic information learned from unannotated text. An illustration of the DeViSE architecture is shown in the center portion of Figure <ref type="figure">10</ref>.1. It is initialized with the parameters pre-trained at the lower layers of two models: the deep-CNN for image classification in the left portion of the figure and the text embedding model in the right portion of the figure. The part of the deep CNN, labeled "core visual model" in Figure <ref type="figure">10</ref>.1, is further learned to predict the target word-embedding vector using a projection layer labeled "transformation" and using a similarity metric. The loss function used in training adopts a combination of dot-product similarity and max-margin, hinge rank loss. The former is the un-normalized version of the cosine loss function used for training the DSSM model in <ref type="bibr" target="#b157">[170]</ref> as described in Section 9.3. The latter is similar to the earlier joint image-text model called WSABIE (web scale annotation by image embedding developed by Weston et al. <ref type="bibr" target="#b375">[388,</ref><ref type="bibr" target="#b376">389]</ref>. The results show that the information provided by text improves zero-shot image predictions, achieving good hit rates (close to 15%) across thousands of the labels never seen by the image model. The earlier WSABIE system as described in <ref type="bibr" target="#b375">[388,</ref><ref type="bibr" target="#b376">389]</ref> adopted a shallow architecture and trained a joint embedding model of both images and labels. Rather than using deep architectures to derive the highly nonlinear image (as well as text-embedding) feature vectors as in DeViSE, the WSABIE uses simple image features and a linear mapping to arrive at the joint embedding space. Further, it uses an embedding vector for each possible label. Thus, unlike DeViSE, WSABIE could not generalize to new classes. It is also interesting to compare the DeViSE architecture of Figure <ref type="figure">11</ref>.1 with the DSSM architecture of Figure <ref type="figure">9</ref>.2 in Section 9. The branches of "Query" and "Documents" in DSSM are analogous to the branches of "image" and "text-label" in DeViSE. Both DeViSE and DSSM use the objective function related to cosine distance between two vectors for training the network weights in an end-to-end fashion. One key difference, however, is that the two sets of inputs to the DSSM are both text (i.e., "Query" and "Documents" designed for IR), and thus mapping "Query" and "Documents" to the same semantic space is conceptually more straightforward compared with the need in DeViSE for mapping from one modality (image) to another (text). Another key difference is that the generalization ability of DeViSE to unseen image classes comes from computing text embedding vectors for many unsupervised text sources (i.e., with no image counterparts) that would cover the text labels corresponding to the unseen classes. The generalization ability of the DSSM over unseen words, however, is derived from a special coding scheme for words in terms of their constituent letters.</p><p>The DeViSE architecture has inspired a more recent method, which maps images into the semantic embedding space via convex combination of embedding vectors for the text label and the image classes <ref type="bibr" target="#b257">[270]</ref>. Here is the main difference. DeViSE replaces the last, softmax layer of a CNN image classifier with a linear transformation layer. The new transformation layer is then trained together with the lower layers of the CNN. The method in <ref type="bibr" target="#b257">[270]</ref> is much simpler -keeping the softmax layer of the CNN while not training the CNN. For a test image, the CNN first produces top N-best candidates. Then, the convex combination of the corresponding N embedding vectors in the semantic space is computed. This gives a deterministic transformation from the outputs of the softmax classifier into the embedding space. This simple multi-modal learning method is shown to work very well on the ImageNet zero-shot learning task.</p><p>Another thread of studies separate from but related to the above work on multi-modal learning involving text and image have centered on the use of multi-modal embeddings, where data from multiple sources with separate modalities of text and image are projected into the same vector space. For example, Socher and Fei-Fei <ref type="bibr" target="#b328">[341]</ref> project words and images into the same space using kernelized canonical correlation analysis. Socher et al. <ref type="bibr" target="#b329">[342]</ref> map images to single-word vectors so that the constructed multi-modal system can classify images without seeing any examples of the class, i.e., zero-shot learning similar to the capability of DeViSE. The most recent work by Socher et al. <ref type="bibr" target="#b330">[343]</ref> extends their earlier work from single-word embeddings to those of phrases and full-length sentences. The mechanism for mapping sentences instead of the earlier single words into the multi-modal embedding space is derived from the power of the recursive neural network described in Socher et al. <ref type="bibr" target="#b334">[347]</ref> as summarized in Section 8.2, and its extension with dependency tree.</p><p>In addition to mapping text to image (or vice versa) into the same vector space or to creating the joint image/text embedding space, multi-modal learning for text and image can also be cast in the framework of language models. In <ref type="bibr" target="#b183">[196]</ref>, a model of natural language is made conditioned on other modalities such as image as the focus of the study. This type of multi-modal language model is used to (1) retrieve images given complex description queries, (2) retrieve phrase descriptions given image queries, and (3) generate text conditioned on images. Word representations and image features are jointly learned by training the multi-modal language model together with a convolutional network. An illustration of the multi-modal language model is shown in Figure <ref type="figure">11</ref>.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.2">Multi-modalities: Speech and image</head><p>Ngiam et al. <ref type="bibr" target="#b255">[268,</ref><ref type="bibr" target="#b256">269]</ref> propose and evaluate an application of deep networks to learn features over audio/speech and image/video modalities. They demonstrate cross-modality feature learning, where better features for one modality (e.g., image) is learned when multiple modalities (e.g., speech and image) are present at feature learning time. A bi-modal deep autoencoder architecture for separate audio/speech and video/image input channels are shown in Figure <ref type="figure">11</ref>.3. The essence of this architecture is to use a shared, middle layer to represent both types of modalities. This is a straightforward generalization from the single-modal deep autoencoder for speech shown in  learn a shared audio and video representation, and evaluate it on a fixed task, where the classifier is trained with audio-only data but tested with video-only data and vice versa. The work concludes that deep learning architectures are generally effective in learning multimodal features from unlabeled data and in improving single modality features through cross modality information transfer. One exception is the cross-modality setting using the CUAVE dataset. The results presented in <ref type="bibr" target="#b256">[269,</ref><ref type="bibr" target="#b255">268]</ref> show that learning video features with both video and audio outperforms that with only video data. However, the same paper also shows that a model of <ref type="bibr" target="#b265">[278]</ref> in which a sophisticated signal processing technique for extracting visual features, together with the uncertainty-compensation method developed originally from robust speech recognition <ref type="bibr" target="#b91">[104]</ref>, gives the best classification accuracy in the cross-modal learning task, beating the features derived from the generative deep architecture designed for this task.</p><p>While the deep generative architecture for multimodal learning described in <ref type="bibr" target="#b255">[268,</ref><ref type="bibr" target="#b256">269]</ref> is based on non-probabilistic autoencoder neural nets, a probabilistic version based on deep Boltzmann machine (DBM) has appeared more recently for the same multimodal application. In <ref type="bibr" target="#b335">[348]</ref>, a DBM is used to extract a unified representation integrating separate modalities, useful for both classification and information retrieval tasks. Rather than using the "bottleneck" layers in the deep autoencoder to represent multimodal inputs, here a probability density is defined on the joint space of multimodal inputs, and states of suitably defined latent variables are used for the representation. The advantage of this probabilistic formulation, possibly lacking in the traditional deep autoencoder, is that the missing modality's information can be filled in naturally by sampling from its conditional distribution. More recent work on autoencoders <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30]</ref> shows the capability of generalized denoising autoencoders in carrying out sampling, thus they may overcome the earlier problem of filling-in the missing modality's information. For the bi-modal data consisting of image and text, the multimodal DBM was shown to slightly outperform the traditional version of the deep multimodal autoencoder as well as multimodal DBN in classification and information retrieval tasks. No results on the comparisons with the generalized version of deep autoencoders has been reported but may appear soon.</p><p>The several architectures discussed so far in this chapter for multimodal processing and learning can be regarded as special cases of more general multi-task learning and transfer learning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b46">47]</ref>. Transfer learning, encompassing both adaptive and multi-task learning, refers to the ability of a learning architecture and technique to exploit common hidden explanatory factors among different learning tasks. Such exploitation permits sharing of aspects of diverse types of input data sets, thus allowing the possibility of transferring knowledge across seemingly different learning tasks. As argued in <ref type="bibr" target="#b21">[22]</ref>, the learning architecture shown in Figure <ref type="figure">11</ref>.4 and the associated learning algorithms have an advantage for such tasks because they learn representations that capture underlying factors, a subset of which may be relevant for each particular task. We will discuss a number of such multi-task learning applications in the remainder of this chapter that are confined with a single modality of speech, natural language processing, or image domain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.3">Multi-task learning within the speech, NLP or image domain</head><p>Within the speech domain, one most interesting application of multitask learning is multi-lingual or cross-lingual speech recognition, where speech recognition for different languages is considered as different tasks. Various approaches have been taken to attack this rather challenging acoustic modeling problem for speech recognition, where the difficulty lies in the lack of transcribed speech data due to economic considerations in developing speech recognition systems for all languages in the world. Cross-language data sharing and data weighing are common and useful approaches for the GMM-HMM system <ref type="bibr" target="#b212">[225]</ref>.</p><p>Another successful approach for the GMM-HMM is to map pronunciation units across languages either via knowledge-based or data-driven methods <ref type="bibr" target="#b407">[420]</ref>. But they are much inferior to the DNN-HMM approach which we now summarize.</p><p>In recent papers of <ref type="bibr" target="#b81">[94,</ref><ref type="bibr" target="#b157">170]</ref> and <ref type="bibr" target="#b137">[150]</ref>, two research groups independently developed closely related DNN architectures with multi-task learning capabilities for multilingual speech recognition. See Figure <ref type="figure">11</ref>.5 for an illustration of this type of architecture. The idea behind these architectures is that the hidden layers in the DNN, when learned appropriately, serve as increasingly complex feature transformations sharing common hidden factors across the acoustic data in different languages. The final softmax layer representing a log-linear classifier makes use of the most abstract feature vectors represented in the topmost hidden layer. While the log-linear classifier is necessarily separate for different languages, the feature transformations can be shared across languages. Excellent multilingual speech recognition results are reported, far exceeding the earlier results using the GMM-HMM based approaches <ref type="bibr" target="#b212">[225,</ref><ref type="bibr" target="#b407">420]</ref>. The implication of this set of work is significant and far reaching. It points to the possibility of quickly building a high-performance DNN-based system for a new language from an existing multilingual DNN. This huge benefit would require only a small amount of training data from the target language, although having more data would further improve the performance. This multitask learning approach can reduce the need for the unsupervised pre-training stage, and can train the DNN with much fewer epochs. Extension of this set of work would be to efficiently build a language-universal speech recognition system. Such a system cannot only recognize many languages and improve the accuracy for each individual language, but A closely related DNN architecture, as shown in Figure <ref type="figure">11</ref>.6, with multitask learning capabilities was also recently applied to another acoustic modeling problem -learning joint representations for two separate sets of acoustic data <ref type="bibr" target="#b81">[94,</ref><ref type="bibr" target="#b208">221]</ref>. The set that consists of the speech data with 16 kHz sampling rate is of wideband and high quality, which is often collected from increasingly popular smart phones under the voice search scenario. Another, narrowband data set has a lower sampling rate of 8kHz, often collected using the telephony speech recognition systems.</p><p>As a final example of multi-task learning within the speech domain, let us consider phone recognition and word recognition as separate "tasks." That is, phone recognition results are used not for producing text outputs but for language-type identification or for spoken document retrieval. Then, the use of pronunciation dictionary in almost all speech systems can be considered as multi-task learning that share the tasks of phone recognition and word recognition. More advanced frameworks in speech recognition have pushed this direction further by advocating the use of even finer units of speech than phones to bridge the raw acoustic information of speech to semantic content of speech via a hierarchy of linguistic structure. These atomic speech units include "speech attributes" in the detection-based and knowledge-rich modeling framework for speech recognition, whose accuracy has been significantly boosted recently by the use of deep learning methods <ref type="bibr" target="#b319">[332,</ref><ref type="bibr" target="#b317">330,</ref><ref type="bibr" target="#b414">427]</ref>.</p><p>Within the natural language processing domain, the best known example of multi-task learning is the comprehensive studies reported in <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b62">63]</ref>, where a range of separate "tasks" of part-of-speech tagging, chunking, named entity tagging, semantic role identification, and similar-word identification in natural language processing are attacked using a common representation of words and a unified deep learning approach. A summary of these studies can be found in Section 8.2.</p><p>Finally, within the domain of image/vision as a single modality, deep learning has also been found effective in multi-task learning. Srivastava and Salakhutdinov <ref type="bibr" target="#b336">[349]</ref> present a multi-task learning approach based on hierarchical Bayesian priors in a DNN system applied to various image classification data sets. The priors are combined with a DNN, which improves discriminative learning by encouraging information sharing among tasks and by discovering similar classes among which knowledge is transferred. More specifically, methods are developed to jointly learn to classify images and a hierarchy of classes, such that "poor classes," for which there are relatively few training examples, can benefit from similar "rich classes," for which more training examples are available. This work can be considered as an excellent instance of learning output representations, in addition to learning input representation of the DNN as the focus of nearly all deep learning work reported in the literature.</p><p>As another example of multi-task learning within the singlemodality domain of image, Ciresan et al. <ref type="bibr" target="#b57">[58]</ref> applied the architecture of deep CNNs to character recognition tasks for Latin and for Chinese. The deep CNNs trained on Chinese characters are shown to be easily capable of recognizing uppercase Latin letters. Further, learning Chinese characters is accelerated by first pre-training a CNN on a small subset of all classes and then continuing to train on all classes. This monograph first presented a brief history of deep learning (focusing on speech recognition) and developed a categorization scheme to analyze the existing deep networks in the literature into unsupervised (many of which are generative), supervised, and hybrid classes. The deep autoencoder, the DSN (as well as many of its variants), and the DBN-DNN or pre-trained DNN architectures, one in each of the three classes, are discussed and analyzed in detail, as they appear to be popular and promising approaches based on the authors' personal research experiences. Applications of deep learning in five broad areas of information processing are also reviewed, including speech and audio (Section 7), natural language modeling and processing (Section 8), information retrieval (Section 9), object recognition and computer vision (Section 10), and multi-modal and multi-task learning (Section 11). There are other interesting yet non-mainstream applications of deep learning, which are not covered in this monograph. For interested readers, please consult recent papers on the applications of deep learning to optimal control in <ref type="bibr" target="#b206">[219]</ref>, to reinforcement learning in <ref type="bibr" target="#b243">[256]</ref>, to malware classification in <ref type="bibr" target="#b65">[66]</ref>, to compressed sensing in <ref type="bibr" target="#b264">[277]</ref>, to recognition confidence prediction in <ref type="bibr" target="#b160">[173]</ref>, to acoustic-articulatory inversion mapping in <ref type="bibr" target="#b356">[369]</ref>, to emotion recognition from video in <ref type="bibr" target="#b176">[189]</ref>, to emotion recognition from speech in <ref type="bibr" target="#b194">[207,</ref><ref type="bibr" target="#b209">222]</ref>, to spoken language understanding in <ref type="bibr" target="#b229">[242,</ref><ref type="bibr" target="#b353">366,</ref><ref type="bibr" target="#b390">403]</ref>, to speaker recognition in <ref type="bibr" target="#b338">[351,</ref><ref type="bibr" target="#b359">372]</ref>, to language-type recognition in <ref type="bibr" target="#b99">[112]</ref>, to dialogue state tracking for spoken dialogue systems in <ref type="bibr" target="#b81">[94,</ref><ref type="bibr" target="#b139">152]</ref>, to automatic voice activity detection in <ref type="bibr" target="#b429">[442]</ref>, to speech enhancement in <ref type="bibr" target="#b383">[396]</ref>, to voice conversion in <ref type="bibr" target="#b253">[266]</ref>, and to single-channel source separation in <ref type="bibr" target="#b119">[132,</ref><ref type="bibr" target="#b374">387]</ref>.</p><p>The literature on deep learning is vast, mostly coming from the machine learning community. The signal processing community embraced deep learning only within the past four years or so (starting around end of 2009) and the momentum is growing fast ever since. This monograph is written mainly from the signal and information processing perspective. Beyond surveying the existing deep learning work, a classificatory scheme based on the architectures and on the nature of the learning algorithms is developed, and an analysis and discussions with concrete examples are presented. It is our hope that the survey conducted in this monograph will provide insight for readers to better understand the capability of the various deep learning systems discussed in the monograph, the connection among different but similar deep learning methods, and how to design proper deep learning algorithms under different circumstances.</p><p>Throughout this review, the important message is conveyed that building and learning deep hierarchies of features are highly desirable. We have discussed the difficulty of learning parameters in all layers of deep networks in one shot due to optimization difficulties that need to be better understood. The unsupervised pre-training method in the hybrid architecture of the DBN-DNN, which we reviewed in detail in Section 5, appears to have offered a useful, albeit empirical, solution to poor local optima in optimization and to regularization for the deep model containing massive parameters even though a solid theoretical foundation is still lacking. The effectiveness of the pre-training method, which was one factor that stimulated the interest in deep learning by the signal processing community in 2009 via collaborations between academic and industrial researchers, is most prominent when the supervised training data are limited.</p><p>Deep learning is an emerging technology. Despite the empirical promising results reported so far, much more work needs to be carried out. Importantly, it has not been the experience of deep learning researchers that a single deep learning technique can be successful for all classification tasks. For example, while the popular learning strategy of generative pre-training followed by discriminative fine-tuning seems to work well empirically for many tasks, it failed to work for some other tasks that have been explored (e.g., language identification or speaker recognition; unpublished). For these tasks, the features extracted at the generative pre-training phase seem to describe the underlying speech variations well but do not contain sufficient information to distinguish between different languages. A learning strategy that can extract discriminative yet also invariant features is expected to provide better solutions. This idea has also been called "disentangling" and is developed further in <ref type="bibr" target="#b23">[24]</ref>. Further, extracting discriminative features may greatly reduce the model size needed in many of the current deep learning systems. Domain knowledge such as what kind of invariance is useful for a specific task in hand (e.g., vision, speech, or natural language) and what kind of regularization in terms of parameter constraints is key to the success of applying deep learning methods. Moreover, new types of DNN architectures and learning beyond the several popular ones discussed in this monograph are currently under active development by the deep learning research community (e.g., <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b76">89]</ref>), holding the promise to improve the performance of deep learning models in more challenging applications in signal processing and in artificial intelligence.</p><p>Recent published work showed that there is vast room to improve the current optimization techniques for learning deep architectures <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b195">208,</ref><ref type="bibr" target="#b225">238,</ref><ref type="bibr" target="#b226">239,</ref><ref type="bibr" target="#b298">311,</ref><ref type="bibr" target="#b343">356,</ref><ref type="bibr" target="#b380">393]</ref>. To what extent pre-training is essential to learning the full set of parameters in deep architectures is currently under investigation, especially when very large amounts of labeled training data are available, reducing or even obliterating the need for model regularization. Some preliminary results have been discussed in this monograph and in <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b148">161,</ref><ref type="bibr" target="#b310">323,</ref><ref type="bibr" target="#b416">429]</ref>.</p><p>In recent years, machine learning is becoming increasingly dependent on large-scale data sets. For instance, many of the recent successes of deep learning as discussed in this monograph have relied on the access to massive data sets and massive computing power. It would become increasingly difficult to explore the new algorithmic space without the access to large, real-world data sets and without the related engineering expertise. How well deep learning algorithms behave would depend heavily on the amount of data and computing power available. As we showed with speech recognition examples, a deep learning algorithm that appears to be performing not so remarkably on small data sets can begin to perform considerably better when these limitations are removed, one of main reasons for the recent resurgence in neural network research. As an example, the DBN pre-training that ignited a new era of (deep) machine learning research appears unnecessary if enough data and computing power are used.</p><p>As a consequence, effective and scalable parallel algorithms are critical for training deep models with large data sets, as in many common information processing applications such as speech recognition and machine translation. The popular mini-batch stochastic gradient technique is known to be difficult to parallelize over computers. The common practice nowadays is to use GPGPUs to speed up the learning process, although recent advance in developing asynchronous stochastic gradient descent learning has shown promises by using large-scale CPU clusters <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b196">209]</ref> and GPU clusters <ref type="bibr" target="#b58">[59]</ref>. In this interesting computing architecture, many different replicas of the DNN compute gradients on different subsets of the training data in parallel. These gradients are communicated to a central parameter server that updates the shared weights. Even though each replica typically computes gradients using parameter values not immediately updated, stochastic gradient descent is robust to the slight errors this has introduced. To make deep learning techniques scalable to very large training data, theoretically sound parallel learning and optimization algorithms together with novel architectures need to be further developed <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b168">181,</ref><ref type="bibr" target="#b309">322,</ref><ref type="bibr" target="#b343">356]</ref>. Optimization methods specific to speech recognition problems may need to be taken into account in order to push speech recognition advances to the next level <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b136">149,</ref><ref type="bibr" target="#b380">393]</ref>.</p><p>One major barrier to the application of DNNs and related deep models is that it currently requires considerable skill and experience to choose sensible values for hyper-parameters such as the learning rate schedule, the strength of the regularizer, the number of layers and the number of units per layer, etc. Sensible values for one hyper-parameter may depend on the values chosen for other hyper-parameters and hyper-parameter tuning in DNNs is especially expensive. Some interesting methods for solving the problem have been developed recently, including random sampling <ref type="bibr" target="#b31">[32]</ref> and Bayesian optimization procedure <ref type="bibr" target="#b324">[337]</ref>. Further research is needed in this important area.</p><p>This monograph, mainly in Sections 8 and 11 on natural language and multi-modal applications, has touched on some recent work on using deep learning methods to do reasoning, moving beyond the topic of more straightforward pattern recognition using supervised, unsupervised or hybrid learning methods to which much of this monograph has been devoted to. In principle, since deep networks are naturally equipped with distributed representations (rf. Table <ref type="table">3</ref>.1) using their layer-wise collections of units for coding relations and coding entities, concepts, events, topics, etc., they can potentially perform powerful reasoning over structures, as argued in various historical publications as well as recent essays <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b143">156,</ref><ref type="bibr" target="#b273">286,</ref><ref type="bibr" target="#b275">288,</ref><ref type="bibr" target="#b279">292,</ref><ref type="bibr" target="#b323">336,</ref><ref type="bibr" target="#b322">335]</ref>. While initial explorations on this capability of deep networks have recently appeared in the literature, as reviewed in Sections 8 and 11, much research is needed. If successful, this new type of deep learning "machine" will open up many novel and exciting applications in applied artificial intelligence as a "thinking brain." We expect growing work of deep learning in this area, full of new challenges, in the future.</p><p>Further, solid theoretical foundations of deep learning need to be established in a myriad of aspects. As an example, the success of deep learning in unsupervised learning has not been demonstrated as much as for supervised learning; yet the essence and major motivation of deep learning lie right in unsupervised learning for automatically discovering data representation. The issues involve appropriate objectives for learning effective feature representations and the right deep learning architectures/algorithms for distributed representations to effectively disentangle the hidden explanatory factors of variation in the data. Unfortunately, a majority of the successful deep learning techniques have so far dealt with unstructured or "flat" classification problems. For example, although speech recognition is a sequential classification problem by nature, in the most successful and large-scale systems, a separate HMM is used to handle the sequence structure and the DNN is only used to produce the frame-level, unstructured posterior distributions. Recent proposals have called for and investigated moving beyond the "flat" representations and incorporating structures in both the deep learning architectures and input and output representations <ref type="bibr">[79,</ref><ref type="bibr" target="#b123">136,</ref><ref type="bibr" target="#b325">338,</ref><ref type="bibr" target="#b336">349]</ref>.</p><p>Finally, deep learning researchers have been advised by neuroscientists to seriously consider a broader set of issues and learning architectures so as to gain insight into biologically plausible representations in the brain that may be useful for practical applications <ref type="bibr" target="#b259">[272]</ref>. How can computational neuroscience models about hierarchical brain structure help improve engineering deep learning architectures? How may the biologically feasible learning styles in the brain <ref type="bibr" target="#b145">[158,</ref><ref type="bibr" target="#b382">395]</ref> help design more effective and more robust deep learning algorithms? All these issues and those discussed earlier in this section will need intensive research in order to further push the frontier of deep learning. [73] L. Deng. A dynamic, feature-based approach to the interface between phonology and phonetics for speech modeling and recognition. Speech Communication, 24(4):299-323, 1998.</p><p>[74] L. Deng. Computational models for speech production. In Computational Models of Speech Pattern Processing, pages 199-213. Springer Verlag, 1999.</p><p>[75] L. Deng. Switching dynamic system models for speech articulation and acoustics. In Mathematical Foundations of Speech and Language Processing, pages 115-134. Springer-Verlag, New York, 2003. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 . 1 :</head><label>21</label><figDesc>Figure 2.1: Gartner hyper cycle graph representing five phases of a technology (http://en.wikipedia.org/wiki/Hype_cycle).</figDesc><graphic coords="18,143.88,176.70,334.80,211.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 . 2 :</head><label>22</label><figDesc>Figure 2.2: Applying Gartner hyper cycle graph to analyzing the history of artificial neural network technology (We thank our colleague John Platt during 2012 for bringing this type of "Hyper Cycle" graph to our attention for concisely analyzing the neural network history).</figDesc><graphic coords="19,156.60,174.78,309.00,235.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 . 3 :Figure 2 . 4 :</head><label>2324</label><figDesc>Figure 2.3: The famous NIST plot showing the historical speech recognition error rates achieved by the GMM-HMM approach for a number of increasingly difficult speech recognition tasks. Data source: http://itl.nist.gov/iad/mig/publications/ ASRhistory/index.html</figDesc><graphic coords="20,161.16,458.61,300.00,198.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 . 1 :</head><label>41</label><figDesc>Figure 4.1: The architecture of the deep autoencoder used in [100] for extracting binary speech codes from high-resolution spectrograms. [after [100], @Elsevier].</figDesc><graphic coords="40,187.68,175.23,247.01,240.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 . 1 ,</head><label>41</label><figDesc>where the two RBMs are shown in separate boxes. (See more detailed discussions on the RBM and DBN in Section 5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 . 2 :</head><label>42</label><figDesc>Figure 4.2: Top to Bottom: The ordinal spectrogram; reconstructions using input window sized of N = 1, 3, 9, and 13 while forcing the coding units to take values of zero one (i.e., a binary code) . [after [100], @Elsevier].</figDesc><graphic coords="41,176.64,410.29,269.36,220.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 4.3 shows various aspects of the encoding errors. At the top is the original speech utterance's spectrogram. The next two spectrograms are the blurry reconstruction from the 312-bit VQ and the much more faithful reconstruction from the 312-bit deep autoencoder. Coding errors from both coders, plotted as a function of time, are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 . 3 :</head><label>43</label><figDesc>Figure 4.3: Top to bottom: The original spectrogram from the test set; reconstruction from the 312-bit VQ coder; reconstruction from the 312-bit autoencoder; coding errors as a function of time for the VQ coder (blue) and autoencoder (red); spectrogram of the VQ coder residual; spectrogram of the deep autoencoder's residual.[after<ref type="bibr" target="#b87">[100]</ref>, @ Elsevier].</figDesc><graphic coords="42,166.68,375.67,288.65,233.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 . 4 :</head><label>44</label><figDesc>Figure 4.4: The original speech spectrogram and the reconstructed counterpart. A total of 312 binary codes are with one for each single frame.</figDesc><graphic coords="43,184.08,175.47,254.42,197.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figures 4 .</head><label>4</label><figDesc>4 to 4.10 show additional examples (unpublished) for the original un-coded speech spectrograms and their reconstructions using the deep autoencoder. They give a diverse number of binary codes for either a single or three consecutive frames in the spectrogram samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 . 5 :</head><label>45</label><figDesc>Figure 4.5: Same as Figure 4.4 but with a different TIMIT speech utterance.</figDesc><graphic coords="44,183.12,175.47,256.36,198.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 4 . 6 :</head><label>46</label><figDesc>Figure 4.6: The original speech spectrogram and the reconstructed counterpart. A total of 936 binary codes are used for three adjacent frames.</figDesc><graphic coords="44,179.16,429.79,264.00,159.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 4 . 7 :</head><label>47</label><figDesc>Figure 4.7: Same as Figure 4.6 but with a different TIMIT speech utterance.</figDesc><graphic coords="45,179.16,384.67,264.00,159.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 4 . 8 :</head><label>48</label><figDesc>Figure 4.8: Same as Figure 4.6 but with yet another TIMIT speech utterance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 4 . 9 :</head><label>49</label><figDesc>Figure 4.9: The original speech spectrogram and the reconstructed counterpart. A total of 2000 binary codes with one for each single frame.</figDesc><graphic coords="46,179.16,392.92,264.00,158.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 4 . 10 :</head><label>410</label><figDesc>Figure 4.10: Same as Figure 4.9 but with a different TIMIT speech utterance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 5 . 1 :</head><label>51</label><figDesc>Figure 5.1: A pictorial view of sampling from a RBM during RBM learning (courtesy of Geoff Hinton).</figDesc><graphic coords="52,179.16,175.61,264.00,73.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 5 . 2 :</head><label>52</label><figDesc>Figure 5.2: An illustration of the DBN-DNN architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 5 . 3 :</head><label>53</label><figDesc>Figure 5.3: Interface between DBN/DNN and HMM to form a DNN-HMM. This architecture, developed at Microsoft, has been successfully used in speech recognition experiments reported in<ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref>. [after<ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b67">68]</ref>, @IEEE].</figDesc><graphic coords="57,191.16,174.85,240.00,303.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 6 . 1 :</head><label>61</label><figDesc>Figure 6.1: A DSN architecture using input-output stacking. Four modules are illustrated, each with a distinct color. Dashed lines denote copying layers. [after [366], @IEEE].</figDesc><graphic coords="61,205.32,175.21,211.60,294.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 6 . 2 :</head><label>62</label><figDesc>Figure 6.2: Comparisons of a single module of a DSN (left) and that of a tensor DSN (TDSN). Two equivalent forms of a TDSN module are shown to the right. [after [180], @IEEE].</figDesc><graphic coords="64,191.16,175.68,240.00,122.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 6 . 3 :</head><label>63</label><figDesc>Figure 6.3: Stacking of TDSN modules by concatenating prediction vector with input vector. [after [180], @IEEE].</figDesc><graphic coords="65,235.92,175.30,150.48,265.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 6 . 4 :</head><label>64</label><figDesc>Figure 6.4: Stacking of TDSN modules by concatenating two hidden-layers' vectors with the input vector.</figDesc><graphic coords="66,191.40,175.05,239.40,232.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 6 . 5 :</head><label>65</label><figDesc>Figure 6.5: An example architecture of the K-DSN with three modules each of which uses a Gaussian kernel with different kernel parameters. [after [102], @IEEE].</figDesc><graphic coords="68,191.16,175.63,240.00,248.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 7 . 1 :</head><label>71</label><figDesc>Figure 7.1: Illustration of the joint learning of filter parameters and the rest of the deep network. [after [307], @IEEE].</figDesc><graphic coords="74,172.68,175.70,277.40,206.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 7 . 2 :</head><label>72</label><figDesc>Figure 7.2: Illustration of the use of bottleneck (BN) features extracted from a DNN in a GMM-HMM speech recognizer. [after [425], @IEEE].</figDesc><graphic coords="78,209.16,545.01,204.00,96.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>3</head><label>3</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head></head><label></label><figDesc>and 7.4, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 7 . 3 :</head><label>73</label><figDesc>Figure 7.3: Information flow in the bi-directional RNN, with both diagrammatic and mathematical descriptions. W's are weight matrices, not shown but can be easily inferred in the diagram. [after [136], @IEEE].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 7 . 4 :</head><label>74</label><figDesc>Figure 7.4: Information flow in an LSTM unit of the RNN, with both diagrammatic and mathematical descriptions. W's are weight matrices, not shown but can easily be inferred in the diagram. [after [136], @IEEE].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>Figure 8 . 1 :</head><label>81</label><figDesc>Figure 8.1: The SOUL-NNLM architecture with hierarchical structure in the output layers of the neural network [after [207], @IEEE].</figDesc><graphic coords="104,143.88,176.13,336.00,167.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head>Figure 8 .</head><label>8</label><figDesc>2 to show during training how the RNN unfolds as a deep feed-forward network (with three time steps back in time).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 8 . 2 :</head><label>82</label><figDesc>Figure 8.2: During the training of RNNLMs, the RNN unfolds into a deep feedforward network; based on Figure 3.2 of [245].</figDesc><graphic coords="105,185.16,175.05,252.00,348.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure 8 . 3 :</head><label>83</label><figDesc>Figure 8.3: The CBOW architecture (a) on the left, and the Skip-gram architecture (b) on the right. [after [246], @ICLR].</figDesc><graphic coords="109,143.88,176.44,336.00,200.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure 8 . 4 :</head><label>84</label><figDesc>Figure 8.4: The extended word-embedding model using a recursive neural network that takes into account not only local context but also global context. The global context is extracted from the document and put in the form of a global semantic vector, as part of the input into the original word-embedding model with local context. Taken from Figure 1 of [169]. [after [169], @ACL].</figDesc><graphic coords="110,143.88,508.93,336.00,103.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Figure 8 . 5 :</head><label>85</label><figDesc>Figure 8.5: Illustration of the basic approach reported in [122] for machine translation. Parallel pairs of source (denoted by f) and target (denoted by e) phrases are projected into continuous-valued vector representations (denoted by the two y vectors), and their translation score is computed by the distance between the pair in this continuous space. The projection is performed by deep neural networks (denoted by the two arrows) whose weights are learned on parallel training data. [after [121], @NIPS].</figDesc><graphic coords="112,143.88,176.29,336.00,135.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head>Figure 8 . 6 :</head><label>86</label><figDesc>Figure 8.6: Illustration of the neural tensor network described in<ref type="bibr" target="#b327">[340]</ref>, with two relationships shown as two slices in the tensor. The tensor is denoted by W[1:2] . The network contains a bilinear tensor layer that directly relates the two entity vectors (shown as e1 and e2) across three dimensions. Each dashed box denotes one of the two slices of the tensor. [after<ref type="bibr" target="#b327">[340]</ref>, @NIPS].</figDesc><graphic coords="114,191.16,454.35,240.00,160.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>Figure 9 . 1 :</head><label>91</label><figDesc>Figure 9.1: The DNN component of the DSSM architecture for computing semantic features. The DNN uses multiple layers to map high-dimensional sparse text features, for both Queries and Documents into low-dimensional dense features in a semantic space. [after [172], @CIKM].</figDesc><graphic coords="121,188.64,175.17,244.76,136.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>Figure 9 . 2 :</head><label>92</label><figDesc>Figure 9.2: Architectural illustration of the DSSM for document retrieval (from [170, 171]). All DNNs shown have shared weights. A set of n documents are shown here to illustrate the random negative sampling discussed in the text for simplifying the training procedure for the DSSM. [after [172], @CIKM].</figDesc><graphic coords="124,143.88,176.13,336.00,139.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>Figure 9 . 3 :</head><label>93</label><figDesc>Figure 9.3: The convolutional neural network component of the C-DSSM, with the window size of three is illustrated for the convolutional layer. [after [328], @WWW].</figDesc><graphic coords="125,163.68,175.32,294.93,214.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>Figure 10 . 1 :</head><label>101</label><figDesc>Figure 10.1: Left: A typical architecture of the stochastic feed-forward neural network with four hidden layers. Right: Illustration of how the network can produce a distribution with two distinct modes and use them to represent two or more different facial expressions y given a neutral face x. [after [359], @NIPS].</figDesc><graphic coords="131,144.84,175.54,332.60,103.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head>Figure 10 . 2 :</head><label>102</label><figDesc>Figure 10.2:The original convolutional neural network that is composed of multiple alternating convolution and pooling layers followed by fully connected layers.[after<ref type="bibr" target="#b199">[212]</ref>, @IEEE].</figDesc><graphic coords="133,143.88,175.91,336.00,90.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head>Figure 10 . 3 :</head><label>103</label><figDesc>Figure 10.3: The architecture of the deep-CNN system which won the 2012 Ima-geNet competition by a large margin over the second-best system and the state of the art by 2012.[after<ref type="bibr" target="#b185">[198]</ref>, @NIPS].</figDesc><graphic coords="134,143.88,175.79,336.00,103.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head>Figure 10 . 4 :</head><label>104</label><figDesc>Figure 10.4: Summary results of ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013), representing the state-of-the-are performance of object recognition systems. Data source: http://www.image-net.org/challenges/ LSVRC/2013/results.php.</figDesc><graphic coords="135,153.12,175.30,316.06,169.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head>Figure 10 . 5 :</head><label>105</label><figDesc>Figure 10.5: The top portion shows how a deconvolutional network's layer (left) is attached to a corresponding CNN's layer (right). The d econvolutional network reconstructs an approximate version of the CNN features from the layer below. The bottom portion is an illustration of the unpooling operation in the deconvolutional network, where "Switches" are used to record the location of the local max in each pooling region during pooling in the CNN. [after [436], @arXiv].</figDesc><graphic coords="137,148.08,174.97,326.18,326.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_45"><head>Figure 11 . 1 :</head><label>111</label><figDesc>Figure 11.1: Illustration of the multi-modal DeViSE architecture. The left portion is an image recognition neural network with a softmax output layer. The right portion is a skip-gram text model providing word embedding vectors; see Section 8.2 and Figure 8.3 for details. The center is the joint deep image-text model of DeViSE, with the two Siamese branches initialized by the image and word embedding models below the softmax layers. The layer labeled "transformation" is responsible for mapping the outputs of the image (left) and text (right) branches into the same semantic space. [after [117], @NIPS].</figDesc><graphic coords="142,170.52,175.30,280.86,104.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_46"><head>Figure 11 . 2 :</head><label>112</label><figDesc>Figure 11.2: Illustration of the multi-modal DeViSE architecture. The left portion is an image recognition neural network with a softmax output layer. The right portion is a skip-gram text model providing word embedding vectors; see Section 8.2 and Figure 8.3 for details. The center is the joint deep image-text model of DeViSE, with the two Siamese branches initialized by the image and word embedding models below the softmax layers. The layer labeled "transformation" is responsible for mapping the outputs of the image (left) and text (right) branches into the same semantic space.[after<ref type="bibr" target="#b183">[196]</ref>, @NIPS].</figDesc><graphic coords="144,185.16,175.59,252.12,125.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_47"><head></head><label></label><figDesc>Figure 4.1 of Section 4 to bi-modal counterpart. The authors further show how to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_48"><head>Figure 11 . 3 :</head><label>113</label><figDesc>Figure 11.3: The architecture of a deep denoising autoencoder for multi-modal audio/speech and visual features. [after [269], @ICML].</figDesc><graphic coords="145,212.16,175.17,197.80,213.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_49"><head>Figure 11 . 4 :</head><label>114</label><figDesc>Figure 11.4: A DNN architecture for multitask learning that is aimed to discover hidden explanatory factors shared among three tasks A, B, and C. [after [22], @IEEE].</figDesc><graphic coords="147,227.16,174.78,168.00,136.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_50"><head>Figure 11 . 5 :</head><label>115</label><figDesc>Figure 11.5: A DNN architecture for multilingual speech recognition. [after [170], @IEEE].</figDesc><graphic coords="148,228.36,175.60,165.68,187.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_51"><head>Figure 11 . 6 :</head><label>116</label><figDesc>Figure 11.6: A DNN architecture for speech recognition trained with mixedbandwidth acoustic data with 16-kHz and 8-kHz sampling rates; [after [221], @IEEE].</figDesc><graphic coords="149,226.44,175.64,169.01,173.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_52"><head>[</head><label></label><figDesc>71] L. Deng. A generalized hidden Markov model with state-conditioned trend functions of time for the speech signal. Signal Processing, 27(1):65-78, 1992. [72] L. Deng. A stochastic model of speech incorporating hierarchical nonstationarity. IEEE Transactions on Speech and Audio Processing, 1(4):471-475, 1993.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_53"><head>[</head><label></label><figDesc>76] L. Deng. Dynamic Speech Models -Theory, Algorithm, and Application. Morgan &amp; Claypool, December 2006. [77] L. Deng. An overview of deep-structured learning for information processing. In Proceedings of Asian-Pacific Signal &amp; Information Processing Annual Summit and Conference (APSIPA-ASC). October 2011.[78] L. Deng. The MNIST database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6), November 2012.[79] L.Deng. Design and learning of output representations for speech recognition. In Neural Information Processing Systems (NIPS) Workshop on Learning Output Representations. December 2013. [80] L. Deng. A tutorial survey of architectures, algorithms, and applications for deep learning. In Asian-Pacific Signal &amp; Information Processing Association Transactions on Signal and Information Processing. 2013. [81] L. Deng, O. Abdel-Hamid, and D. Yu. A deep convolutional neural network using heterogeneous pooling for trading acoustic invariance with phonetic confusion. In Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP). 2013. [82] L. Deng, A. Acero, L. Jiang, J. Droppo, and X. Huang. High performance robust speech recognition using stereo training data. In Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP). 2001.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>2008 NIPS Deep Learning Workshop;</figDesc><table><row><cell>• 2011 ICML Workshop on Learning Architectures, Representa-</cell></row><row><cell>tions, and Optimization for Speech and Visual Information Pro-</cell></row><row><cell>cessing;</cell></row><row><cell>• 2012 ICASSP Tutorial on Deep Learning for Signal and Informa-</cell></row><row><cell>tion Processing;</cell></row><row><cell>• 2012 ICML Workshop on Representation Learning;</cell></row><row><cell>• 2009 NIPS Workshop on Deep Learning for Speech Recognition</cell></row><row><cell>and Related Applications;</cell></row><row><cell>• 2009 ICML Workshop on Learning Feature Hierarchies;</cell></row></table><note>• 2012 Special Section on Deep Learning for Speech and Language Processing in IEEE Transactions on Audio, Speech, and Language Processing (T-ASLP, January); • 2010, 2011, and 2012 NIPS Workshops on Deep Learning and Unsupervised Feature Learning; • 2013 NIPS Workshops on Deep Learning and on Output Representation Learning; • 2013 Special Issue on Learning Deep Architectures in IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI, September).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 . 1 :</head><label>31</label><figDesc>Basic deep learning terminologies.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 . 1</head><label>31</label><figDesc></figDesc><table /><note>:(Continued)    </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7 . 1 :</head><label>71</label><figDesc>Comparisons of the DNN-HMM architecture with the generative model (e.g., the GMM-HMM) in terms of phone or word recognition error rates. From sub-tables A to D, the training data are increased approximately three orders of magnitudes.</figDesc><table><row><cell>Features</cell><cell>Setup</cell><cell>Error Rates</cell></row><row><cell cols="3">A: TIMIT Phone recognition (3 hours of training)</cell></row><row><cell>GMM</cell><cell>w. Hidden dynamics</cell><cell>24.8%</cell></row><row><cell>DNN</cell><cell>5 layers × 2048</cell><cell>23.0%</cell></row><row><cell cols="3">B: Voice Search SER (24-48 hours of training)</cell></row><row><cell>GMM</cell><cell>MPE (760 24-mix)</cell><cell>36.2%</cell></row><row><cell>DNN</cell><cell>5 layers × 2048</cell><cell>30.1%</cell></row><row><cell cols="3">C: Switch Board WER (309 hours of training)</cell></row><row><cell>GMM</cell><cell>BMMI (9K 40-mix)</cell><cell>23.6%</cell></row><row><cell>DNN</cell><cell>7 layers × 2048</cell><cell>15.8%</cell></row><row><cell cols="3">D: Switch Board WER (2000 hours of training)</cell></row><row><cell>GMM</cell><cell>BMMI (18K 72-mix)</cell><cell>21.7%</cell></row><row><cell>DNN</cell><cell>7 layers × 2048</cell><cell>14.6%</cell></row><row><cell>7</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>.1.2.2 The use of DNN-derived features in a separate recognizer One</head><label></label><figDesc>clear weakness of the above DNN-HMM architecture for speech recognition is that much of the highly effective techniques for the GMM-HMM systems, including discriminative training (in both feature space and model space), unsupervised speaker adaptation, noise robustness, and scalable batch training tools for big training data,</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploring convolutional neural network structures and optimization for speech recognition</title>
		<author>
			<persName><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep segmental neural networks for speech recognition</title>
		<author>
			<persName><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition</title>
		<author>
			<persName><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">HMM adaptation using vector taylor series for noisy speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kristjansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What regularized autoencoders learn from the data generating distribution</title>
		<author>
			<persName><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
				<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning comes of age</title>
		<author>
			<persName><forename type="first">G</forename><surname>Anthes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the Association for Computing Machinery (ACM)</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="13" to="15" />
			<date type="published" when="2013-06">June 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep machine learning -a new frontier in artificial intelligence</title>
		<author>
			<persName><forename type="first">I</forename><surname>Arel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="13" to="18" />
			<date type="published" when="2010-11">November 2010</date>
		</imprint>
	</monogr>
	<note>References</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep neural network language models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Arisoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Human Language Technology Conference and the North American Chapter of the Association of Computational Linguistics (HLT-NAACL) Workshop</title>
				<meeting>the Joint Human Language Technology Conference and the North American Chapter of the Association of Computational Linguistics (HLT-NAACL) Workshop</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convex two-layer modeling</title>
		<author>
			<persName><forename type="first">O</forename><surname>Aslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive dropout for training deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Research developments and directions in speech recognition and understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>O'shaughnessy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="75" to="80" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Updated MINS report on speech recognition and understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>O'shaughnessy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2009-07">July 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding dropout</title>
		<author>
			<persName><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sadowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning for music</title>
		<author>
			<persName><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bello</surname></persName>
		</author>
		<ptr target="http://www.icassp2014.org/special_sections.html#ss8" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Analyzing drum patterns using conditional deep belief networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Batternberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wessel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Music Information Retrieval (ISMIR)</title>
				<meeting>International Symposium on Music Information Retrieval (ISMIR)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-level adaptive networks in tandem and hybrid ASR systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Artificial neural networks and their application to sequence recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<pubPlace>Montreal, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>McGill University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">New distributed probabilistic language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>University of Montreal</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Neural net language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Scholarpedia</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations and Trends in Machine Learning</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research Workshop and Conference Proceedings</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="17" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning of representations: Looking forward</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Language and Speech Processing</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Advances in optimizing recurrent networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Boulanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Global optimization of a neural network-hidden markov model hybrid</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Flammia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kompe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="252" to="259" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Greedy layerwise training of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Thibodeau-Laufer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<idno>arXiv 1306:1091</idno>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013">2013. 2014</date>
		</imprint>
	</monogr>
	<note>Deep generative stochastic networks trainable by backprop</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generalized denoising autoencoders as generative models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An application of discriminative feature extraction to filter-bank-based speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Biem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katagiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="96" to="110" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic graphical models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="29" to="42" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graphical model architectures for speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bartels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="89" to="100" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data -application to word-sense disambiguation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for the Advancement of Artificial Intelligence (AAAI)</title>
				<meeting>Association for the Advancement of Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">From machine learning to machine reasoning: An essay</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="3207" to="3260" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Large scale online learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Modeling Temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription</title>
		<author>
			<persName><forename type="first">N</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Audio chord recognition with recurrent neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Music Information Retrieval (ISMIR)</title>
				<meeting>International Symposium on Music Information Retrieval (ISMIR)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<title level="m">Connectionist Speech Recognition: A Hybrid Approach</title>
				<meeting><address><addrLine>Norwell, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Hierarchical learning: Theory with applications in speech and vision</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bouvrie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Ph.D. thesis, MIT</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stacked regression</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="49" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An investigation of segmental hidden dynamic models of speech coarticulation for automatic speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bridle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Picone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Reagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Workshop on Language Engineering</title>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
			<publisher>Johns Hopkins</publisher>
		</imprint>
	</monogr>
	<note>CLSP</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Large vocabulary speech recognition on parallel architectures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cardinal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2290" to="2300" />
			<date type="published" when="2013-11">November 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A primal-dual method for training recurrent neural networks constrained by the echo-state property</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
				<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014-04">April 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pipelined backpropagation for context-dependent deep neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eversole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hmm-based speech recognition using state-dependent, discriminatively derived transforms on Mel-warped DFT features</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chengalvarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Speech and Audio Processing</title>
				<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="243" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Use of generalized dynamic feature parameters for speech recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chengalvarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Speech and Audio Processing</title>
				<imprint>
			<date type="published" when="1997">1997a</date>
			<biblScope unit="page" from="232" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Speech trajectory discrimination using the minimum classification error learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chengalvarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="505" to="515" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Kernel methods for deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="342" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep neural networks segment neuronal membranes in electron microscopy images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep, big, simple neural nets for handwritten digit recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="2010-12">December 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A committee of neural networks for traffic sign classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Neural Networks (IJCNN)</title>
				<meeting>International Joint Conference on Neural Networks (IJCNN)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Transfer learning for Latin and Chinese characters with deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Neural Networks (IJCNN)</title>
				<meeting>International Joint Conference on Neural Networks (IJCNN)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep learning with COTS HPC</title>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>References</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Stacked sequential learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>De Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence (IJCAI)</title>
				<meeting>International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="671" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep learning for efficient discriminative parsing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Artificial Intelligence and Statistics (AISTATS)</title>
				<meeting>Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Phone recognition with the mean-covariance restricted boltzmann machine</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Improving deep neural networks for LVCSR using rectified linear units and dropout</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Large-scale malware classification using random projections and neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stokes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Context-dependent DBN-HMMs in large vocabulary continuous speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Context-dependent, pre-trained deep neural networks for large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012-01">January 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Large scale distributed deep networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Porting concepts from DNNs back to GMMs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Demuynck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Triefenbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<meeting>the Automatic Speech Recognition and Understanding Workshop (ASRU)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>References</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Speaker-independent phonetic classification using hidden markov models with state-conditioned mixtures of trend functions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aksmanovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="319" to="324" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Speech recognition using hidden Markov models with polynomial regression functions as nonstationary states</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aksmanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="507" to="520" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Sequence classification using the high-level features extracted from deep neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Structural design of a hidden Markov model based speech recognizer using multi-valued phonetic features: Comparison with segmental speech units</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Erler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3058" to="3067" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Analysis of correlation structure for a neural predictive model with application to speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hassanein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elmasry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="331" to="339" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Deep stacking networks for information retrieval</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">New types of deep neural network learning for speech recognition and related applications: An overview</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Challenges in adopting speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the Association for Computing Machinery (ACM)</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="13" />
			<date type="published" when="2004-01">January 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Parallel training of deep stacking networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012">2012b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Phonemic hidden Markov models with continuous mixture output densities for large vocabulary word recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lennig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1677" to="1681" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Large vocabulary word recognition using context-dependent allophonic hidden Markov models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lennig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="345" to="357" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Recent advances in deep learning for speech research at Microsoft</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Machine learning paradigms in speech recognition: An overview</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, &amp; Language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1060" to="1089" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Spontaneous speech recognition using a statistical coarticulatory model for the vocal tract resonance dynamics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society America</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="3036" to="3048" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Speech Processing -A Dynamic and Optimization-Oriented Approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>O'shaughnessy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Marcel Dekker</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Production models as a structural basis for automatic speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="93" to="111" />
			<date type="published" when="1997-08">August 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Transitional speech units and their representation by regressive Markov states: Applications to speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sameti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on speech and audio processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="301" to="306" />
			<date type="published" when="1996-07">July 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Binary coding of speech spectrograms using a deep autoencoder</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">A statistical approach to automatic speech recognition using the atomic speech units constructed from overlapping articulatory features</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2702" to="2719" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Use of kernel deep convex networks and end-to-end learning for spoken language understanding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hakkani-Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Workshop on Spoken Language Technologies</title>
				<meeting>IEEE Workshop on Spoken Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012-12">December 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Distributed speech processing in mipad&apos;s multimodal user interface</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Hon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="605" to="619" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Dynamic compensation of HMM variances using the feature enhancement uncertainty computed from a parametric model of speech distortion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="412" to="421" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>References</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Use of differential cepstra as acoustic features in hidden trajectory modeling for phonetic recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Deep convex network: A scalable architecture for speech pattern classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">A bidirectional target filtering model of speech coarticulation: Two-stage implementation for phonetic recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio and Speech Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="256" to="265" />
			<date type="published" when="2006-01">January 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Structured speech modeling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1492" to="1504" />
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Deep learning for speech recognition and related applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS) Workshop</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Scalable stacking and learning for building deep architectures</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2012">2012a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">A deep learning approach to machine transliteration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 4th Workshop on Statistical Machine Translation</title>
				<meeting>4th Workshop on Statistical Machine Translation<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-03">March 2009</date>
			<biblScope unit="page" from="233" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Automatic language recognition using deep neural networks. Thesis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Diez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-09">September 2013</date>
		</imprint>
		<respStmt>
			<orgName>Universidad Autonoma de Madrid, SPAIN</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Combining stochastic average gradient and hessian-free optimization for sequence training of deep neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dognin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<meeting>the Automatic Speech Recognition and Understanding Workshop (ASRU)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courvelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vencent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="201" to="208" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">F0 contour prediction with a deep belief network-gaussian process hybrid model</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hoory</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6885" to="6889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">The hierarchical hidden Markov model: Analysis and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="41" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Phone-discriminating minimum classification error (p-mce) training for phonetic recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Model-based approaches to handling uncertainty</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robust Speech Recognition of Uncertain or Missing Data: Theory and Application</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="101" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Clickthrough-based translation models for web search: From word models to phrase models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Information and Knowledge Management (CIKM)</title>
				<meeting>Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Learning semantic representations for the phrase translation model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS) Workshop on Deep Learning</title>
				<meeting>Neural Information Processing Systems (NIPS) Workshop on Deep Learning</meeting>
		<imprint>
			<date type="published" when="2013-12">December 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Learning semantic representations for the phrase translation model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<idno>MSR-TR-2013-88</idno>
		<imprint>
			<date type="published" when="2013-09">September 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations for translation modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics (ACL)</title>
				<meeting>Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Clickthrough-based latent semantic models for web search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Special Interest Group on Information Retrieval (SIGIR)</title>
				<meeting>Special Interest Group on Information Retrieval (SIGIR)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Discriminative learning of sum-product networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Domingo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">How the brain might work: A hierarchical and temporal model for learning and recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>George</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Error approximation and minimum phone error acoustic model estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1269" to="1279" />
			<date type="published" when="2010-08">August 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2524v1</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feed-forward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Artificial Intelligence and Statistics (AISTATS)</title>
				<meeting>Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>References</note>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Artificial Intelligence and Statistics (AIS-TATS)</title>
				<meeting>Artificial Intelligence and Statistics (AIS-TATS)</meeting>
		<imprint>
			<date type="published" when="2011-04">April 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Multi-prediction deep boltzmann machines</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Grais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2746v1</idno>
		<title level="m">Deep neural networks for single channel source separation</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Sequence transduction with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Representation Learning Workshop, International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labeling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional LSTM</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<meeting>the Automatic Speech Recognition and Understanding Workshop (ASRU)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Optimizing bottle-neck features for LVCSR</title>
		<author>
			<persName><forename type="first">F</forename><surname>Grezl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fousek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Learnednorm pooling for deep feedforward and recurrent neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1311.1780" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="307" to="361" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Transcribing meetings with the AMIDA systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Grezl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hannani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huijbregts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lincoln</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="486" to="498" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Learning features from music audio with deep belief networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Music Information Retrieval (ISMIR)</title>
				<meeting>International Symposium on Music Information Retrieval (ISMIR)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Hierarchical temporal memory including HTM cortical learning algorithms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dubinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numenta Technical Report</title>
		<imprint>
			<date type="published" when="2010-12-10">December 10 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">On Intelligence: How a New Understanding of the Brain will lead to the Creation of Truly Intelligent Machines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Blakeslee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Times Books</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Speech recognition, machine translation, and speech translation -a unifying discriminative framework</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2011-11">November 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Optimization in speech-centric information processing: Criteria and techniques</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Speech-centric information processing: An optimization-oriented approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
				<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Discriminative learning in sequential pattern recognition -a unifying review for optimization-oriented speech recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="14" to="36" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Equivalence of generative and log-liner models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lehnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schluter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1138" to="1148" />
			<date type="published" when="2011-02">February 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Investigations on an EM-style optimization algorithm for discriminative training of HMMs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schluter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2616" to="2626" />
			<date type="published" when="2013-12">December 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Multilingual acoustic models using distributed deep neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Discriminative input stream combination for conditional random field phone recognition</title>
		<author>
			<persName><forename type="first">I</forename><surname>Heintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fosler-Lussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1533" to="1546" />
			<date type="published" when="2009-11">November 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Deep neural network approach for the dialog state tracking challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Special Interest Group on Disclosure and Dialogue (SIGDIAL)</title>
				<meeting>Special Interest Group on Disclosure and Dialogue (SIGDIAL)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Training and analysing deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Tandem connectionist feature extraction for conventional HMM systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Speech recognition using augmented conditional random fields</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hifny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="354" to="365" />
			<date type="published" when="2009-02">February 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Mapping part-whole hierarchies into connectionist networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="47" to="75" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Preface to the special issue on connectionist symbol processing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">The ups and downs of Hebb synapses</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Psychology</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="10" to="13" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<title level="m" type="main">A practical guide to training restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>2010-003</idno>
		<imprint>
			<date type="published" when="2010-08">August 2010</date>
		</imprint>
		<respStmt>
			<orgName>Univ. Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">UTML Tech Report</note>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">A better way to learn features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the Association for Computing Machinery (ACM)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2011-10">October 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012-11">November 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Transforming autoencoders</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Artificial Neural Networks</title>
				<meeting>International Conference on Artificial Neural Networks</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006-07">July 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Discovering binary codes for documents by learning deep generative models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in Cognitive Science</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580v1</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<title level="m" type="main">Untersuchungen zu dynamischen neuronalen netzen. Diploma thesis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
		<respStmt>
			<orgName>Institut fur Informatik, Technische Universitat Munchen</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics (ACL)</title>
				<meeting>Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Cross-language knowledge transfer using multilingual deep neural networks with shared hidden layers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Random features for kernel deep convex network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM) International Conference Information and Knowledge Management (CIKM)</title>
				<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Predicting speech recognition confidence using deep learning with word identity and score features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Hierarchical bayesian language models for conversational speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1941" to="1954" />
			<date type="published" when="2010-11">November 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Mipad: A multimodal interaction prototype</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duchene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Semi-supervised GMM and DNN acoustic model training with multi-system combination and confidence re-calibration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2360" to="2364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Rethinking automatic chord recognition with convolutional neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning and Application (ICMLA)</title>
				<meeting>International Conference on Machine Learning and Application (ICMLA)</meeting>
		<imprint>
			<date type="published" when="2012">2012a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Moving beyond feature design: Deep architectures and automatic feature learning in music informatics</title>
		<author>
			<persName><forename type="first">E</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Symposium on Music Information Retrieval (ISMIR)</title>
				<meeting>International Symposium on Music Information Retrieval (ISMIR)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>References</note>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Feature learning and deep architectures: New directions for music informatics</title>
		<author>
			<persName><forename type="first">E</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">A deep architecture with bilinear modeling of hidden representations: Applications to phonetic recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Tensor deep stacking networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1944" to="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Impact of deep MLP architecture on different modeling techniques for under-resourced speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Imseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<meeting>the Automatic Speech Recognition and Understanding Workshop (ASRU)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Learning a better representation of speech sound waves using restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Application of pre-trained deep neural networks to large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">What is the best multistage architecture for object recognition?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision</title>
				<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2146" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Parameter estimation of statistical models using convex optimization: An advanced method of discriminative training for speech and language processing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="115" to="127" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation for multivariate mixture observations of Markov chains</title>
		<author>
			<persName><forename type="first">B</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sondhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="307" to="309" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Minimum classification error rate methods for speech recognition</title>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions On Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="257" to="265" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Combining modality specific deep neural networks for emotion recognition in video</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kahou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Multimodal Interaction (ICMI)</title>
				<meeting>International Conference on Multimodal Interaction (ICMI)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Multi-distribution deep belief network for speech synthesis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="8012" to="8016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Discriminative piecewise linear transformation based on deep learning for noise robust automatic speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kashiwagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Minematsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hirose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<meeting>the Automatic Speech Recognition and Understanding Workshop (ASRU)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Learning convolutional feature hierarchies for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Enhanced phone posteriors for improving speech recognition systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ketabdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1094" to="1106" />
			<date type="published" when="2010-08">August 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Scalable minimum bayes risk training of deep neural network acoustic models using distributed hessian-free optimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Multimodal neural language models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS) Deep Learning Workshop</title>
				<meeting>Neural Information Processing Systems (NIPS) Deep Learning Workshop</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Eigentriphones for context-dependent acoustic modeling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1285" to="1294" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Integrating deep neural networks into structural classification approach based on weighted finite-state transducers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kubo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<monogr>
		<title level="m" type="main">How to Create a Mind</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kurzweil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-12">December 2012</date>
			<publisher>Viking Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Cross-lingual automatic speech recognition using tandem features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2506" to="2515" />
			<date type="published" when="2013-12">December 2013</date>
		</imprint>
	</monogr>
	<note>References</note>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">A time-delay neural network architecture for isolated word recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="43" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">Classification using discriminative restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Emotion recognition from spontaneous speech using hidden markov models with deep belief networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<meeting>the Automatic Speech Recognition and Understanding Workshop (ASRU)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Training continuous space language models: Some practical issues</title>
		<author>
			<persName><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="778" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Structured output layer neural network language model</title>
		<author>
			<persName><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Oparin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Structured output layer neural network language models for speech recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Oparin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="197" to="206" />
			<date type="published" when="2013-01">January 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">On optimization methods for deep learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prochnow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Learning invariant feature hierarchies</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
				<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>M. Arbib</editor>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="255" to="258" />
			<pubPlace>Cambridge, Massachusetts</pubPlace>
		</imprint>
	</monogr>
	<note>The Handbook of Brain Theory and Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
				<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Energy-based models in document recognition and computer vision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Document Analysis and Recognition (ICDAR)</title>
				<meeting>International Conference on Document Analysis and Recognition (ICDAR)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">From knowledge-ignorant to knowledge-rich modeling: A new speech research paradigm for next-generation automatic speech recognition</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Spoken Language Processing (ICSLP)</title>
				<meeting>International Conference on Spoken Language Processing (ICSLP)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="109" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Unsupervised learning of hierarchical representations with convolutional deep belief networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the Association for Computing Machinery (ACM)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="95" to="103" />
			<date type="published" when="2011-10">October 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for audio classification using convolutional deep belief networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Largman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Deep spatiotemporal architectures and learning for protein structure prediction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<monogr>
		<title level="m" type="main">Exploring deep and recurrent architectures for optimal control</title>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.1761v1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">An overview of noise-robust automatic speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/Association for Computing Machinery (ACM) Transactions on Audio, Speech, and Language Processing</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Improving wideband speech recognition using mixed-bandwidth training data in CD-DNN-HMM</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Spoken Language Technology (SLT)</title>
				<meeting>IEEE Spoken Language Technology (SLT)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Hybrid deep neural networkhidden markov model (DNN-HMM) based speech emotion recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang Etc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Conference on Affective Computing and Intelligent Interaction (ACII)</title>
				<meeting>Conference on Affective Computing and Intelligent Interaction (ACII)</meeting>
		<imprint>
			<date type="published" when="2013-09">September 2013</date>
			<biblScope unit="page" from="312" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Speaker adaptation of context dependent deep neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>References</note>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Large scale deep neural network acoustic modeling with semi-supervised training data for youtube video transcription</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<meeting>the Automatic Speech Recognition and Understanding Workshop (ASRU)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main">A study on multilingual acoustic modeling for large vocabulary ASR</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">Large-scale image classification: Fast feature extraction and SVM training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Modeling spectral envelopes using restricted boltzmann machines and deep belief networks for statistical parametric speech synthesis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio Speech Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2129" to="2139" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Modeling spectral envelopes using restricted boltzmann machines for statistical parametric speech synthesis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7825" to="7829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Articulatory control of HMMbased parametric speech synthesis using feature-space-switched multiple regression</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Richmond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2013-01">January 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Joint uncertainty decoding for noise robust subspace gaussian mixture models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1791" to="1804" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">A path-stack algorithm for optimizing dynamic regimes in a statistical hidden dynamical model of speech</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer, Speech and Language</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main">Efficient decoding strategies for conversational speech recognition using a constrained nonlinear state-space model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="590" to="602" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">Target-directed mixture dynamic models for spontaneous speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="58" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Rectifier nonlinearities improve neural network acoustic models. International Conference on Machine Learning (ICML) Workshop on Deep Learning for Audio, Speech, and Language Processing</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for noise reduction in robust ASR</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>O'neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">Scientists see promise in deep-learning programs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Markoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New York Times</title>
		<imprint>
			<date type="published" when="2012-11-24">November 24 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">Deep learning with hessian-free optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main">Learning recurrent neural networks with hessian-free optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<monogr>
		<title level="m" type="main">A PAC-bayesian tutorial with a dropout bound. ArX-ive1307</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2118. July 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Learning lexicons from speech using a pronunciation mixture model</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mcgraw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Badr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">366</biblScope>
			<date type="published" when="2013-02">February 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">Investigation of recurrentneural-network architectures and learning methods for spoken language understanding</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">Improving low-resource CD-DNN-HMM using dropout and multilingual DNN training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">Deep maxout networks for low resource speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<meeting>the Automatic Speech Recognition and Understanding Workshop (ASRU)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b232">
	<monogr>
		<title level="m" type="main">Statistical language models based on neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
				<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>References</note>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">Strategies for training large scale neural network language models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<meeting>the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168v1</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">A recognition method with parametric trajectory synthesized using direct relations between static and dynamic feature vector time series</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Minami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katagiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="957" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-T</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1751" to="1758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main">Playing arari with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602v1</idno>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS) Deep Learning Workshop</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main">Deep belief networks for phone recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS) Workshop Deep Learning for Speech Recognition and Related Applications</title>
				<meeting>Neural Information Processing Systems (NIPS) Workshop Deep Learning for Speech Recognition and Related Applications</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">Acoustic modeling using deep belief networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012-01">January 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b246">
	<analytic>
		<title level="a" type="main">Understanding how deep belief networks perform acoustic modelling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">Investigation of full-sequence training of deep belief networks for speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b248">
	<analytic>
		<title level="a" type="main">Deep and wide: Multiple layers in automatic speech recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012-01">January 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b249">
	<analytic>
		<title level="a" type="main">Pushing the envelope -aside [speech recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sonmez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sivadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shinozaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Cretin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Athineos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="81" to="88" />
			<date type="published" when="2005-09">September 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b250">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Artificial Intelligence and Statistics (AISTATS)</title>
				<meeting>Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b251">
	<monogr>
		<title level="m" type="main">Machine Learning -A Probabilistic Perspective</title>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b252">
	<analytic>
		<title level="a" type="main">3-d object recognition with deep belief nets</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b253">
	<analytic>
		<title level="a" type="main">Voice conversion in high-order eigen space using deep belief nets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nakashika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Takashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Takiguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ariki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b254">
	<analytic>
		<title level="a" type="main">Speech translation: Coupling of recognition and translation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b255">
	<analytic>
		<title level="a" type="main">Learning deep energy models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b256">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b257">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5650v2</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">Layered representations for learning and inferring office activity from multiple sensory channels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="163" to="180" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b259">
	<analytic>
		<title level="a" type="main">Can &apos;deep learning&apos; offer deep insights about visual representation?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Olshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS) Workshop on Deep Learning and Unsupervised Feature Learning</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b260">
	<analytic>
		<title level="a" type="main">Moving beyond the &apos;beads-on-a-string&apos; model of speech</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<meeting>the Automatic Speech Recognition and Understanding Workshop (ASRU)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b261">
	<analytic>
		<title level="a" type="main">From HMMs to segment models: A unified view of stochastic modeling for speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Digalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kimball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1996-09">September 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<analytic>
		<title level="a" type="main">Probabilistic template-based chord recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Oudre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fevotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Grenier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2249" to="2259" />
			<date type="published" when="2011-11">November 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b263">
	<analytic>
		<title level="a" type="main">Learning input and recurrent weight matrices in echo state networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS) Deep Learning Workshop</title>
				<imprint>
			<date type="published" when="2013-12">December 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b264">
	<analytic>
		<title level="a" type="main">Using deep stacking network to improve structured compressive sensing with multiple measurement vectors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b265">
	<analytic>
		<title level="a" type="main">Adaptive multimodal fusion by uncertainty compensation with application to audiovisual speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katsamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pitsikalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="423" to="435" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b266">
	<analytic>
		<title level="a" type="main">How to construct deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
				<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b267">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b268">
	<analytic>
		<title level="a" type="main">Conditional neural fields</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b269">
	<analytic>
		<title level="a" type="main">Initial evaluation of hidden dynamic models on conversational speech</title>
		<author>
			<persName><forename type="first">P</forename><surname>Picone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Regan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bridle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b270">
	<analytic>
		<title level="a" type="main">Analysis of MLP-based hierarchical phone posterior probability estimators</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garimella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Magimai-Doss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011-02">February 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b271">
	<analytic>
		<title level="a" type="main">Improved pre-training of deep belief networks using sparse encoding symmetric machines</title>
		<author>
			<persName><forename type="first">C</forename><surname>Plahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nahamoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b272">
	<analytic>
		<title level="a" type="main">Hierarchical bottleneck features for LVCSR</title>
		<author>
			<persName><forename type="first">C</forename><surname>Plahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b273">
	<analytic>
		<title level="a" type="main">Holographic reduced representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Plate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="623" to="641" />
			<date type="published" when="1995-05">May 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b274">
	<analytic>
		<title level="a" type="main">How the brain might work: The role of information and learning in understanding and replicating intelligence</title>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information: Science and Technology for the New Century</title>
				<editor>
			<persName><forename type="first">G</forename><surname>Jacovitt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Pettorossi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Consolo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Senni</surname></persName>
		</editor>
		<imprint>
			<publisher>Lateran University Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="45" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b275">
	<analytic>
		<title level="a" type="main">Recursive distributed representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="77" to="105" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b276">
	<analytic>
		<title level="a" type="main">Sum-product networks: A new deep architecture</title>
		<author>
			<persName><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Uncertainty in Artificial Intelligence</title>
				<meeting>Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b277">
	<analytic>
		<title level="a" type="main">Minimum phone error and I-smoothing for improved discriminative training</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b278">
	<analytic>
		<title level="a" type="main">Backpropagation training for multilayer conditional random field based phone recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fosler-Lussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b279">
	<analytic>
		<title level="a" type="main">Optimality: From neural networks to universal grammar</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page" from="1604" to="1610" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b280">
	<analytic>
		<title level="a" type="main">A tutorial on hidden markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
				<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
	<note>References</note>
</biblStruct>

<biblStruct xml:id="b281">
	<analytic>
		<title level="a" type="main">Sparse feature learning for deep belief networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b282">
	<analytic>
		<title level="a" type="main">Energy-based models in document recognition and computer vision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Document Analysis and Recognition (ICDAR)</title>
				<meeting>International Conference on Document Analysis and Recognition (ICDAR)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b283">
	<analytic>
		<title level="a" type="main">Modeling pixel means and covariances using factorized third-order boltzmann machines</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b284">
	<analytic>
		<title level="a" type="main">Efficient learning of sparse representations with an energy-based model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Poultney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b285">
	<analytic>
		<title level="a" type="main">On deep generative models with applications to recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b286">
	<analytic>
		<title level="a" type="main">Construction of state-dependent dynamic parameters by maximum likelihood: Applications to speech recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rathinavalu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="149" to="165" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b287">
	<analytic>
		<title level="a" type="main">Factorial hidden restricted boltzmann machines for noise robust speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fouset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dognin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b288">
	<analytic>
		<title level="a" type="main">Single-channel multi-talker speech recognition -graphical modeling approaches</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Olsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="66" to="80" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b289">
	<analytic>
		<title level="a" type="main">A direct adaptive method for faster backpropagation learning: The RPROP algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Braun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Neural Networks</title>
				<meeting>the IEEE International Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b290">
	<analytic>
		<title level="a" type="main">Contractive autoencoders: Explicit invariance during feature extraction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="833" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b291">
	<analytic>
		<title level="a" type="main">An application of recurrent nets to phone probability estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="298" to="305" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b292">
	<monogr>
		<title level="m" type="main">Ramabhadran. Accelerating hessian-free optimization for deep neural networks by implicit pre-conditioning and sampling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Horesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aravkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1309.1508v3</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b293">
	<analytic>
		<title level="a" type="main">Improvements to deep convolutional neural networks for LVCSR</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Beran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aravkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<meeting>the Automatic Speech Recognition and Understanding Workshop (ASRU)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b294">
	<analytic>
		<title level="a" type="main">Learning filter banks within a deep neural network framework</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<meeting>The Automatic Speech Recognition and Understanding Workshop (ASRU)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b295">
	<analytic>
		<title level="a" type="main">Autoencoder bottleneck features using deep belief networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b296">
	<analytic>
		<title level="a" type="main">Making deep belief networks effective for large vocabulary continuous speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<meeting>the Automatic Speech Recognition and Understanding Workshop (ASRU)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b297">
	<analytic>
		<title level="a" type="main">Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Arisoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b298">
	<analytic>
		<title level="a" type="main">Optimization techniques to improve training speed of deep neural networks for large speech tasks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2267" to="2276" />
			<date type="published" when="2013-11">November 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b299">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for LVCSR</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b300">
	<analytic>
		<title level="a" type="main">Exemplar-based sparse representation features: From TIMIT to LVCSR</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nahamoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kanevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<date type="published" when="2011-11">November 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b301">
	<analytic>
		<title level="a" type="main">Semantic hashing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Special Interest Group on Information Retrieval (SIGIR) Workshop on Information Retrieval and Applications of Graphical Models</title>
				<meeting>Special Interest Group on Information Retrieval (SIGIR) Workshop on Information Retrieval and Applications of Graphical Models</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b302">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Artificial Intelligence and Statistics (AISTATS)</title>
				<meeting>Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b303">
	<analytic>
		<title level="a" type="main">A better way to pretrain deep boltzmann machines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>References</note>
</biblStruct>

<biblStruct xml:id="b304">
	<analytic>
		<title level="a" type="main">Speaker adaptation of neural network acoustic models using i-vectors</title>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nahamoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<meeting>the Automatic Speech Recognition and Understanding Workshop (ASRU)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b305">
	<analytic>
		<title level="a" type="main">Deep belief nets for natural language call-routing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="5680" to="5683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b306">
	<analytic>
		<title level="a" type="main">Learning emotion-based acoustic features with deep belief networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE of Signal Processing to Audio and Acoustics</title>
				<meeting>IEEE of Signal Processing to Audio and Acoustics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b307">
	<analytic>
		<title level="a" type="main">Continuous space translation models for phrase-based statistical machine translation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computional Linguistics</title>
				<meeting>Computional Linguistics</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b308">
	<analytic>
		<title level="a" type="main">Large, pruned or continuous space language models on a gpu for statistical machine translation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohammed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Human Language Technology Conference and the North American Chapter of the Association of Computational Linguistics (HLT-NAACL) 2012 Workshop on the future of language modeling for Human Language Technology (HLT)</title>
				<meeting>the Joint Human Language Technology Conference and the North American Chapter of the Association of Computational Linguistics (HLT-NAACL) 2012 Workshop on the future of language modeling for Human Language Technology (HLT)</meeting>
		<imprint>
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b309">
	<analytic>
		<title level="a" type="main">On parallelizability of stochastic gradient descent for speech DNNs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b310">
	<analytic>
		<title level="a" type="main">Feature engineering in contextdependent deep neural networks for conversational speech transcription</title>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<meeting>the Automatic Speech Recognition and Understanding Workshop (ASRU)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="24" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b311">
	<analytic>
		<title level="a" type="main">Conversational speech transcription using context-dependent deep neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="437" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b312">
	<analytic>
		<title level="a" type="main">An investigation of deep neural networks for noise robust speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b313">
	<analytic>
		<title level="a" type="main">Autoregressive models for statistical parametric speech synthesis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="587" to="597" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b314">
	<analytic>
		<title level="a" type="main">Waveform-based speech recognition using hidden filter models: Parameter selection and sensitivity to power normalization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sheikhzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="80" to="91" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>ICASSP)</note>
</biblStruct>

<biblStruct xml:id="b315">
	<analytic>
		<title level="a" type="main">Learning semantic representations using convolutional neural networks for web search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings World Wide Web</title>
				<meeting>World Wide Web</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b316">
	<analytic>
		<title level="a" type="main">Deep fisher networks for large-scale image classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b317">
	<analytic>
		<title level="a" type="main">Hermitian polynomial for speaker adaptation of connectionist speech recognition systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Siniscalchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2152" to="2161" />
			<date type="published" when="2013">2013a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b318">
	<analytic>
		<title level="a" type="main">A bottom-up modular search approach to large vocabulary continuous speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Siniscalchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Svendsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b319">
	<analytic>
		<title level="a" type="main">Exploiting deep neural networks for detection-based speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Siniscalchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="148" to="157" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b320">
	<analytic>
		<title level="a" type="main">Speech recognition using long-span temporal patterns in a deep network model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Siniscalchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="201" to="204" />
			<date type="published" when="2013-03">March 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b321">
	<analytic>
		<title level="a" type="main">Sparse multilayer perceptrons for phoneme recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sivaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012-01">January 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b322">
	<analytic>
		<title level="a" type="main">Tensor product variable binding and the representation of symbolic structures in connectionist systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="159" to="216" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b323">
	<monogr>
		<title level="m" type="main">The Harmonic Mind -From Neural Computation to Optimality-Theoretic Grammar</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Legendre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b324">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b325">
	<analytic>
		<title level="a" type="main">New directions in deep learning: Structured models, tasks, and datasets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS) Workshop on Deep Learning and Unsupervised Feature Learning</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b326">
	<analytic>
		<title level="a" type="main">Deep learning for NLP</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.socher.org/index.php/DeepLearningTutorial" />
	</analytic>
	<monogr>
		<title level="m">Association of Computational Logistics (ACL), 2012, and North American Chapter of the Association of Computational Linguistics (NAACL)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b327">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b328">
	<analytic>
		<title level="a" type="main">Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b329">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013">2013b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b330">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS) Deep Learning Workshop</title>
				<imprint>
			<date type="published" when="2013">2013c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b331">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b332">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b333">
	<analytic>
		<title level="a" type="main">Semisupervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b334">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b335">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b336">
	<analytic>
		<title level="a" type="main">Discriminative transfer learning with tree-based priors</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b337">
	<analytic>
		<title level="a" type="main">Compete to compute</title>
		<author>
			<persName><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kazerounian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b338">
	<analytic>
		<title level="a" type="main">Preliminary investigation of boltzmann machine classifiers for speaker recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Senoussaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Odyssey</title>
				<meeting>Odyssey</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b339">
	<analytic>
		<title level="a" type="main">Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure</title>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ropson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Artificial Intelligence and Statistics (AISTATS)</title>
				<meeting>Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b340">
	<analytic>
		<title level="a" type="main">Error back propagation for sequence training of context-dependent deep networks for conversational speech transcription</title>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b341">
	<analytic>
		<title level="a" type="main">Multi-sensory speech processing: Incorporating automatically extracted hidden dynamic information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Multimedia &amp; Expo (ICME)</title>
				<meeting>IEEE International Conference on Multimedia &amp; Expo (ICME)<address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-07">July 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b342">
	<analytic>
		<title level="a" type="main">An overlapping-feature based phonological model incorporating linguistic constraints: Applications to speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1086" to="1101" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b343">
	<monogr>
		<title level="m" type="main">Training recurrent neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b344">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b345">
	<analytic>
		<title level="a" type="main">Deep networks for robust visual recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b346">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<title level="m">Learning Stochastic Feedforward Neural Networks. NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b347">
	<analytic>
		<title level="a" type="main">Small codes and large image databases for recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tarralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b348">
	<analytic>
		<title level="a" type="main">Modeling human motion using binary latent variables</title>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b349">
	<analytic>
		<title level="a" type="main">Deep neural network features and semi-supervised training for low resource speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b350">
	<analytic>
		<title level="a" type="main">Training restricted boltzmann machines using approximations to the likelihood gradient</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
				<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b351">
	<analytic>
		<title level="a" type="main">Speech synthesis based on hidden markov models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nankaku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Oura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1234" to="1252" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b352">
	<analytic>
		<title level="a" type="main">Acoustic modeling with hierarchical reservoirs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Triefenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jalalvand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Demuynck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2439" to="2450" />
			<date type="published" when="2013-11">November 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b353">
	<analytic>
		<title level="a" type="main">Towards deep understanding: Deep convex networks for semantic utterance classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b354">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics (ACL)</title>
				<meeting>Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b355">
	<analytic>
		<title level="a" type="main">Context-dependent MLPs for LVCSR: TANDEM, hybrid or both?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tüske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b356">
	<analytic>
		<title level="a" type="main">A deep neural network for acoustic-articulatory speech inversion</title>
		<author>
			<persName><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Richmond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS) Workshop on Deep Learning and Unsupervised Feature Learning</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b357">
	<analytic>
		<title level="a" type="main">Extended VTS for noise-robust speech recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Van Dalen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="733" to="743" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b358">
	<analytic>
		<title level="a" type="main">Deep content-based music recommendation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b359">
	<analytic>
		<title level="a" type="main">Speaker recognition by means of deep belief networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vasilakakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cumani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laface</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Biometric Technologies in Forensic Science</title>
				<meeting>Biometric Technologies in Forensic Science</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b360">
	<analytic>
		<title level="a" type="main">Sequence-discriminative training of deep neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b361">
	<analytic>
		<title level="a" type="main">Semi-supervised training of deep neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<meeting>the Automatic Speech Recognition and Understanding Workshop (ASRU)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b362">
	<analytic>
		<title level="a" type="main">A connection between score matching and denoising autoencoder</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b363">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b364">
	<analytic>
		<title level="a" type="main">Learning with recursive perceptual representations</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b365">
	<analytic>
		<title level="a" type="main">Krylov subspace descent for deep learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Artificial Intelligence and Statistics (AISTATS)</title>
				<meeting>Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b366">
	<analytic>
		<title level="a" type="main">Comparing multilayer perceptron to deep belief network tandem features for robust ASR</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b367">
	<analytic>
		<title level="a" type="main">Revisiting recurrent neural networks for robust ASR</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b368">
	<analytic>
		<title level="a" type="main">Dropout training as adaptive regularization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b369">
	<analytic>
		<title level="a" type="main">Phoneme recognition using time-delay neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustical Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="328" to="339" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b370">
	<analytic>
		<title level="a" type="main">Context-dependent modelling of deep neural network using logistic regression</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<meeting>the Automatic Speech Recognition and Understanding Workshop (ASRU)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b371">
	<analytic>
		<title level="a" type="main">Regression-based context-dependent modeling of deep neural networks for speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
				<imprint>
			<publisher>IEEE/Association for Computing Machinery</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b372">
	<analytic>
		<title level="a" type="main">An empirical analysis of dropout in piecewise linear networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
				<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b373">
	<analytic>
		<title level="a" type="main">Exponential family harmoniums with an application to information retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosen-Zvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
				<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b374">
	<analytic>
		<title level="a" type="main">Single-channel mixed speech recognition using deep neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b375">
	<analytic>
		<title level="a" type="main">Large scale image annotation: Learning to rank with joint word-image embeddings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="21" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b376">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence (IJCAI)</title>
				<meeting>International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b377">
	<analytic>
		<title level="a" type="main">Investigations on hessian-free optimization for cross-entropy training of deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wiesler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b378">
	<analytic>
		<title level="a" type="main">A probabilistic interaction model for multi-pitch tracking with factorial hidden markov model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wohlmayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pernkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b379">
	<analytic>
		<title level="a" type="main">Stacked generalization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="241" to="259" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b380">
	<analytic>
		<title level="a" type="main">Optimization algorithms and applications for speech and language processing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kanevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2231" to="2243" />
			<date type="published" when="2013-11">November 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b381">
	<analytic>
		<title level="a" type="main">A geometric perspective of large-margin training of gaussian models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="118" to="123" />
			<date type="published" when="2010-11">November 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b382">
	<analytic>
		<title level="a" type="main">Equivalence of backpropagation and contrastive hebbian learning in a layered network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="441" to="454" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b383">
	<analytic>
		<title level="a" type="main">An experimental study on speech enhancement based on deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="68" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b384">
	<analytic>
		<title level="a" type="main">Restructuring of deep neural network acoustic models with singular value decomposition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b385">
	<analytic>
		<title level="a" type="main">An integrative and discriminative technique for spoken utterance classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1207" to="1214" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b386">
	<analytic>
		<title level="a" type="main">A scalable approach to using DNN-derived features in GMM-HMM based acoustic modeling for LVCSR</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b387">
	<analytic>
		<title level="a" type="main">Combining a two-step CRF model and a joint source-channel model for machine transliteration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics (ACL)</title>
				<meeting>Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="275" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b388">
	<analytic>
		<title level="a" type="main">A fast maximum likelihood nonlinear feature transformation method for GMM-HMM speaker adaptation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2013">2013a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b389">
	<analytic>
		<title level="a" type="main">Adaptation of context-dependent deep neural networks for automatic speech recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b390">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for language understanding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b391">
	<analytic>
		<title level="a" type="main">Noise model transfer: Novel approach to robustness against nonstationary noise</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2182" to="2192" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b392">
	<analytic>
		<title level="a" type="main">Investigation of unsupervised adaptation of DNN acoustic models with filter bank input</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ragni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b393">
	<monogr>
		<title level="m" type="main">On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates. Stochastics and Stochastic Reports</title>
		<author>
			<persName><forename type="first">L</forename><surname>Younes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="177" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b394">
	<analytic>
		<title level="a" type="main">Factorized deep neural networks for adaptive speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Statistical Machine Learning for Speech Processing</title>
				<imprint>
			<date type="published" when="2012-03">March 2012b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b395">
	<analytic>
		<title level="a" type="main">Learning in the deep-structured conditional random fields</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS) 2009 Workshop on Deep Learning for Speech Recognition and Related Applications</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>References</note>
</biblStruct>

<biblStruct xml:id="b396">
	<analytic>
		<title level="a" type="main">Solving nonlinear estimation problems using splines</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="86" to="90" />
			<date type="published" when="2009-07">July 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b397">
	<analytic>
		<title level="a" type="main">Deep-structured hidden conditional random fields for phonetic recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2010-09">September 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b398">
	<analytic>
		<title level="a" type="main">Accelerated parallelizable neural networks learning algorithms for speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b399">
	<analytic>
		<title level="a" type="main">Deep learning and its applications to signal and information processing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="page" from="145" to="154" />
			<date type="published" when="2011-01">January 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b400">
	<analytic>
		<title level="a" type="main">Efficient and effective algorithms for training singlehidden-layer neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="554" to="558" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b401">
	<analytic>
		<title level="a" type="main">Roles of pre-training and fine-tuning in context-dependent DBN-HMMs for real-world speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS) 2010 Workshop on Deep Learning and Unsupervised Feature Learning</title>
				<imprint>
			<date type="published" when="2010-12">December 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b402">
	<analytic>
		<title level="a" type="main">Robust speech recognition using cepstral minimum-mean-square-error noise suppressor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2008-07">July 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b403">
	<analytic>
		<title level="a" type="main">A novel framework and training algorithm for variable-parameter hidden markov models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1348" to="1360" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b404">
	<analytic>
		<title level="a" type="main">Large-margin minimum classification error training: A theoretical risk minimization perspective</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="415" to="429" />
			<date type="published" when="2008-10">October 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b405">
	<analytic>
		<title level="a" type="main">Large-margin minimum classification error training for large-scale speech recognition tasks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b406">
	<monogr>
		<title level="m" type="main">Discriminative pretraining of deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-11">November 2011</date>
		</imprint>
		<respStmt>
			<orgName>U.S. Patent Filing</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b407">
	<analytic>
		<title level="a" type="main">Cross-lingual speech recognition under runtime resource constraints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2009">2009b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b408">
	<analytic>
		<title level="a" type="main">Large vocabulary speech recognition using deep tensor neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012">2012c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b409">
	<analytic>
		<title level="a" type="main">The deep tensor neural network with applications to large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="388" to="396" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b410">
	<analytic>
		<title level="a" type="main">Calibration of confidence measures in speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech and Language</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2461" to="2473" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b411">
	<analytic>
		<title level="a" type="main">Exploiting sparseness in deep neural networks for large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b412">
	<analytic>
		<title level="a" type="main">Improved bottleneck features using pre-trained deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
				<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b413">
	<analytic>
		<title level="a" type="main">Feature learning in deep neural networks -studies on speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
				<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b414">
	<analytic>
		<title level="a" type="main">Boosting attribute and phone estimation accuracies with deep neural networks for detectionbased speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Siniscalchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b415">
	<analytic>
		<title level="a" type="main">Sequential labeling using deep-structured conditional random fields</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="965" to="973" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b416">
	<analytic>
		<title level="a" type="main">Language recognition using deep-structured conditional random fields</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Karam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="5030" to="5033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b417">
	<analytic>
		<title level="a" type="main">KL-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b418">
	<analytic>
		<title level="a" type="main">Unsupervised adaptation with discriminative mapping transforms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="714" to="723" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b419">
	<analytic>
		<title level="a" type="main">Learning image representations from the pixel level via hierarchical sparse coding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b420">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Zamora-Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Castro-Bleda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>España-Boquera</surname></persName>
		</author>
		<title level="m">Fast evaluation of connectionist language models. International Conference on Artificial Neural Networks</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b421">
	<monogr>
		<title level="m" type="main">Hierarchical convolutional deep learning in computer vision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-01">January 2014</date>
		</imprint>
		<respStmt>
			<orgName>New York University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b422">
	<analytic>
		<title level="a" type="main">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR)</title>
				<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b423">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2901</idno>
		<title level="m">Visualizing and understanding convolutional networks</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b424">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer vision (ICCV)</title>
				<meeting>International Conference on Computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b425">
	<analytic>
		<title level="a" type="main">Product of experts for statistical parametric speech synthesis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Nankaku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Tokuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="794" to="805" />
			<date type="published" when="2012-03">March 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b426">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nankaku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
		<title level="m">Continuous stochastic feature mapping based on trajectory HMMs. IEEE Transactions on Audio, Speech, and Language Processings</title>
				<imprint>
			<date type="published" when="2011-02">February 2011</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="417" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b427">
	<analytic>
		<title level="a" type="main">Statistical parametric speech synthesis using deep neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7962" to="7966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b428">
	<analytic>
		<title level="a" type="main">Improving deep neural network acoustic models using generalized maxout networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b429">
	<analytic>
		<title level="a" type="main">Deep belief networks based voice activity detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="697" to="710" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b430">
	<analytic>
		<title level="a" type="main">Multi-sensory microphones for robust speech detection, enhancement and recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sinclair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP)</title>
				<meeting>International Conference on Acoustics Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b431">
	<analytic>
		<title level="a" type="main">Nonlinear compensation using the gauss-newton method for noise-robust speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2191" to="2206" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b432">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b433">
	<analytic>
		<title level="a" type="main">A segmental CRF approach to large vocabulary continuous speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
				<meeting>the Automatic Speech Recognition and Understanding Workshop (ASRU)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>

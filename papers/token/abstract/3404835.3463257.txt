The milestone improvements brought about by deep representation learning and pretraining techniques have led to large performance gains across downstream NLP IR and Vision tasks Multimodal modeling techniques aim to leverage large highquality visiolinguistic datasets for learning complementary information across image and text modalities In this paper we introduce the Wikipediabased Image Text WIT Dataset to better facilitate multimodal multilingual learning WIT is composed of a curated set of 375 million entity rich imagetext examples with 115 million unique images across 108 Wikipedia languages Its size enables WIT to be used as a pretraining dataset for multimodal models as we show when applied to downstream tasks such as imagetext retrieval WIT has four main and unique advantages First WIT is the largest multimodal dataset by the number of imagetext examples by 3x at the time of writing Second WIT is massively multilingual first of its kind with coverage over 100 languages each of which has at least 12K examples and provides crosslingual texts for many images Third WIT represents a more diverse set of concepts and real world entities relative to what previous datasets cover Lastly WIT provides a very challenging realworld test set as we empirically illustrate using an imagetext retrieval task as an example WIT Dataset is available for download and use via a Creative Commons license here httpsgithubcomgoogleresearchdatasetswit 